{
  "hash": "b42b6aee22198d86f5a47ca3aed554c8",
  "result": {
    "markdown": "---\ntitle: \"Assignment 8 Solutions: Autocorrelation and Interpolation\"\nexecute:\n  eval: false\n---\n\n\n  \n<span style=\"color:#9F281A;\">Read in the disasters dataset, convert it to points, filter it to those disasters in Idaho, and select any relevant columns. You will also need to use `tigris::county()` to download a county shapefile for the region. Make sure your data are projected correctly</span>\n\n>I start by loading the packages necessary for the entire analysis. Then, I use the `tigris` package to get my county files. We need this for two reasons: to subset the disaster data into our region of interest. Note that I used the entire region because it's possible that there is information to be learned from the data on the borders of Idaho that don't conform to the state boundaries. I then load the disaster dataset, select a handful of columns that I'm interest in, drop any records that are missing their coordinates, convert the `csv` to a `sf` object, project it to the same CRS as the county dataset, and then keep only the `distinct` point locations. This last step is important because having multiple events in the exact same location creates issues for calculating our spatial autocorrelation estimates (because the distance is exactly zero making it difficult to determine which event is the \"parent\").\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\nlibrary(tidyverse, quietly = TRUE)\nlibrary(spdep)\nlibrary(spatstat)\nlibrary(sp)\nlibrary(terra)\nlibrary(tmap)\n\ncty <- tigris::counties(state = c(\"ID\", \"WA\", \"OR\"), progress_bar=FALSE)\n\n\n\ndisast.sf <- read_csv(\"data/opt/data/2023/assignment07/ics209-plus-wf_incidents_1999to2020.csv\") %>% \n  filter(., START_YEAR >= 2000 & START_YEAR <= 2017) %>% \n  select(INCIDENT_ID, , POO_STATE, POO_LATITUDE, POO_LONGITUDE, FATALITIES, PROJECTED_FINAL_IM_COST, STR_DESTROYED_TOTAL, PEAK_EVACUATIONS) %>% \n  drop_na(c(POO_LATITUDE, POO_LONGITUDE)) %>% \n  st_as_sf(coords = c(\"POO_LONGITUDE\", \"POO_LATITUDE\"), crs= 4326) %>% \n  st_transform(., st_crs(cty)) %>% \n  distinct(., geometry, .keep_all=TRUE)\n  \ndisast.sf <- disast.sf[cty,]\n```\n:::\n\n\n<span style=\"color:#9F281A;\">Generate the Ripley's K curves for the disaster dataset. What do you think? Is there evidence that the data is spatially autocorrelated?</span>\n>We use the same code from class here to estimate the Ripley's K function. We first select the variable we're interested in (`STR_DESTROYED_TOTAL` in my case), `transform` the CRS to a planar coordinate system, and convert it to a `ppp` object for `spdep`. We use the `envelope` function with `Kest` to calculate several theoretical values for Ripley's K under complete spatial randomness. Comparing the `K_{obs}` to the envelope of theoretical values suggests that there is more aggregation in the data than would be predicted under CSR.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkf.env <- envelope(as.ppp(st_transform(select(disast.sf, STR_DESTROYED_TOTAL), crs=8826)), Kest, correction = \"translation\",  nsim= 1000, envelope = TRUE, verbose = FALSE)\n\nplot(kf.env)\n```\n:::\n\n\n<span style=\"color:#9F281A;\">Use the nearest-neighbor approach that we used in class to estimate the lagged values for the disaster dataset and estimate the slope of the line describing Moran's I statistic.</span>\n\n>We begin by finding the nearest neighbor for each observation using the `knearneigh` function which finds the `k` closest neighbors for each point. Because we only want the nearest neighbor, we set `k=1`. We need to convert this to a neighbor list (`class(geog.nearnb) = nb`) and do this by wrapping the output of `knearneigh` inside of `knn2nb` which converts `knn` objects to `nb` objects. We then need to estimate the distance to each neighbor (using `dnearneigh`) and convert it to a spatial weights matrix (using `nb2listw`). Finally, we convert this weight's matrix into a vector of the same number of rows as our disaster dataset using `lag.listw`. This function creates a new estimate of `STR_DESTROYED_TOTAL` for each row based on the spatially weighted value of the nearest neighbor. Finally, we fit a simple linear regression to the data and see that there is a slight positive slope to the line suggesting that there is some autocorrelation (remember, the slope of this simple linear model is the Moran's I coefficient). \n\n\n::: {.cell}\n\n```{.r .cell-code}\ngeog.nearnb <- knn2nb(knearneigh(disast.sf, k = 1), row.names = disast.sf$INCIDENT_ID, sym=TRUE); #estimate distance to first neareset neighbor\nnb.nearest <- dnearneigh(disast.sf, 0,  max( unlist(nbdists(geog.nearnb, disast.sf))));\nlw.nearest <- nb2listw(nb.nearest, style=\"W\")\nbldg.lag <- lag.listw(lw.nearest, disast.sf$STR_DESTROYED_TOTAL)\nM <- lm(bldg.lag ~ disast.sf$STR_DESTROYED_TOTAL)\nsummary(M)\nplot(bldg.lag ~ disast.sf$STR_DESTROYED_TOTAL, xlim=c(0,20))\nabline(M, col=\"red\")\n```\n:::\n\n\n<span style=\"color:#9F281A;\">Now use the permutation approach to compare your measured value to one generated from multiple simulations. Generate the plot of the data. Do you see more evidence of spatial autocorrelation?</span>\n\n>We can verify this by using a Monte Carlo permutation approach. We use a `for` loop to \"shuffle\" the data (using `sample`), but keep the same neighbor structure (using our same `lw.nearest` spatial weights matrix). We then fit a linear model to the reshuffled data and estimate the slope to see what values are plausible under complete spatial randomness (which we achieve by shuffling the data independent of their location). Run this loop 1000 times (set by `n <- 1000L`) and you'll generate a distribution of plausible values. Based on the distribution and our actual value (in red), we can see that this value for Moran's I is generally larger than we'd expect under CSR, but not terribly so.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000L   # Define the number of simulations\nI.r <- vector(length=n)  # Create an empty vector\n\nfor (i in 1:n){\n  # Randomly shuffle income values\n  x <- sample(disast.sf$STR_DESTROYED_TOTAL, replace=FALSE)\n  # Compute new set of lagged values\n  x.lag <- lag.listw(lw.nearest, x)\n  # Compute the regression slope and store its value\n  M.r    <- lm(x.lag ~ x)\n  I.r[i] <- coef(M.r)[2]\n}\n\nhist(I.r, main=NULL, xlab=\"Moran's I\", las=1)\nabline(v=coef(M)[2], col=\"red\")\n```\n:::\n\n\n<span style=\"color:#9F281A;\">Generate the 0th, 1st, and 2nd order spatial trend surfaces for the data. Is there evidence for a second order trend? How can you tell?</span>\n\n>In order to generate a spatial trend surface, we need to predict values across a uniform grid that covers the study region. We initialize that grid by using our county dataset and drawing 15000 random sample points across the region. We then create a series of formula object depicting the 0th, 1st (linear), and 2nd (quadratic) models to the data where the predictors are just the X and Y coordinates. We fit each of the models using `lm`, convert them into a `SpatialGridDataFrame` from the `sp` package, and then convert them to a raster to make plotting easier. Based on the curvature we see in the 2nd order trend surface, there is an indication of a 2nd order trend, though it is not super strong. We'll use that model for kriging in the subsequent steps.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrd <- as.data.frame(spsample(as(cty, \"Spatial\"), \"regular\", n=15000))\nnames(grd)       <- c(\"X\", \"Y\")\ncoordinates(grd) <- c(\"X\", \"Y\")\ngridded(grd)     <- TRUE  # Create SpatialPixel object\nfullgrid(grd)    <- TRUE  # Create SpatialGrid object\nproj4string(grd) <- proj4string(as(disast.sf, \"Spatial\"))\n\nf.0  <- as.formula(PROJECTED_FINAL_IM_COST ~ 1)\n\n# Run the regression model\nlm.0 <- lm( f.0 , data=disast.sf)\n\n# Use the regression model output to interpolate the surface\ndat.0th <- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.0, newdata=grd)))\nr   <- rast(dat.0th)\nr.m0 <- mask(r, st_as_sf(cty))\n\nf.1  <- as.formula(STR_DESTROYED_TOTAL ~ X + Y)\n\ndisast.sf$X <- st_coordinates(disast.sf)[,1]\ndisast.sf$Y <- st_coordinates(disast.sf)[,2]\n\n# Run the regression model\nlm.1 <- lm( f.1 , data=disast.sf)\n\n# Use the regression model output to interpolate the surface\ndat.1st <- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.1, newdata=grd)))\n\n# Convert to raster object to take advantage of rasterVis' imaging\n# environment\nr   <- rast(dat.1st)\nr.m1 <- mask(r, st_as_sf(cty))\n\nf.2 <- as.formula(STR_DESTROYED_TOTAL ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))\n\n# Run the regression model\nlm.2 <- lm( f.2, data=disast.sf)\n\n# Use the regression model output to interpolate the surface\ndat.2nd <- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.2, newdata=grd))) \n\nr   <- rast(dat.2nd)\nr.m2 <- mask(r, st_as_sf(cty))\nrst.stk <- c(r.m0, r.m1, r.m2)\nnames(rst.stk) <- c(\"zeroOrder\", \"firstOrder\", \"secondOrder\")\nplot(rst.stk)\n```\n:::\n\n\n<span style=\"color:#9F281A;\">Now use the spatial trend surface to perform some ordinary krigging. You'll want to have a grid of 15,000 points, fit 3 different experimental variogram functions (see the `vgm` function helpfile to learn more about the shapes available to you). Plot your variogram fits. Which one would you choose? Why?</span>\n\n>A variogram simply plots the relationship between distance and the residuals of a model. We first assign those residuals to our disaster dataset. We then generate a cloud-style variogram for data without eliminating the spatial trend. As you can see, there are some strange bands that show up in the data likely due to the second order effects we saw in the model previously.\n\n\n::: {.cell hash='08-combinationssolutions_cache/html/unnamed-chunk-6_6bf62808319faa1f0e4674ee9bb2b302'}\n\n```{.r .cell-code}\ndisast.sf$res <- lm.2$residuals\n\nvar.cld  <- gstat::variogram(res ~ 1, disast.sf, cloud = TRUE)\nvar.df  <- as.data.frame(var.cld)\n\n\nOP <- par( mar=c(4,6,1,1))\nplot(var.cld$dist/1000 , var.cld$gamma, col=\"grey\", \n     xlab = \"Distance between point pairs (km)\",\n     ylab = expression( frac((res[2] - res[1])^2 , 2)) )\npar(OP)\n```\n:::\n\n\n>We then fit a variogram to the detrended data (this is the sample variogram) by passing our `f.2` formula to the variogram function. We take the mean values of the pairwise differences and plot them in bins on top of the original data. As you can see, this reduces a considerable amount of noise and the shape of the variogram begins to materialize.\n\n\n::: {.cell hash='08-combinationssolutions_cache/html/unnamed-chunk-7_1a49d0757b518fe15932f0d0256f8736'}\n\n```{.r .cell-code}\nvar.smpl <- gstat::variogram(f.2, disast.sf, cloud = FALSE)\n\nbins.ct <- c(0, var.smpl$dist , max(var.cld$dist) )\nbins <- vector()\nfor (i in 1: (length(bins.ct) - 1) ){\n  bins[i] <- mean(bins.ct[ seq(i,i+1, length.out=2)] ) \n}\nbins[length(bins)] <- max(var.cld$dist)\nvar.bins <- findInterval(var.cld$dist, bins)\nvar.cld2 <- var.cld[var.cld$gamma < 500,]\nOP <- par( mar = c(5,6,1,1))\nplot(var.cld2$gamma ~ eval(var.cld2$dist/1000), col=rgb(0,0,0,0.2), pch=16, cex=0.7,\n     xlab = \"Distance between point pairs (km)\",\n     ylab = expression( gamma ) )\npoints( var.smpl$dist/1000, var.smpl$gamma, pch=21, col=\"black\", bg=\"red\", cex=1.3)\nabline(v=bins/1000, col=\"red\", lty=2)\npar(OP)\n```\n:::\n\n\n>You can use `vgm()` to see potential shapes of the different variograms that we can fit using `gstat`. I chose linear, Gaussian, and spherical as they are common choices and because the initial rise to the sill seemed consistent with those shapes. Note that the semivariance begins to increase again you get further out. This might suggest that we need a more complicated de-trending model, but we won't worry about that now. The 3 forms seem to fit the data in similar ways, but the spherical form has a slightly more gradual rise to the sill. I choose that one as that may help smooth a bit more than the other 2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute the variogram model by passing the nugget, sill and range values\n# to fit.variogram() via the vgm() function.\ndat.fit.lin  <- gstat::fit.variogram(var.smpl, gstat::vgm(psill= 50, model=\"Lin\"))\ndat.fit.gau  <- gstat::fit.variogram(var.smpl, gstat::vgm(psill=50, model=\"Gau\"))\ndat.fit.sph  <- gstat::fit.variogram(var.smpl, gstat::vgm(psill=50, model=\"Sph\"))\n\n# The following plot allows us to gauge the fit\nplot(var.smpl, dat.fit.lin, main = \"Linear variogram\")\nplot(var.smpl, dat.fit.gau, main = \"Gaussian variogram\")\nplot(var.smpl, dat.fit.sph, main = \"Spherical variogram\")\n```\n:::\n\n\n<span style=\"color:#9F281A;\">Using your spatial trend model and your fitted variogram, krige the data and generate a map of the interpolated value and a map of the error.</span>\n\n>Now that we've got our Spherical variogram estimated on the detrended data, we can use the `krige` function to generate spatial predictions across the grid. We can also access the variance resulting from that model. We do that, convert them to rasters and plot the two outcomes. We can see that we've eliminated the bulk of spatial patterns in the residuals (as evidenced by light colors on the predicted residual map); however, the predictions for the western cost are much less stable (as evidenced by the variance map). \n\n\n::: {.cell hash='08-combinationssolutions_cache/html/unnamed-chunk-9_86a3e3e982c342b9bd84aa01ad03a533'}\n\n```{.r .cell-code}\ndat.krg <- gstat::krige( res~1, as(disast.sf, \"Spatial\"), grd, dat.fit.sph)\n\nr <- rast(dat.krg)$var1.pred\nr.m.pred <- mask(r, st_as_sf(cty))\ntm_shape(r.m.pred) + tm_raster(n=10, palette=\"RdBu\", title=\"Predicted residual \\nstructures destroyed\")  +\n  tm_legend(legend.outside=TRUE)\n\nr <- rast(dat.krg)$var1.var\nr.m.var <- mask(r, st_as_sf(cty))\ntm_shape(r.m.var) + tm_raster(n=7, palette =\"Reds\", ,title=\"Variance map \") +\n  tm_legend(legend.outside=TRUE)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}