{
  "hash": "2a0c2c1b7ac408b67536e8333dfbd25b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Assignment 9 Solutions: Fitting models to your dataframe\"\nexecute:\n  eval: false\n---\n\n\n\n\n<span style=\"color:#9F281A;\">1. Use the variables that you chose from assignment 6 along with the wildfire hazard and land use dataset to attribute each disaster in the disaster dataset.</span>\n\n>Here I am following the same procedure from assignment 7 for creating the spatial database. The only real change is that we are attributing point data (in the incidents dataset) instead of summarizing to polygons (like we did with the Forest Service data). We drop any incomplete cases to avoid problems with NAs (though this may not be the best thing to do in practice) and store that data for later. We then set up our model dataframe by making sure that the cost variable is an integer (for Poisson modeling) and that we drop levels from the land use dataset that don't appear in our incident locations. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\nlibrary(tidyverse, quietly = TRUE)\nlibrary(terra)\nlibrary(tmap, quietly = TRUE)\nlibrary(caret)\n\n \ncejst.pnw <- read_sf(\"data/opt/data/2023/assignment07/cejst_pnw.shp\")%>% \n  filter(., !st_is_empty(.))\n\nincidents.csv <- read_csv(\"data/opt/data/2023/assignment07/ics209-plus-wf_incidents_1999to2020.csv\")\n\nland.use <- rast(\"data/opt/data/2023/assignment07/land_use_pnw.tif\")\nfire.haz <- rast(\"data/opt/data/2023/assignment07/wildfire_hazard_agg.tif\")\n\n\nfire.haz.proj <- project(fire.haz, land.use)\n\n\ncejst.proj <- cejst.pnw %>% \n  st_transform(., crs=crs(land.use))\n\nincidents.proj <- incidents.csv %>% \n  filter(., !is.na(POO_LONGITUDE) | !is.na(POO_LATITUDE) ) %>% \n  st_as_sf(., coords = c(\"POO_LONGITUDE\", \"POO_LATITUDE\"), crs= 4269) %>% \n  st_transform(., crs=crs(land.use))\nincidents.pnw <- st_crop(incidents.proj, st_bbox(cejst.proj))\n\nhazard.smooth <- focal(fire.haz.proj, w=5, fun=\"mean\")\nland.use.smooth <- focal(land.use, w=5, fun=\"modal\")\nlevels(land.use.smooth) <- levels(land.use)\n\ncejst.select <- cejst.proj %>% \n  select(., c(TPF, HBF_PFS, P200_I_PFS))\n\nincident.cejst <- incidents.pnw %>% \n  st_join(., y=cejst.select, join=st_within) \n\nincident.landuse.ext <- terra::extract(x=land.use.smooth, y = vect(incident.cejst), fun=\"modal\", na.rm=TRUE)\n\nincident.firehaz.ext <- terra::extract(x= hazard.smooth, y = vect(incident.cejst), fun=\"mean\", na.rm=TRUE)\n\nincident.cejst.join <- cbind(incident.cejst,incident.landuse.ext$category, incident.firehaz.ext$focal_mean) %>% \n  rename(category = \"incident.landuse.ext.category\", hazard = \"incident.firehaz.ext.focal_mean\")\n\nincident.cejst.prep <- incident.cejst.join %>% \n  select(., PROJECTED_FINAL_IM_COST, TPF, HBF_PFS, P200_I_PFS, hazard, category,) %>% \n  st_drop_geometry(.) %>% \n  filter(., complete.cases(.))\n\nincident.cejst.model <- incident.cejst.prep  %>% \n  mutate(across(TPF:hazard, ~ as.numeric(scale(.x))),\n         category=droplevels(category),\n         cost = as.integer(floor(PROJECTED_FINAL_IM_COST))) %>% \n  select(-PROJECTED_FINAL_IM_COST)\n```\n:::\n\n\n\n\n\n<span style=\"color:#9F281A;\">2. Fit a Poisson regression using your covariates and the cost of the incident data (using `glm` with `family=poisson()`)</span>\n\n>Now that we have our data, it's time to set up some models. We take advantage of the `caret` package to split the data into a training and testing set using the category variable to make sure we have representation of all the cateogries. We then set up our `trainControl` options to use cross validation as a means of adjusting tuning parameters and tell `R` to only save the best model once the tuning is complete. Finally, we use the `train` function from `caret` to fit our first model. For a simple Poisson regression, we can rely on the `glm` method with the `family` set to `poisson`. Note that because this is not binary data, our ROC metric doesn't work as a means of evaluating the performance of the different tuning parameters. Instead, we use something called the Root-Mean Squared Error (RMSE).The RMSE is a measure of the difference between the fitted value and the observed value (in this case, for the cross-validation data within the model training). Larger values indicate poorer fits.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(998)\ninTraining <- createDataPartition(incident.cejst.model$category, p = .8, list = FALSE)\ntraining <- incident.cejst.model[ inTraining,]\ntesting  <- incident.cejst.model[-inTraining,]\n\nfitControl <- trainControl(\n   method = \"cv\",  # k-fold cross validation\n   number = 10,  # 10 folds\n   savePredictions = \"final\"       # save predictions for the optimal tuning parameter\n)\n\nPoisFit <- train( cost ~ ., data = training, \n                 method = \"glm\", \n                 family = poisson,\n                 trControl = fitControl,\n                 metric=\"RMSE\"\n                 )\n```\n:::\n\n\n\n\n\n<span style=\"color:#9F281A;\">3. Fit a regression tree using your covariates and the cost of the incident data (using `caret` package `method=`rpart`)</span>\n\n>We use similar syntax to fit the regression tree to the data, but make a few changes. First, we set cost `as.numeric()` to ensure that this is a regression tree (because our data do not reflect categories). We then set the method to `rpart`. Because `rpart` has a complexity parameter, there is a bit of tuning to be done. We tell `R` that we're willing to look at 20 different values of this complexity parameter. We can use `plot` and `rpart.plot` to inspect the results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRtFit <- train(as.numeric(cost) ~ ., data = training, \n                 method = \"rpart\",\n                 trControl = fitControl,\n                 metric = \"RMSE\",\n                tuneLength = 20,  \n                 )\nplot(RtFit)\nrpart.plot::rpart.plot(RtFit$finalModel, type=4)\n```\n:::\n\n\n\n\n<span style=\"color:#9F281A;\">4. Fit a random forest model using your covariates and the cost of the incident data (using `caret` package `method= 'rf'`)</span>\n\n>The syntax is similar to the previous models, but with `method=rf` to signal that we want to use the `rf` package to fit the Random Forest. Here, the tuning parameter is the number of variables to include in the tree. We've only got 7 variables so we'll set the `tuneLength` to the maximum number of variables.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRFFit <- train(as.numeric(cost) ~ ., data = training, \n                 method = \"rf\",\n                 trControl = fitControl, \n               tuneLength=7\n                 )\nplot(RFFit)\nplot(RFFit$finalModel)\n```\n:::\n\n\n\n\n\n<span style=\"color:#9F281A;\">5. Use cross-validation to identify the best performing model of the 3 that you fit</span>\n\n>Now that we have three different models, let's see how well they do predicting the testing dataset. We first generate predictions using the `predict` function and supplying the model object and the `newdata`. In this case, our new data is the five covariate columns from the testing partition. Once we have the predictions, we can calculate the RMSE. Based on RMSE values the regression tree and Random Forest model seem to be the better performers.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois.pred <- predict(PoisFit, newdata = testing[,1:5])\nrmse.pois <- sqrt(sum(pois.pred - testing$cost)^2/length(pois.pred))\nrt.pred <- predict(RtFit, newdata = testing[,1:5])\nrmse.rt <- sqrt(sum(rt.pred - testing$cost)^2/length(rt.pred))\nrf.pred <- predict(RFFit, newdata = testing[,1:5])\nrmse.rf <- sqrt(sum(rf.pred - testing$cost)^2/length(rf.pred))\n```\n:::\n\n\n\n\n\n<span style=\"color:#9F281A;\">6. Convert all of your predictors into rasters of the same resolution and generate a spatial prediction based on your model</span>\n\n>Now that we've identified the models we want to use to generate our spatial surface, we need to prepare all of the input rasters. We use `rasterize` to create the cejst variables. These are on original scale of the data and so we need to rescale them to the same range of our modeled datasets. Here, we can't use `scale` because the mean of the total dataset would differ from the mean that we used for the incidents-only data so we have to manually set up the scale. Lastly, we have to drop the levels from the land use raster that weren't present in the incident dataset. We do that with the `subst` call from `terra`. Once we've got our rasters set up, we can just use `predict`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTPF.rast <- (rasterize(cejst.select, hazard.smooth, field=\"TPF\") - mean(incident.cejst.prep$TPF,na.rm=TRUE))/sd(incident.cejst.prep$TPF)\nHBF_PFS.rast <- (rasterize(cejst.select, hazard.smooth, field=\"HBF_PFS\")- mean(incident.cejst.prep$HBF_PFS,na.rm=TRUE))/sd(incident.cejst.prep$HBF_PFS)\nP200_I_PFS.rast <- (rasterize(cejst.select, hazard.smooth, field=\"P200_I_PFS\")- mean(incident.cejst.prep$P200_I_PFS,na.rm=TRUE))/sd(incident.cejst.prep$P200_I_PFS)\nland.use.smooth <- subst(land.use.smooth, from=c(\"Non-Forest Wetland\",\"Non-Processing Area Mask\"), to=c(NA, NA))\nhazard.smooth.scl <- (hazard.smooth - mean(incident.cejst.prep$hazard))/sd(incident.cejst.prep$hazard)\npred.rast <- c(TPF.rast, HBF_PFS.rast, P200_I_PFS.rast, land.use.smooth, hazard.smooth.scl)\nnames(pred.rast)[5] <- \"hazard\"\n\n\nrt.spatial <- terra :: predict(pred.rast, RtFit, na.rm=TRUE) \nrf.spatial <- terra :: predict(pred.rast, RFFit, na.rm=TRUE) \n```\n:::\n\n\n\n\n>If you looked at the RMSE values for our initial models, you'll notice that they were quite high and the models weren't particularly interesting. Because the cost data ranges over several orders of magnitude, we might try log-transforming them and fitting a linear model (because the data are no longer integers) along with the other two models. We do that here following the syntax above. When calculating the RMSE, we have to remember to log-transform the cost variable in the testing dataset to make sure that the predictions are comperable. Again, the regression tree and Random Forest are the better performers, but the RMSE suggests that we are doing considerably better ($10^{2}=100$ as opposed to the 100,000s we were getting before).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraining.log <- training %>% \n  mutate(cost = log(cost, 10))\n\nLinFit <- train( cost ~ ., data = training.log, \n                 method = \"lm\", \n                 trControl = fitControl,\n                 metric=\"RMSE\"\n                 )\n\nRtFit.log <- train(cost ~ ., data = training.log, \n                 method = \"rpart\",\n                 trControl = fitControl,\n                 metric = \"RMSE\",\n                tuneLength = 20,  \n                 )\n\nRFFit.log <- train(cost ~ ., data = training.log, \n                 method = \"rf\",\n                 trControl = fitControl\n                 )\n\nlin.pred <- predict(LinFit, newdata = testing[,1:5])\nrmse.lin <- sqrt(sum(lin.pred - log(testing$cost,10))^2/length(pois.pred))\nrt.pred <- predict(RtFit.log, newdata = testing[,1:5])\nrmse.rt <- sqrt(sum(rt.pred - log(testing$cost,10))^2/length(rt.pred))\nrf.pred <- predict(RFFit.log, newdata = testing[,1:5])\nrmse.rf <- sqrt(sum(rf.pred - log(testing$cost,10))^2/length(rf.pred))\n```\n:::\n\n\n\n\n\n<span style=\"color:#9F281A;\">7. Plot your result</span>\n>We use the `par` argument to set up a 2x2 layout and print all 4 plots.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt.spatial.log <- terra :: predict(pred.rast, RtFit.log, na.rm=TRUE) \nrf.spatial.log <- terra :: predict(pred.rast, RFFit.log, na.rm=TRUE) \n\npar(mfrow=c(2,2))\nplot(rt.spatial, main=\"Regression Tree Classifier\")\nplot(rf.spatial, main=\"Random Forest Classifier\")\nplot(rt.spatial.log, main=\"Regression Tree Classifier (log)\")\nplot(rf.spatial.log, main=\"Random Forest Classifier(log)\")\npar(mfrow=c(1,1))\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}