[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dr. Matt Williamson\n   4125 Environmental Research Building\n   mattwilliamson@boisestate.edu\n   Schedule an appointment\n\n\n\n\n\n   Mondays and Wednesdays\n   August 25–December 12, 2025\n   1:30–2:45 PM\n   ILC 404\n   Slack"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nSpatial data are ubiquitous and form the basis for many of our inquiries into social, ecological, and evolutionary processes. As such, developing the skills necessary for incorporating spatial data into reproducible statistical workflows is critical. In this course, we will introduce the core components of manipulating spatial data within the R statistical environment including managing vector and raster data, projections, extraction of data values, interpolation, and plotting. Students will also learn to prototype and benchmark different workflows to aid in applying the appropriate tools to their research questions."
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus",
    "section": "Course Objectives",
    "text": "Course Objectives\nStudents completing this course should be able to:\n\nArticulate the opportunities and challenges posed by geographic analysis.\nSelect the appropriate R packages and functions for manipulating different types of spatial data\nDesign statistical analyses that integrate geospatial and tabular data\n\nConstruct appropriate data visualizations for conveying geospatial data\nDevelop reproducible workflows for manipulating, visualizing, and analyzing spatial data."
  },
  {
    "objectID": "syllabus.html#expectations",
    "href": "syllabus.html#expectations",
    "title": "Syllabus",
    "section": "Expectations",
    "text": "Expectations\nBe nice. Be honest. Try hard.\nThe beauty of working with open source software is the community of users working on problems just like yours (and nothing like yours). Like any community, this one functions best when its members are kind, genuine, and make good-faith efforts to solve their problems along the way (more on this below).\nYou can (and should) expect me to:\n\nCreate a space where you can ask questions without fear of embarrassment or retribution\nProvide feedback on your work within 1 week of submission\nRespond to email and slack messages within 48 hours\nMake every attempt to answer your questions (when I can) or point you toward resources that may help\n\nIn turn, I expect you to:\n\nTreat all of us with respect and compassion\nMake an honest effort to work through the assignments\nDemonstrate that you have tried to solve your coding errors before asking me\nCommunicate with me when the course isn’t working for you"
  },
  {
    "objectID": "syllabus.html#prerequisite-knowledge-and-skills",
    "href": "syllabus.html#prerequisite-knowledge-and-skills",
    "title": "Syllabus",
    "section": "Prerequisite Knowledge and Skills",
    "text": "Prerequisite Knowledge and Skills\nYou can succeed in this class.\nSome familiarity with the R statistical environment is helpful, but not necessary. My goal is to foster an environment where we are all learning from each other and sharing the tips and tricks that help us along the way. Learning R can be difficult at first—it’s like learning a new language, just like Spanish, French, or Chinese. I find it helpful to remember the following:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later. Even experienced programmers find themselves bashing their heads against seemingly intractable errors. If you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, e-mail me, etc.\n\n— Hadley Wickham\n\n\nIf you want to start learning a few of the basics, the Resources tab has some background information to get you started. Note that this is not an exhaustive list - the number of new R tutorials available on the internet seems to be growing exponentially.\n\nGetting Help With R problems\nI am happy to help you work through your R coding challenges, but there are a lot of you and only one of me. Moreover, I may not always know exactly how to fix your problem any better than you do. In order to make sure that I am not the primary obstacle to your ability to complete the class assignments, I’m asking that you use the following steps prior to emailing/Slacking me with your coding questions.\n\n\n\n\n\n\nTip\n\n\n\nWhen you send me a question, please let me know what you searched, why the solutions you found don’t work for you, and what output you are expecting**\n\n\nWe’ll spend a bit of time on asking better questions and getting better answers so don’t worry if you aren’t quite sure how this all works.\n\nGoogle it! Searching for help with R on Google can sometimes be tricky. Google is generally smart enough to figure out what you mean when you search for “r reproject polygons”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats reproject polygons”). Also, since most of your R work will deal with the RSpatial packages, it’s often easier to just search for the package name and operation rather than the letter “r” (e.g. “sf reproject polygons”). I often paste the specific error message I get along with the spatial package I’m using to try and help Google find my solutions.\nAsk your colleagues We have an r_spatial chatroom at Slack where anyone in this class can ask questions and anyone can answer. Ask questions about code or class materials. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other peoples’ questions too. As a bonus, Slack allows you to format code to make it easy for all of us to copy and paste your code and distinguish it from the rest of your question.\nUse the forums Two of the most important sources for help with R-coding are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)). If you aren’t able to find an answer to your question from the thousands of existing questions, you can post your own. You’ll need to create a reproducible example so others can figure out what you’re trying to do and what error you’re receiving, but you’d be amazed how helpful the community can be.\nAsk me! Sign up for a time to meet with me during student hours at https://calendly.com/mattwilliamson/. I’ll want to know what searches you’ve tried (so I don’t chase down answers that you’ve already seen) and what approaches you’ve tried and why they haven’t worked. Remember, I’m here to help (but not write your code for you)."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nR and RStudio\nR is free, but it can sometimes be a pain to install and configure especially when dealing with spatial packages (we’ll talk more about why this is during class). To make life easier, I have set up an online RStudio server service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer and we should be able to avoid a number of the machine-specific issues that pop-up when 20 students have 20 different computers, operating systems (OS), etc. If you haven’t installed R on your local machine and would like some help getting that set up, there’ a useful set of instructions for installing R, RStudio, and all the tidyverse packages here.\n\n\nGit and Github Classroom\nAll assignments will be managed using Github classroom. This will allow each you to have your own repositories for each assignment and make it easier for me to comment on and help with your code. To use this, you should sign up for the GitHub Student Developers Pack as soon as possible and send me your github username. Once I have that, I can add you to the course and make sure that you have access to all of the necessary data and example code.\n\n\nReadings\nThe goal of this course is primarily to get you started with spatial workflows in R. That said, maps (and the spatial data that produce them) are extremely powerful and their use comes with risks and responsibilities. Although most of this course will focus on getting the code right, I’ll mix in a few readings each week to help tie the technical details of our code back to the broader contexts of spatial analysis or to illustrate new applications of the methods you are learning."
  },
  {
    "objectID": "syllabus.html#course-schedule",
    "href": "syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nThis course is organized in 5 sections:\n\nGetting Started: Introducing the course, the tools we’ll use, workflow and project management\nFoundations of Spatial Data: Describing objects in space, modifying and combining spatial data\nDescribing Spatial Patterns: Using metrics, models, and visualization to identify underlying spatial patterns.\nExplaining Spatial Patterns: Using statistical methods to identify potential causes of spatial patterns.\nPredicting Spatial Patterns: Creating and evaluating predictions of new observations based on statistical and spatial patterns\n\nThe schedule page provides an overview of what to expect each week.\nThis syllabus reflects a plan for the semester. Deviations may become necessary as the course progresses."
  },
  {
    "objectID": "syllabus.html#assignments-and-grades",
    "href": "syllabus.html#assignments-and-grades",
    "title": "Syllabus",
    "section": "Assignments and Grades",
    "text": "Assignments and Grades\nI teach this course because I believe that a) we can learn a lot about social and ecological processes by studying where they happen, b) integrating spatial analysis directly into statistical workflows makes those analyses more robust and reproducible, and c) overcoming coding challenges can provide a profound sense of accomplishment. That said, I recognize that there are many reasons that you are taking this course and that my objectives may differ from yours. In order to make sure that you get what you need out of this class, we’ll be using a mix of approaches for determining your grade in this course.\n\n\n\n\n\nAssignment Group\nPoints Possible\nPercentage of Grade\n\n\n\n\nSelf Reflection\n25\n4\n\n\nClass Exercises\n81\n15\n\n\nUnit Homework\n100\n18\n\n\nHomework Revision\n200\n36\n\n\nFinal Project\n150\n27\n\n\n\n\n\n\n\nSelf-assessment (12.5 pts x 2):  During the first week of the course, I’m going to ask you to reflect on what you want out of this course (concepts, skills, practice, etc.). This assessment will help me do a better job of aligning the content of the course to your specific needs. Grading for the self-assessment is described on the assignments page.\nIn-Class Excercises (3pts x 30): The latter part of each class session will involve some hands-on practice in the form of in-class excercises. These exercises are designed to encourage you to work with your peers to practice different skills in class, allow me to identify any challenges you’re experiencing, and encourage attendance. Exercises are due at 3:00PM after each class period. I will not accept submissions after the deadline as these are about in-class participation. If you turn the assignment in on-time you’ll recieve full credit.\n\n\n\n\n\n\nTip\n\n\n\nYou can miss up to 3 in-class exercises and still receive full-credit for this portion of the course.\n\n\nUnit Homework (20pts x 5): There are five homework assignments, one at the end of each section. These exercises are designed to reinforce the material we cover in lecture, give you practice designing and implementing your own workflows, and build habits that promote reproducibility in science. They also allow me get a sense for your engagement in the course. Exercises are due at 11:59PM on their due date (generally Thursdays). I will post the “key” within 3 days of the due date and will not accept submissions after the key is posted. If you turn the assignment in on-time with the required number of commits, you’ll recieve full credit.\nAssignment Revisions (50pts x 5): We will have five “assignment revisions” due during the course. These provide an opportunity for me to check in and see how things are going. You’ll be able to update your responses to the homeworks based on the keys and reflect on what you’ve learned throughout the course of the assignments. You’ll also be able to provide additional feedback on how the course is going for you. Rather than assign arbitrary points to each assignment, I’m going to grade your assignment revisions using the following ‘levels’ (inspired by Sarah K. Johnson’s description of her graduate data analysis course at Tufts):\n\nPlease Resubmit: This indicates that either your code does not run as written (i.e., your Quarto document will not compile on my computer), you did not use Git as instructed, and/or that your responses to the questions I posed indicate that you do not quite understand the material as well as I would like. You’ll need to schedule an appointment to talk with me and we’ll work out what you need to do to get credit for the assignment. Although there isn’t a hard deadline for this resubmission, the assignments build on each other so it’s in your best interest to complete the resubmission before you get to the next assessment. Failure to resubmit will result in no credit for the assessment.\nResubmit If You Like: This indicates that all of the code works as written and that you used Git, but that you may have missed some important concepts. Your are welcome to resubmit the assignment and address my comments to help polish the final product, but it is not required for you to get credit for the assignment.\nGood To Go: All of your code works, you completed the necessary Git steps, and all of the pieces are there and polished. I may have some minor comments, but I don’t need you to address them for this assignment.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can miss 1 assignment revision and still receive full-credit for this portion of the course.\n\n\nFinal Project (150pts):  The final project asks you to conduct an entire spatial analysis from layout to results. Grades on the final project are based on your objectives and your self-assessment of whether or not you achieved those objectives.\n\n\n\n\n\n\nFinal Project Deadlines and Points\n\n\n\nPoints for the project are broken down by:\n\nGrading Contract Submitted (October 1): 10 pts (full credit if submitted)\nProposed Project Description (November 1): 15 pts (full credit if submitted)\nFirst Draft Submitted (December 9): 25 pts (full credit if submitted)\nFinal Draft Submitted with Self Evaluation (December 19): 100 pts (based on contract grade and self-evaluation)\n\n\n\nYou can find descriptions for all the assignments on the assignments page.\n\nGrades\n\n\n\n\n\nGrade\nTotal Points\n\n\n\n\nA\n517\n\n\nA-\n500\n\n\nB+\n484\n\n\nB\n461\n\n\nB-\n445\n\n\nC+\n428\n\n\nC\n406\n\n\nC-\n389\n\n\nD+\n373\n\n\nD\n350\n\n\n\n\n\n\n\n\n\nAttendance and incomplete assignments\nAttendance is an important part of this course. You are allowed to miss 2 classes without providing any justification (stuff happens). Beyond that, each additional absence will result in a 0.5 grading reduction (i.e., an A becomes and A-). Similarly, completing the assignments to a satisfactory level is vital to ensure you have a firm grip on the code and concepts. Hence, each assignment that fails to achieve a “Resubmit If You Like” will result in 0.5 grading reduction.\n\n\nLate work\nI would highly recommend staying caught up as much as possible, but if you need to turn something (other than the exercises and final project) in late, that’s fine—there’s no penalty."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAcademic integrity is the principle that asks students to engage with their academic work to the fullest and to behave honestly, transparently, and ethically in every assignment and every interaction with a peer, professor, or research participant. When a strong culture of academic integrity is fostered by students and faculty in an academic program, students learn more, build positive relationships and collaborations, and can feel more confident in the value of their degrees.\nIn order to cultivate fairness and credibility, everyone must participate in upholding academic integrity. Students in this class are responsible for asking for help or clarification when it’s needed, speaking up when they see unethical behavior taking place, and understanding and adhering to the Student Code of Conduct, including the section on academic misconduct. Boise State and I take academic misconduct very seriously. It’s important to know that when a student engages in academic misconduct, I will report the incident to the Office of the Dean of Students. I also have the right to assign sanctions, which could include requirements to revise or redo work, complete educational assignments to learn about academic integrity, and grade penalties ranging from lower credit on an assignment to failing this class1. Students should learn more by reviewing the Student Code of Conduct.\n\nA Note on Using AI in This Course\nThere are a seemingly ever-growing number of AI platforms (e.g., chatgpt, claude, GitHub CoPilot) designed to help you streamline code development (and probably everything else). These tools can help you figure out tricky syntax and make progress when you’re stuck. In my experience, they cannot help you learn to recognize when the code they give you actually does what you want. They are also not particularly good at understanding how your analysis relates to your research questions. This course is about being able to do both and so I offer the following guidelines:\n\n\n\n\n\n\nWarning\n\n\n\nPlease note if you are submitting code generated entirely by an AI agent and tell me which agent you’ve used (they’re not all equally good at different tasks)\n\n\n\nIf you use AI to help you with syntax, it is your responsibility to understand why it works, not just demonstrate that it works.\nEven if you are copying code from an AI-engine, I recommend actually typing the code. This helps you build some “muscle memory” for thinking through programming problems.\n\n\n\n\n\n\n\nImportant\n\n\n\nUsing AI for code is ok, using AI for writing responses to the questions in your assignments or your final project, is not."
  },
  {
    "objectID": "syllabus.html#this-course-was-designed-with-you-in-mind",
    "href": "syllabus.html#this-course-was-designed-with-you-in-mind",
    "title": "Syllabus",
    "section": "This course was designed with you in mind",
    "text": "This course was designed with you in mind\nI developed this course to provide a welcoming environment and effective, equitable learning experience for all students. If you encounter barriers in this course, please bring them to my attention so that I may work to address them.\nIf you tell me you’re having trouble, I will not judge you or think less of you. I hope you’ll extend me the same grace.\nYou never owe me personal information about your health (mental or physical). You are always welcome to talk to me about things that you’re going through, though. If I can’t help you, I usually know somebody who can.\nIf you need extra help, or if you need more time with something, or if you feel like you’re behind or not understanding everything, do not suffer in silence! Talk to me! I will work with you. I promise.\nIf you are struggling for any reason (COVID, relationship, family, or life’s stresses) and believe these may impact your performance in the course, I encourage you to contact the Dean of Students at (208) 426-1527 or email deanofstundents@boisestate.edu for support. If you notice a significant change in your mood, sleep, feelings of hopelessness or a lack of self worth, consider connecting immediately with Counseling Services (1529 Belmont Street, Norco Building) at (208) 426-1459 or email healthservices@boisestate.edu.\n\nThis class’s community is inclusive.\nStudents in this class represent a rich variety of backgrounds and perspectives. The Human-Environment Systems group is committed to providing an atmosphere for learning that respects diversity and creates inclusive environments in our courses. While working together to build this community, we ask all members to: * share their unique experiences, values, and beliefs, if comfortable doing so.\n\nlisten deeply to one another.\nhonor the uniqueness of their peers.\nappreciate the opportunity we have to learn from each other in this community.\nuse this opportunity together to discuss ways in which we can create an inclusive environment in this course and across the campus community.\nrecognize opportunities to invite a community member to exhibit more inclusive, equitable speech or behavior—and then also invite them into further conversation. We also expect community members to respond with gratitude and to take a moment of reflection when they receive such an invitation, rather than react immediately from defensiveness.\nkeep confidential any discussions that the community has of a personal (or professional) nature, unless the speaker has given explicit permission to share what they have said.\nrespect the right of students to be addressed and referred to by the names and pronouns that correspond to their gender identities, including the use of non-binary pronouns.\n\n\n\nWe use each other’s preferred names and pronouns.\nI will ask you to let me know your preferred or adopted name and gender pronoun(s), and I will make those changes to my own records and address you that way in all cases.\nTo change to a preferred name so that it displays on all BSU sites, including Canvas and our course roster, contact the Registrar’s Office at (208) 426-4249. Note that only a legal name change can alter your name on BSU official and legal documents (e.g., your transcript).\n\n\nThis course is accessible to students with disabilities.\nI recognize that navigating your education and life can often be more difficult if you have disabilities. I want you to achieve at your highest capacity in this class. If you have a disability, I need to know if you encounter inequitable opportunities in my course related to:\n\naccessing and understanding course materials engaging with course materials and other students in the course\ndemonstrating your skills and knowledge on assignments and exams.\n\nIf you have a documented disability, you may be eligible for accommodations in all of your courses. To learn more, make an appointment with the university’s Educational Access Center.\n\n\nFor students responsible for children\nI recognize the unique challenges that can arise for students who are also parents or guardians of children. Any student needing to temporarily bring children or another dependent to class is welcome to do so to stay engaged with the class."
  },
  {
    "objectID": "syllabus.html#note-on-course-content-and-idaho-law",
    "href": "syllabus.html#note-on-course-content-and-idaho-law",
    "title": "Syllabus",
    "section": "Note on Course Content and Idaho Law",
    "text": "Note on Course Content and Idaho Law\nUnder Idaho law (Section § 67-5909D), some university courses with content related to diversity, equity, inclusion, or critical theory may be subject to certain restrictions. However, the law affirms and does not limit free discussion in the classroom. Like all Boise State courses, this course supports open inquiry, intellectual honesty, and respectful engagement with a range of perspectives, all of which are consistent with student rights and responsibilities described in the Student Code of Conduct (Policy 2020).\nThis course may include content that touches on concepts related to diversity, equity, inclusion (DEI), or critical theory—such as systemic inequality, cultural identity, or gender and race in society. If these topics are included, it is because they are relevant to the learning outcomes for this course and are explored to support critical thinking, deeper understanding, and respectful engagement with different perspectives. As part of the course, you may be asked to apply or explain ideas that come from a particular perspective. However, you are not required to adopt such perspectives as your own.\nOur classroom is a space for open dialogue and thoughtful discussion, including complex or challenging topics. Everyone is expected to engage with curiosity, listen respectfully, and contribute in ways that support a productive and welcoming learning environment. Boise State and the Idaho State Board of Education affirm the importance of free expression and academic inquiry. As outlined in SBOE Policy III.B: “Membership in the academic community imposes on administrators, faculty members, other institutional employees, and students an obligation to respect the dignity of others, to acknowledge the right of others to express differing opinions, and to foster and defend intellectual honesty, freedom of inquiry and instruction, and free expression on and off the campus of an institution.” Disruptive behavior that interferes with the learning environment will not be tolerated and may result in removal from this course, in line with university policy (See Policy 3240 Maintaining Effective Learning Environments).\nIn this course, I will foster critical discussion and analysis, and a respectful consideration of a wide range of ideas, in accordance with the Faculty Code of Rights, Responsibilities, and Conduct (Policy 4000). You are encouraged to think critically, question ideas, and form your own conclusions. As always, you have the freedom to choose courses that align with your academic goals—if you have concerns about course content, please talk with your instructor or advisor. Refer to the academic calendar for important deadlines related to course withdrawal."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo seriously, just don’t cheat or plagiarize!↩︎"
  },
  {
    "objectID": "slides/05-slides.html#what-is-literate-programming",
    "href": "slides/05-slides.html#what-is-literate-programming",
    "title": "Data Manipulation with the tidyverse",
    "section": "What is literate programming?",
    "text": "What is literate programming?\n\nDocumentation containing code (not vice versa!)\nDirect connection between code and explanation\nConvey meaning to humans rather than telling computer what to do!"
  },
  {
    "objectID": "slides/05-slides.html#what-is-a-script",
    "href": "slides/05-slides.html#what-is-a-script",
    "title": "Data Manipulation with the tidyverse",
    "section": "What is a script?",
    "text": "What is a script?\n\n\n\nClarity\nAutomation\nMinimal Documentation\n\n\n\n\n\n\nIf it’s for a computer, it’s great for a script. If it’s for a human, we need more."
  },
  {
    "objectID": "slides/05-slides.html#pseudocode",
    "href": "slides/05-slides.html#pseudocode",
    "title": "Data Manipulation with the tidyverse",
    "section": "Pseudocode",
    "text": "Pseudocode\n\nAn informal way of writing the ‘logic’ of your program\nBalance between readability and precision\nAvoid syntactic drift"
  },
  {
    "objectID": "slides/05-slides.html#writing-pseudocode",
    "href": "slides/05-slides.html#writing-pseudocode",
    "title": "Data Manipulation with the tidyverse",
    "section": "Writing pseudocode",
    "text": "Writing pseudocode\n\n\n\nFocus on statements\nMathematical operations\nConditionals\nIteration\nExceptions"
  },
  {
    "objectID": "slides/05-slides.html#pseudocode-1",
    "href": "slides/05-slides.html#pseudocode-1",
    "title": "Data Manipulation with the tidyverse",
    "section": "Pseudocode",
    "text": "Pseudocode\n\nStart function\nInput information\nLogical test: if TRUE\n  (what to do if TRUE)\nelse\n  (what to do if FALSE)\nEnd function"
  },
  {
    "objectID": "slides/05-slides.html#why-care-about-style",
    "href": "slides/05-slides.html#why-care-about-style",
    "title": "Data Manipulation with the tidyverse",
    "section": "Why care about style?",
    "text": "Why care about style?\n\n\nEasier for you, future you, and others to read\nEasier to get help"
  },
  {
    "objectID": "slides/05-slides.html#what-why",
    "href": "slides/05-slides.html#what-why",
    "title": "Data Manipulation with the tidyverse",
    "section": "What? Why?",
    "text": "What? Why?\n\nA group of task-specific packages built on shared grammar\nReduced dependencies on other packages\nConsistent logic and shared style\nAllows us to code “out loud”"
  },
  {
    "objectID": "slides/05-slides.html#dplyr-and-a-grammar-for-data-transformation",
    "href": "slides/05-slides.html#dplyr-and-a-grammar-for-data-transformation",
    "title": "Data Manipulation with the tidyverse",
    "section": "dplyr and a grammar for data transformation",
    "text": "dplyr and a grammar for data transformation\n\nfunctions as verbs\nfunctions work on and return data frames\nfirst argument is always a data frame\nsubsequent arguments say what to do with it"
  },
  {
    "objectID": "slides/05-slides.html#basic-structure-verbs",
    "href": "slides/05-slides.html#basic-structure-verbs",
    "title": "Data Manipulation with the tidyverse",
    "section": "Basic structure: Verbs",
    "text": "Basic structure: Verbs\n\nselect: pick columns by name\narrange: reorder rows\nslice: pick rows using index(es)\nfilter: pick rows matching criteria\ndistinct: filter for unique rows\nmutate: add new variables\nsummarise: reduce variables to values\ngroup_by: for grouped operations\n… (many more)"
  },
  {
    "objectID": "slides/05-slides.html#basic-structure-helpers",
    "href": "slides/05-slides.html#basic-structure-helpers",
    "title": "Data Manipulation with the tidyverse",
    "section": "Basic structure: Helpers",
    "text": "Basic structure: Helpers\n\nCan be combined with select to apply verbs to multiple columns\n\n\nstarts_with(): Starts with a prefix\ncontains(): Contains a literal string\none_of(): Matches variable names in a character vector\neverything(): Matches all variables\nlast_col(): Select last variable, possibly with an offset"
  },
  {
    "objectID": "slides/05-slides.html#basic-structure-pipes",
    "href": "slides/05-slides.html#basic-structure-pipes",
    "title": "Data Manipulation with the tidyverse",
    "section": "Basic structure: Pipes",
    "text": "Basic structure: Pipes\nIn programming, a pipe is a technique for passing information from one process to another.\n\nYou can think about the following sequence of actions - find keys, unlock car, start car, drive to work, park.\nExpressed as a set of nested functions in R pseudocode this would look like:\n\n\npark(drive(start_car(find(\"keys\")), to = \"work\"))"
  },
  {
    "objectID": "slides/05-slides.html#basic-structure-pipes-1",
    "href": "slides/05-slides.html#basic-structure-pipes-1",
    "title": "Data Manipulation with the tidyverse",
    "section": "Basic structure: Pipes",
    "text": "Basic structure: Pipes\n\nYou can think about the following sequence of actions - find keys, unlock car, start car, drive to work, park.\nWriting it out using pipes give it a more natural (and easier to read) structure:\n\n\nfind(\"keys\") %&gt;%\n  start_car() %&gt;%\n  drive(to = \"work\") %&gt;%\n  park()"
  },
  {
    "objectID": "slides/05-slides.html#extensions",
    "href": "slides/05-slides.html#extensions",
    "title": "Data Manipulation with the tidyverse",
    "section": "Extensions",
    "text": "Extensions\n\ntidyr for cleaning and reshaping data\ndplyr::xxxx_join for combining data"
  },
  {
    "objectID": "slides/05-slides.html#scripts-and-your-workflow",
    "href": "slides/05-slides.html#scripts-and-your-workflow",
    "title": "Data Manipulation with the tidyverse",
    "section": "Scripts and your workflow",
    "text": "Scripts and your workflow\n\nAnalysis = many functions, many scripts\nLiterate document (Quarto) focuses on communication\nIntegrates inputs and outputs alongside code\nFocus is on research questions, decisions, and interpretation"
  },
  {
    "objectID": "slides/02-slides.html#checking-in",
    "href": "slides/02-slides.html#checking-in",
    "title": "Tools of the Trade",
    "section": "Checking in",
    "text": "Checking in\n\nWhat can I clarify about the course?\nAre there any challenges you can already see?"
  },
  {
    "objectID": "slides/02-slides.html#todays-plan",
    "href": "slides/02-slides.html#todays-plan",
    "title": "Tools of the Trade",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nOpen Science, reproducibility, and R\nWhat is a (spatial) data workflow?\nVersion control for fun and profit"
  },
  {
    "objectID": "slides/02-slides.html#what-is-open-science",
    "href": "slides/02-slides.html#what-is-open-science",
    "title": "Tools of the Trade",
    "section": "What is open science?",
    "text": "What is open science?\n\n\n\n\nScientific research and its outcomes freely accessible to all\nAccelerate research AND improve public trust\nOur focus: Open source software and code\n\n\n\n\n\n\n\nUNESCO.org, CC BY-SA 4.0, via Wikimedia Commons"
  },
  {
    "objectID": "slides/02-slides.html#why-open-source-software-and-code",
    "href": "slides/02-slides.html#why-open-source-software-and-code",
    "title": "Tools of the Trade",
    "section": "Why open source software and code?",
    "text": "Why open source software and code?\n\nFuture-proof: OSS development is fast and ongoing\nInteroperability: Work across hardware types, integrate new software easily\nFree!! (To use and maintain)\nSharing code and data enables innovation and reproducibility"
  },
  {
    "objectID": "slides/02-slides.html#why-not-r",
    "href": "slides/02-slides.html#why-not-r",
    "title": "Tools of the Trade",
    "section": "Why (not) R?",
    "text": "Why (not) R?\n\n\n\n\n\nOpen Source\nHuge useR community\nIntegrated analysis pipelines\nReproducible workflows\n\n\n\n\n\nCoding can be hard…\nMemory challenges\nSpeed\nDecision fatigue"
  },
  {
    "objectID": "slides/02-slides.html#anatomy-of-an-r-session",
    "href": "slides/02-slides.html#anatomy-of-an-r-session",
    "title": "Tools of the Trade",
    "section": "Anatomy of an R session",
    "text": "Anatomy of an R session\n\n\n\n\nMoving beyond Read-Eval-Print Loops\nscripts: contain a record of the code in your analysis and the objects you created\nfunctions: perform operations on objects\npackages: collections of related functions\n\n\n\n\n\nCodePlot\n\n\n\nlibrary(maps)\nlibrary(socviz)\nlibrary(tidyverse)\nparty_colors &lt;- c(\"#2E74C0\", \"#CB454A\") \nus_states &lt;- map_data(\"state\")\nelection$region &lt;- tolower(election$state)\nus_states_elec &lt;- left_join(us_states, election)\np0 &lt;- ggplot(data = us_states_elec,\n             mapping = aes(x = long, y = lat,\n                           group = group, \n                           fill = party))\np1 &lt;- p0 + geom_polygon(color = \"gray90\", \n                        size = 0.1) +\n    coord_map(projection = \"albers\", \n              lat0 = 39, lat1 = 45) \np2 &lt;- p1 + scale_fill_manual(values = party_colors) +\n    labs(title = \"Election Results 2016\", \n         fill = NULL)"
  },
  {
    "objectID": "slides/02-slides.html#why-do-we-need-reproducibility",
    "href": "slides/02-slides.html#why-do-we-need-reproducibility",
    "title": "Tools of the Trade",
    "section": "Why Do We Need Reproducibility?",
    "text": "Why Do We Need Reproducibility?\n\n\n\n\nNoise!!\nConfirmation bias\nHindsight bias\n\n\n\n\n\n\n\n\n\n\nMunafo et al. 2017. Nat Hum Beh."
  },
  {
    "objectID": "slides/02-slides.html#what-do-we-mean-by-reproducible-workflow",
    "href": "slides/02-slides.html#what-do-we-mean-by-reproducible-workflow",
    "title": "Tools of the Trade",
    "section": "What do we mean by reproducible “workflow”?",
    "text": "What do we mean by reproducible “workflow”?"
  },
  {
    "objectID": "slides/02-slides.html#reproducibility-and-your-code",
    "href": "slides/02-slides.html#reproducibility-and-your-code",
    "title": "Tools of the Trade",
    "section": "Reproducibility and your code",
    "text": "Reproducibility and your code\n\nScripts: may make your code reproducible (but not your analysis)\nCommenting and formatting can help!\nThink about future you…\n\n\n```{r}\n#| eval: false\n## load the packages necessary\nlibrary(tidyverse)\n## read in the data\nlandmarks_csv &lt;- read_csv(\"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2023/assignment01/landmarks_ID.csv\")\n\n## How many in each feature class\ntable(landmarks_csv$MTFCC)\n```"
  },
  {
    "objectID": "slides/02-slides.html#reproducible-scripts",
    "href": "slides/02-slides.html#reproducible-scripts",
    "title": "Tools of the Trade",
    "section": "Reproducible scripts",
    "text": "Reproducible scripts\n\nComments explain what the code is doing\nOperations are ordered logically\nOnly relevant commands are presented\nUseful object and function names\nScript runs without errors (on your machine and someone else’s)"
  },
  {
    "objectID": "slides/02-slides.html#toward-efficient-reproducible-workflows",
    "href": "slides/02-slides.html#toward-efficient-reproducible-workflows",
    "title": "Tools of the Trade",
    "section": "Toward Efficient Reproducible Workflows",
    "text": "Toward Efficient Reproducible Workflows\n\nScripts can document what you did, but not why you did it!\nScripts separate your analysis products from your report/manuscript"
  },
  {
    "objectID": "slides/02-slides.html#what-is-literate-programming",
    "href": "slides/02-slides.html#what-is-literate-programming",
    "title": "Tools of the Trade",
    "section": "What is literate programming?",
    "text": "What is literate programming?\n\nDocumentation containing code (not vice versa!)\nDirect connection between code and explanation\nConvey meaning to humans rather than telling computer what to do!"
  },
  {
    "objectID": "slides/02-slides.html#why-literate-programming",
    "href": "slides/02-slides.html#why-literate-programming",
    "title": "Tools of the Trade",
    "section": "Why literate programming?",
    "text": "Why literate programming?\n\nYour analysis scripts are computer software\nIntegrate math, figures, code, and narrative in one place\nExplaining something helps you learn it"
  },
  {
    "objectID": "slides/02-slides.html#what-is-quarto",
    "href": "slides/02-slides.html#what-is-quarto",
    "title": "Tools of the Trade",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\n\n\nEnd-to-End process between data and report\nExplicit linkage between each step (including iteration)\nEach step involves trials and choices"
  },
  {
    "objectID": "slides/02-slides.html#what-is-quarto-1",
    "href": "slides/02-slides.html#what-is-quarto-1",
    "title": "Tools of the Trade",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\nA multi-language platform for developing reproducible documents\nA ‘lab notebook’ for your analyses\nAllows transparent, reproducible scientific reports and presentations"
  },
  {
    "objectID": "slides/02-slides.html#key-components",
    "href": "slides/02-slides.html#key-components",
    "title": "Tools of the Trade",
    "section": "Key components",
    "text": "Key components\n\nMetadata and global options: YAML\nText, figures, and tables: Markdown and LaTeX\nCode: knitr (or jupyter if you’re into that sort of thing)"
  },
  {
    "objectID": "slides/02-slides.html#for-this-class",
    "href": "slides/02-slides.html#for-this-class",
    "title": "Tools of the Trade",
    "section": "For this class…",
    "text": "For this class…\n\nWe’ll use headers to outline the analysis\nWe’ll use code chunks for small, self-contained operations\nWe’ll create our own functions for repeated operations\nWe’ll knit our documents into a standalone, readable document"
  },
  {
    "objectID": "slides/02-slides.html#version-control-in-general",
    "href": "slides/02-slides.html#version-control-in-general",
    "title": "Tools of the Trade",
    "section": "Version control in general",
    "text": "Version control in general\n\nTrack changes without version explosion (via git)\nCreate specific snapshots of a project to facilitate experimentation (via commit and branches)\nCreate centralized backups and ease collaboration (via GitHub)"
  },
  {
    "objectID": "slides/02-slides.html#version-control-and-reproducibility",
    "href": "slides/02-slides.html#version-control-and-reproducibility",
    "title": "Tools of the Trade",
    "section": "Version control and reproducibility",
    "text": "Version control and reproducibility\n\nDocumenting changes to code, manuscripts, figures increases transparency of the scientific process\nCollaboration with other programmers is easier and less risky\nAutomates the sharing of code and original data"
  },
  {
    "objectID": "slides/02-slides.html#version-control-and-sanity",
    "href": "slides/02-slides.html#version-control-and-sanity",
    "title": "Tools of the Trade",
    "section": "Version control and sanity",
    "text": "Version control and sanity\n\ncommit early, commit often\nuse sensible messages to remind yourself where you were\nmake sure you always have the most up-to-date version\nIt will take some practice to git comfortable"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester!\n\nContent (): This page contains the readings, slides, and recorded lectures for the week. Read the various readings before our scheduled class time.\nLesson (): This page contains a tutorial video and additional annotated R code and other supplementary information that you can use to help you prepare for the in-class exercises and assignments. These are most helpful if you watch them before in-class sessions so that you can ask questions when we start working through things together.\nExercises (): This page the scripts that we work on in class as a reminder of some of the live-coding exercises. These are provided as a reference to help you link your notes to the syntax we use in class.\nAssignment (): This page contains the instructions for each assignment. Assignments are due by 11:59 PM on the day they’re listed. One more time!\n\n\n\n\n\n\n\nSubscribe!\n\n\n\nYou can subscribe to this calendar URL in Outlook, Google Calendar, or Apple Calendar:\n\n\n\n Download\n\n\n\n\n\n\n\n\n\n\n\nGetting Started\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExercises\n\n\nAssignment\n\n\n\n\n\n\nAugust 25(Session 1)\n\n\nIntroduction to the Course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 27(Session 2)\n\n\nTools of the Trade\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 29\n\n\n Self-Evaluation 1 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 1\n\n\nNo Class(Labor Day)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 3(Session 4)\n\n\nMeeting Your data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 8(Session 5)\n\n\nData Manipulation with the tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 10(Session 6)\n\n\nPipes, Functions, and Iteration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 11\n\n\n Homework 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentals of Spatial Data\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExercises\n\n\nAssignment\n\n\n\n\n\n\nSeptember 15(Session 7)\n\n\nData Models, Coordinates, and Geometries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 17(Session 8)\n\n\nProjections, Extent, and Resolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 18\n\n\n Homework 1 Revision\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 22(Session 9)\n\n\nAttribute Data Operations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 24(Session 10)\n\n\nSpatial Data Operations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 29(Session 11)\n\n\nGeometry Data Operations and Troubleshooting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 1(Session 12)\n\n\nBuilding Spatial Databases\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 1\n\n\n Final Project Contract Due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 2\n\n\n Homework 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescribing Spatial Patterns\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExercises\n\n\nAssignment\n\n\n\n\n\n\nOctober 6(Session 13)\n\n\nPoint Patterns I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 8(Session 14)\n\n\nPoint Patterns II(remote)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 9\n\n\n Homework 2 Revision\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 13(Session 15)\n\n\nSurface Metrics I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 15(Session 16)\n\n\nSurface Metrics II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 20(Session 17)\n\n\nMultivariate Description I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 22(Session 18)\n\n\nMultivariate Description II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 23\n\n\n Homework 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplaining Spatial Patterns\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExercises\n\n\nAssignment\n\n\n\n\n\n\nOctober 27(Session 19)\n\n\nInterpolation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 29(Session 20)\n\n\nKriging\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 30\n\n\n Homework 3 Revision\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 1\n\n\n Final Project Description Due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 3(Session 21)\n\n\nMachine Learning Models I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 5(Session 22)\n\n\nMachine Learning Models II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 10(Session 23)\n\n\nStatistical Modelling I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 12(Session 24)\n\n\nStatistical Modelling II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 17(Session 25)\n\n\nSpatial Autocorrelation I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 19(Session 26)\n\n\nSpatial Autocorrelation II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 20\n\n\n Homework 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFall Break\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExercises\n\n\nAssignment\n\n\n\n\n\n\nNovember 24\n\n\nNo Class(Fall Break)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 26\n\n\nNo Class(Fall Break)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Spatial Patterns\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExercises\n\n\nAssignment\n\n\n\n\n\n\nDecember 1(Session 29)\n\n\nSpatial Predictions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 2\n\n\n Homework 4 Revision\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 3(Session 30)\n\n\nModel Evaluation I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 8(Session 31)\n\n\nModel Evaluation II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 9\n\n\n Homework 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrapup\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExercises\n\n\nAssignment\n\n\n\n\n\n\nDecember 9\n\n\n Draft Final Project Due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 10(Session 32)\n\n\nConclusion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 11\n\n\n Homework 5 Revision\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 12\n\n\nFinal Project Workday(optional)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 18\n\n\n Final Project Due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 19\n\n\n Final Self-Evaluation Due  (submit by 11:59 PM)"
  },
  {
    "objectID": "resource/r.html",
    "href": "resource/r.html",
    "title": "Getting started with R Spatial",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nThese resources are also really really helpful:\n\n\n\nAn Introduction to R: The definitive introductory text by Venables, Smith, and the R Core Team.\nSwirl: A set of free, embed-resources tutorials that run from within your RStudio terminal.\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online.\nCSE 631: Principles & Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio.\n\n\n\n\n\nsf cheatsheet: An at-a-glance description of the various sf verbs and their application.\nGeocomputation with R: Online version of the textbook by Lovelace, Nowosad, and Muenchow.\n[Robert’s Page]"
  },
  {
    "objectID": "resource/r.html#learning-r",
    "href": "resource/r.html#learning-r",
    "title": "Getting started with R Spatial",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nThese resources are also really really helpful:\n\n\n\nAn Introduction to R: The definitive introductory text by Venables, Smith, and the R Core Team.\nSwirl: A set of free, embed-resources tutorials that run from within your RStudio terminal.\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online.\nCSE 631: Principles & Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio.\n\n\n\n\n\nsf cheatsheet: An at-a-glance description of the various sf verbs and their application.\nGeocomputation with R: Online version of the textbook by Lovelace, Nowosad, and Muenchow.\n[Robert’s Page]"
  },
  {
    "objectID": "resource/install.html",
    "href": "resource/install.html",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "",
    "text": "You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code."
  },
  {
    "objectID": "resource/install.html#rstudio-on-your-computer",
    "href": "resource/install.html#rstudio-on-your-computer",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "RStudio on your computer",
    "text": "RStudio on your computer\nRStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\n\nInstall R\nFirst you need to install R itself (the engine).\n\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\nClick on “Download R for XXX”, where XXX is either Mac or Windows:\n\nIf you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is 4.2.1) and download it.\nIf you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n\nDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\nIf you use macOS, download and install XQuartz. You do not need to do this on Windows.\n\n\n\nInstall RStudio\nNext, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\n\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\nThe website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\nDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\nDouble click on RStudio to run it (check your applications folder or start menu).\n\n\nInstall tidyverse\nThe tidyverse consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nYou can install packages manually in RStudio, but this can be a bit fragile, especially for some of the spatial packages. Instead of using the RStudio GUI we’ll just install thins at the prompt. To install the tidyverse pacakge (and all of its associated dependencies) run the following: install.packages(\"tidyverse\")."
  },
  {
    "objectID": "resource/install.html#footnotes",
    "href": "resource/install.html#footnotes",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎"
  },
  {
    "objectID": "resource/git.html",
    "href": "resource/git.html",
    "title": "Helpful git links",
    "section": "",
    "text": "Getting in the habit of using version control can be challenging, especially if you are collaborating with others. The challenge gets worse when some of those collaborators are not familiar with the importance of version control. Here are a few links to try and make your (and their) transition a little smoother."
  },
  {
    "objectID": "resource/git.html#installing-git-and-making-it-play-nice-with-r",
    "href": "resource/git.html#installing-git-and-making-it-play-nice-with-r",
    "title": "Helpful git links",
    "section": "Installing Git and making it play nice with R",
    "text": "Installing Git and making it play nice with R\nHappy git with R is Jenny Bryan’s: extremely helpful introduction to git and incorporating it into your R workflow."
  },
  {
    "objectID": "resource/git.html#getting-the-hang-of-git",
    "href": "resource/git.html#getting-the-hang-of-git",
    "title": "Helpful git links",
    "section": "Getting the hang of git",
    "text": "Getting the hang of git\nUnderstanding the logic of git: provides a relatively accessible explanation of the various operations in git and links that to commonly used syntax.\nOh Sh@t, Git?!?: A less technical, more irreverant introduction to git workflows and fixing the inevitable challenges of version control. (G-rated version available at Dang it, Git?!?)."
  },
  {
    "objectID": "resource/dataclasses.html",
    "href": "resource/dataclasses.html",
    "title": "Data Structures",
    "section": "",
    "text": "Okay, now that we have all of those details out of the way, let’s take a look at data structures in R. As we discussed,R has six basic types of data: numeric, integer, logical, complex, character, and raw. For this class, we won’t bother with complex or raw as you are unlikely to encounter them in your introductory spatial explorations.\n\nNumeric data are numbers that contain a decimal. They can also be whole numbers\nIntegers are whole numbers (those numbers without a decimal point).\nLogical data take on the value of either TRUE or FALSE. There’s also another special type of logical called NA to represent missing values.\nCharacter data represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). Factors become important in the analyses and visualizations we’ll attempt later in the course.\n\nThere are a variety of ways to learn more about the structure of different data types:\n\nclass() - returns the type of object (high level)\ntypeof() - returns the type of object (low level)\nlength() tells you about the length of an object\nattributes() - does the object have any metadata\n\n\nnum &lt;- 2.2\nclass(num)\n\n[1] \"numeric\"\n\ntypeof(num)\n\n[1] \"double\"\n\ny &lt;- 1:10 \ny\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nclass(y)\n\n[1] \"integer\"\n\ntypeof(y)\n\n[1] \"integer\"\n\nlength(y)\n\n[1] 10\n\nb &lt;- \"3\"\nclass(b)\n\n[1] \"character\"\n\nis.numeric(b)\n\n[1] FALSE\n\nc &lt;- as.numeric(b)\nclass(c)\n\n[1] \"numeric\"\n\n\n\n\n\nYou can store information in a variety of ways in R. The types we are most likely to encounter this semester are:\n\nVectors: a collection of elements that are typically character, logical, integer, or numeric.\n\n\n#sometimes we'll need to make sequences of numbers to facilitate joins\nseries &lt;- 1:10\nseries.2 &lt;- seq(10)\nseries.3 &lt;- seq(from = 1, to = 10, by = 0.1)\nseries\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseries.2\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseries.3\n\n [1]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4\n[16]  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9\n[31]  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4\n[46]  5.5  5.6  5.7  5.8  5.9  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9\n[61]  7.0  7.1  7.2  7.3  7.4  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4\n[76]  8.5  8.6  8.7  8.8  8.9  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9\n[91] 10.0\n\nc(series.2, series.3)\n\n  [1]  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0 10.0  1.0  1.1  1.2  1.3  1.4\n [16]  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9\n [31]  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4\n [46]  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9\n [61]  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.1  7.2  7.3  7.4\n [76]  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9\n [91]  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9 10.0\n\nclass(series.3)\n\n[1] \"numeric\"\n\ntypeof(series.3)\n\n[1] \"double\"\n\nlength(series.3)\n\n[1] 91\n\n\n\nMissing Data: R supports missing data in most of the data structures we use, but they can lead to some strange behaviors. Here are a few ways to find missing data:\n\n\nx &lt;- c(\"a\", NA, \"c\", \"d\", NA)\nis.na(x)\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\nanyNA(x)\n\n[1] TRUE\n\n\n\nMatrices: are an extension of the numeric or character vectors. They are not a separate type of object but simply an atomic vector with dimensions; the number of rows and columns. As with atomic vectors, the elements of a matrix must be of the same data. Matrices are the foundation of rasters, which we’ll be discussing frequently throughout the course\n\n\n#matrices are filled columnwise in R\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\ndim(m)\n\n[1] 2 3\n\nx &lt;- 1:3\ny &lt;- 10:12\n\na &lt;- cbind(x, y)\ndim(a)\n\n[1] 3 2\n\na[3,1]\n\nx \n3 \n\nb &lt;- rbind(x, y)\ndim(b)\n\n[1] 2 3\n\nb[1,3]\n\nx \n3 \n\n\n\nLists: Lists essentially act like containers in R - they can hold a variety of different data types and structures including more lists. We use lists a lot for functional programming in R where we can apply a function to each element in a list. We’ll see this with extracting values from multiple rasters. We can extract elements of lists usin [] and [[]]\n\n\nx &lt;- list(1, \"a\", TRUE, 1+4i)\nx\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n#adding names\nxlist &lt;- list(a = \"Waldo\", b = 1:10, data = head(mtcars))\nxlist\n\n$a\n[1] \"Waldo\"\n\n$b\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nxlist[[1]]\n\n[1] \"Waldo\"\n\nxlist[[3]]\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nxlist[[3]][1]\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\nValiant           18.1\n\nxlist[[3]][1,2]\n\n[1] 6\n\nxlist[3][1]\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nData Frames: data frames resemble that tabular datasets you might be used to in spreadsheet programs and are probably one of the most common types of data in R. A data frame is a special type of list where every element has the same length (but can have different types of data). We’ll be reading in a number of data frames for this first assignment.\n\n\ndat &lt;- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nis.list(dat)\n\n[1] TRUE\n\nclass(dat)\n\n[1] \"data.frame\"\n\n#lots of ways to look at data in data frames\nstr(dat) #compact summary of the structure of a dataframe\n\n'data.frame':   10 obs. of  3 variables:\n $ id: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ x : int  1 2 3 4 5 6 7 8 9 10\n $ y : int  11 12 13 14 15 16 17 18 19 20\n\nhead(dat) #gives the first 6 rows similar to tail()\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ndim(dat)\n\n[1] 10  3\n\ncolnames(dat)\n\n[1] \"id\" \"x\"  \"y\" \n\n## accessing elements of a dataframe\ndat[1,3]\n\n[1] 11\n\ndat[[\"y\"]]\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat$y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\n\nTibbles: are similar to data frames, but allow for lists within columns. They are designed for use with the tidyverse (which we’ll explore more in future classes), but the primary reason for introducing them here is because they are the foundation of sf objects which we’ll use frequently in the weeks to come.\n\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndat.tib &lt;- tibble(dat)\nis.list(dat.tib)\n\n[1] TRUE\n\nclass(dat.tib)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n#lots of ways to look at data in data frames\nstr(dat.tib) #compact summary of the structure of a dataframe\n\ntibble [10 × 3] (S3: tbl_df/tbl/data.frame)\n $ id: chr [1:10] \"a\" \"b\" \"c\" \"d\" ...\n $ x : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ y : int [1:10] 11 12 13 14 15 16 17 18 19 20\n\nhead(dat.tib) #gives the first 6 rows similar to tail()\n\n# A tibble: 6 × 3\n  id        x     y\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 a         1    11\n2 b         2    12\n3 c         3    13\n4 d         4    14\n5 e         5    15\n6 f         6    16\n\ndim(dat.tib)\n\n[1] 10  3\n\ncolnames(dat.tib)\n\n[1] \"id\" \"x\"  \"y\" \n\n## accessing elements of a dataframe\ndat.tib[1,3]\n\n# A tibble: 1 × 1\n      y\n  &lt;int&gt;\n1    11\n\ndat.tib[[\"y\"]]\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat.tib$y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nMany of the packages used for spatial operations in R rely on special objects (e.g., sf, SpatRasters) that are combinations of these various elemental data types. That is why we are taking a little time to understand them before jumping into spatial data."
  },
  {
    "objectID": "resource/dataclasses.html#data-types-and-structures",
    "href": "resource/dataclasses.html#data-types-and-structures",
    "title": "Data Structures",
    "section": "",
    "text": "Okay, now that we have all of those details out of the way, let’s take a look at data structures in R. As we discussed,R has six basic types of data: numeric, integer, logical, complex, character, and raw. For this class, we won’t bother with complex or raw as you are unlikely to encounter them in your introductory spatial explorations.\n\nNumeric data are numbers that contain a decimal. They can also be whole numbers\nIntegers are whole numbers (those numbers without a decimal point).\nLogical data take on the value of either TRUE or FALSE. There’s also another special type of logical called NA to represent missing values.\nCharacter data represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). Factors become important in the analyses and visualizations we’ll attempt later in the course.\n\nThere are a variety of ways to learn more about the structure of different data types:\n\nclass() - returns the type of object (high level)\ntypeof() - returns the type of object (low level)\nlength() tells you about the length of an object\nattributes() - does the object have any metadata\n\n\nnum &lt;- 2.2\nclass(num)\n\n[1] \"numeric\"\n\ntypeof(num)\n\n[1] \"double\"\n\ny &lt;- 1:10 \ny\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nclass(y)\n\n[1] \"integer\"\n\ntypeof(y)\n\n[1] \"integer\"\n\nlength(y)\n\n[1] 10\n\nb &lt;- \"3\"\nclass(b)\n\n[1] \"character\"\n\nis.numeric(b)\n\n[1] FALSE\n\nc &lt;- as.numeric(b)\nclass(c)\n\n[1] \"numeric\"\n\n\n\n\n\nYou can store information in a variety of ways in R. The types we are most likely to encounter this semester are:\n\nVectors: a collection of elements that are typically character, logical, integer, or numeric.\n\n\n#sometimes we'll need to make sequences of numbers to facilitate joins\nseries &lt;- 1:10\nseries.2 &lt;- seq(10)\nseries.3 &lt;- seq(from = 1, to = 10, by = 0.1)\nseries\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseries.2\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseries.3\n\n [1]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4\n[16]  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9\n[31]  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4\n[46]  5.5  5.6  5.7  5.8  5.9  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9\n[61]  7.0  7.1  7.2  7.3  7.4  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4\n[76]  8.5  8.6  8.7  8.8  8.9  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9\n[91] 10.0\n\nc(series.2, series.3)\n\n  [1]  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0 10.0  1.0  1.1  1.2  1.3  1.4\n [16]  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9\n [31]  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4\n [46]  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9\n [61]  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.1  7.2  7.3  7.4\n [76]  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9\n [91]  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9 10.0\n\nclass(series.3)\n\n[1] \"numeric\"\n\ntypeof(series.3)\n\n[1] \"double\"\n\nlength(series.3)\n\n[1] 91\n\n\n\nMissing Data: R supports missing data in most of the data structures we use, but they can lead to some strange behaviors. Here are a few ways to find missing data:\n\n\nx &lt;- c(\"a\", NA, \"c\", \"d\", NA)\nis.na(x)\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\nanyNA(x)\n\n[1] TRUE\n\n\n\nMatrices: are an extension of the numeric or character vectors. They are not a separate type of object but simply an atomic vector with dimensions; the number of rows and columns. As with atomic vectors, the elements of a matrix must be of the same data. Matrices are the foundation of rasters, which we’ll be discussing frequently throughout the course\n\n\n#matrices are filled columnwise in R\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\ndim(m)\n\n[1] 2 3\n\nx &lt;- 1:3\ny &lt;- 10:12\n\na &lt;- cbind(x, y)\ndim(a)\n\n[1] 3 2\n\na[3,1]\n\nx \n3 \n\nb &lt;- rbind(x, y)\ndim(b)\n\n[1] 2 3\n\nb[1,3]\n\nx \n3 \n\n\n\nLists: Lists essentially act like containers in R - they can hold a variety of different data types and structures including more lists. We use lists a lot for functional programming in R where we can apply a function to each element in a list. We’ll see this with extracting values from multiple rasters. We can extract elements of lists usin [] and [[]]\n\n\nx &lt;- list(1, \"a\", TRUE, 1+4i)\nx\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n#adding names\nxlist &lt;- list(a = \"Waldo\", b = 1:10, data = head(mtcars))\nxlist\n\n$a\n[1] \"Waldo\"\n\n$b\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nxlist[[1]]\n\n[1] \"Waldo\"\n\nxlist[[3]]\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nxlist[[3]][1]\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\nValiant           18.1\n\nxlist[[3]][1,2]\n\n[1] 6\n\nxlist[3][1]\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nData Frames: data frames resemble that tabular datasets you might be used to in spreadsheet programs and are probably one of the most common types of data in R. A data frame is a special type of list where every element has the same length (but can have different types of data). We’ll be reading in a number of data frames for this first assignment.\n\n\ndat &lt;- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nis.list(dat)\n\n[1] TRUE\n\nclass(dat)\n\n[1] \"data.frame\"\n\n#lots of ways to look at data in data frames\nstr(dat) #compact summary of the structure of a dataframe\n\n'data.frame':   10 obs. of  3 variables:\n $ id: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ x : int  1 2 3 4 5 6 7 8 9 10\n $ y : int  11 12 13 14 15 16 17 18 19 20\n\nhead(dat) #gives the first 6 rows similar to tail()\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ndim(dat)\n\n[1] 10  3\n\ncolnames(dat)\n\n[1] \"id\" \"x\"  \"y\" \n\n## accessing elements of a dataframe\ndat[1,3]\n\n[1] 11\n\ndat[[\"y\"]]\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat$y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\n\nTibbles: are similar to data frames, but allow for lists within columns. They are designed for use with the tidyverse (which we’ll explore more in future classes), but the primary reason for introducing them here is because they are the foundation of sf objects which we’ll use frequently in the weeks to come.\n\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndat.tib &lt;- tibble(dat)\nis.list(dat.tib)\n\n[1] TRUE\n\nclass(dat.tib)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n#lots of ways to look at data in data frames\nstr(dat.tib) #compact summary of the structure of a dataframe\n\ntibble [10 × 3] (S3: tbl_df/tbl/data.frame)\n $ id: chr [1:10] \"a\" \"b\" \"c\" \"d\" ...\n $ x : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ y : int [1:10] 11 12 13 14 15 16 17 18 19 20\n\nhead(dat.tib) #gives the first 6 rows similar to tail()\n\n# A tibble: 6 × 3\n  id        x     y\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 a         1    11\n2 b         2    12\n3 c         3    13\n4 d         4    14\n5 e         5    15\n6 f         6    16\n\ndim(dat.tib)\n\n[1] 10  3\n\ncolnames(dat.tib)\n\n[1] \"id\" \"x\"  \"y\" \n\n## accessing elements of a dataframe\ndat.tib[1,3]\n\n# A tibble: 1 × 1\n      y\n  &lt;int&gt;\n1    11\n\ndat.tib[[\"y\"]]\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat.tib$y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nMany of the packages used for spatial operations in R rely on special objects (e.g., sf, SpatRasters) that are combinations of these various elemental data types. That is why we are taking a little time to understand them before jumping into spatial data."
  },
  {
    "objectID": "lesson/vector_intro.html",
    "href": "lesson/vector_intro.html",
    "title": "Intro to Vector Data",
    "section": "",
    "text": "Today we’ll build on the introductory discussion we were having about vector operations and the sf package. We’ll build a few vectors from scratch and then move on to explore a broader suite of common vector operations implemented by the sf package.\nWarning: package 'sf' was built under R version 4.4.1\n\n\nWarning: package 'ggplot2' was built under R version 4.4.1"
  },
  {
    "objectID": "lesson/vector_intro.html#a-reminder-about-vector-geometries-in-r",
    "href": "lesson/vector_intro.html#a-reminder-about-vector-geometries-in-r",
    "title": "Intro to Vector Data",
    "section": "A reminder about vector geometries in R",
    "text": "A reminder about vector geometries in R\nYou’ll recall that the sf package organizes the different types of vectors (e.g., points, lines, polygons) in to a hierarchical structure organized by complexity of geometries contained within an R object. For example, a single point will be a POINT, several points will be a MULTIPOINT, and an object containing points, polygons, and lines will be a GEOMETRYCOLLECTION. We need to be aware of what types of geometries and objects we have becasue some operations are restricted to particular types of objects or geometries as indicated by errors that read:\nError in UseMethod(\"st_crs&lt;-\") :    no applicable method for 'st_crs&lt;-' applied to an object of class \"c('XY', 'POINT', 'sfg')\"\nwhich indicates that the function (st_crs) does not have a method defined for the type of object it’s being applied to. Note that the function inside UseMethod will be replaced by whichever function you’re attempting to apply to your object and the object of class component will vary based on the function and the object class.\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nPOINT\nsingle point geometry\n\n\nMULTIPOINT\nset of points\n\n\nLINESTRING\nsingle linestring (two or more points connected by straight lines)\n\n\nMULTILINESTRING\nset of linestrings\n\n\nPOLYGON\nexterior ring with zero or more inner rings, denoting holes\n\n\nMULTIPOLYGON\nset of polygons\n\n\nGEOMETRYCOLLECTION\nset of the geometries above\n\n\n\nAs is, these geometries are built on vertices with coordinates that are based on the Cartesian plane and thus are “spatial”, but not georeferenced or geographic. In order to convert these sf geometries to a geogrphic object (i.e., one with a CRS and whose location depicts and actual spot on the earth’s surface), we use st_sfc() to create a simple feature geography list column (see ?st_sfc for an example of this workflow)."
  },
  {
    "objectID": "lesson/vector_intro.html#conventions-in-sf-and-the-tidyverse",
    "href": "lesson/vector_intro.html#conventions-in-sf-and-the-tidyverse",
    "title": "Intro to Vector Data",
    "section": "Conventions in sf and the tidyverse",
    "text": "Conventions in sf and the tidyverse\nOne of the benefits of the sf package is that it is designed to interface with the tidyverse suite of packages. One of the appealing parts of working with tidyverse packages is that they share an underlying philosophy, data structure, and grammar. This can make life a lot easier as you move from getting your data into R, constructing a set of covariates (including those derived from spatial data), analyzing, and plotting (or mapping) those data. People have strong opinions about the tidyverse, but I find it to be an (eventually) useful way for people to gain some intuition for working in R. One of the grammatical conventions used in the tidyverse suite of packages is the use _ in function calls (this is known as snake case should you ever need to know that at a dinner party). The _ is typically used to separate the verb in a function call from its predicate. For example, bind_rows() in the dplyr package “binds” (the verb) rows (the predicate) wheras bind_cols() binds columns. For the sf package it’s slightly different in that most of the functions begin with a st_ or sf_ prefix to indicate that the function is designed to work on spatial objects followed by a word (or words) describing what the operation does (e.g., st_centroid() returns a MULTIPOINT object with each point located at the centroid of a polygon). We can classify these functions based on what they are expected to return:\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nMeasures: return a numeric value with units based on the units of the CRS\nTransformations: create new geometries based on input geometries.\n\nWe can also distinguish these functions based on how many geometries that operate on:\n\nUnary: operate on a single geometry at a time (meaning that if you have a MULTI* object the function works on each geometry individually)\nBinary: operate on pairs of geometries\nn-ary: operate on sets of geometries\n\nWe’ll focus on the unary operators for now, but the binary and n-ary operators will become more important as we move to develop databases for spatial analysis.\n\nUnary predicates\nUnary predicates are helpful ‘checks’ to make sure the object you are working with has the properties you might expect. Are the geometries valid? Is the data projected? Because we are asking a set of TRUE/FALSE questions, these functions are specified as st_is_:\n\n\n\n\n\n\n\npredicate\nasks…\n\n\n\n\nsimple\nis the geometry self-intersecting (i.e., simple)?\n\n\nvalid\nis the geometry valid?\n\n\nempty\nis the geometry column of an object empty?\n\n\nlonglat\ndoes the object have geographic coordinates? (FALSE if coords are projected, NA if no crs)\n\n\nis(geometry, class)\nis the geometry of a particular class?\n\n\n\n\n\nCode\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n\n\nReading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n\nCode\nst_is_longlat(nc)\n\n\n[1] TRUE\n\n\nCode\nst_is_valid(nc)\n\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n\n\nUnary measures\nMeasures return a quantity that describes the geometry\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndimension\n0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries\n\n\narea\nthe area of a geometry\n\n\nlength\nthe length of a linear geometry\n\n\n\ndistance is a binary measure that returns the distance between pairs of geometries either within a single object or between features in multiple objects\n\n\nCode\nhead(st_area(nc))\n\n\nUnits: [m^2]\n[1] 1137107793  610916077 1423145355  694378925 1520366979  967504822\n\n\nCode\nst_distance(nc)[1:5,1:5]\n\n\nUnits: [m]\n         [,1]     [,2]     [,3]      [,4]      [,5]\n[1,]      0.0      0.0  25591.8 439493.26 299049.94\n[2,]      0.0      0.0      0.0 408416.68 268284.09\n[3,]  25591.8      0.0      0.0 366648.94 226461.23\n[4,] 439493.3 408416.7 366648.9      0.00  67066.43\n[5,] 299049.9 268284.1 226461.2  67066.43      0.00\n\n\n\n\nUnary transformers\nUnary transformations work on a per object basis and return a new geometry for each geometry. These are a few of the most common, we’ll encounter a few more as the semester continues.\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\ncentroid\nof type POINT with the geometry’s centroid\n\n\nbuffer\nthat is this larger (or smaller) than the input geometry, depending on the buffer size\n\n\njitter\nthat was moved in space a certain amount, using a bivariate uniform distribution\n\n\nboundary\nwith the boundary of the input geometry\n\n\nconvex_hull\nthat forms the convex hull of the input geometry\n\n\nline_merge\nafter merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs.\n\n\nmake_valid\nthat is valid\n\n\nnode\nwith added nodes to linear geometries at intersections without a node; only works on individual linear geometries\n\n\npoint_on_surface\nwith a (arbitrary) point on a surface\n\n\npolygonize\nof type polygon, created from lines that form a closed ring\n\n\nsegmentize\na (linear) geometry with nodes at a given density or minimal distance\n\n\nsimplify\nsimplified by removing vertices/nodes (lines or polygons)\n\n\nsplit\nthat has been split with a splitting linestring\n\n\ntransform\ntransformed or convert to a new coordinate reference system (last week)\n\n\ncollection_extract\nwith subgeometries from a GEOMETRYCOLLECTION of a particular type\n\n\ncast\nthat is converted to another type\n\n\n\n\n\nCode\nplot(st_geometry(nc))\nplot(st_geometry(st_centroid(nc)), add=TRUE, col='red')\n\n\nWarning: st_centroid assumes attributes are constant over geometries"
  },
  {
    "objectID": "lesson/vector_intro.html#using-sf-and-the-tidyverse",
    "href": "lesson/vector_intro.html#using-sf-and-the-tidyverse",
    "title": "Intro to Vector Data",
    "section": "Using sf and the tidyverse",
    "text": "Using sf and the tidyverse\nAs I mentioned, one of the benefits of using the sf package is that commands from the other tidyverse package have defined methods for spatial objects. The dplyr package has a ton of helpful functions for maniputlating data in R. For example, we might select a single row from a shapefile based on the value of its attributes by using the dplyr::filter() command:\n\n\nCode\ndurham.cty &lt;- nc %&gt;% \n  filter(., NAME == \"Durham\")\n## We can also use the bracket approach\ndurham.cty2 &lt;- nc[nc$NAME == \"Durham\",]\n\nplot(st_geometry(nc))\nplot(st_geometry(durham.cty), add=TRUE, col=\"blue\")\n\n\n\n\n\nOr perhaps we only want a few of the columns in the dataset (because shapefiles always have lots of extra stuff). We can use dplyr::select() to choose columns by name:\n\n\nCode\nnc.select &lt;- nc %&gt;% \n  select(., c(\"CNTY_ID\", \"NAME\", \"FIPS\"))\nplot(nc.select)\n\n\n\n\n\nNotice that the geometries are sticky, this will be important later"
  },
  {
    "objectID": "lesson/index.html",
    "href": "lesson/index.html",
    "title": "Lessons",
    "section": "",
    "text": "This section has some worked examples demonstrating the use of different packages and giving some ‘roadmaps’ for completing different spatial operations. These are mostly my opinions, your mileage may vary, but I’ll try to justify why I do things the way that I do so that you can make an informed choice when you decide to deviate from that path."
  },
  {
    "objectID": "lesson/04-lesson.html",
    "href": "lesson/04-lesson.html",
    "title": "Introducing your Data",
    "section": "",
    "text": "Practice getting data into R from different sources\nLearn to identify common import errors\nGenerate summaries of imported data"
  },
  {
    "objectID": "lesson/04-lesson.html#objectives",
    "href": "lesson/04-lesson.html#objectives",
    "title": "Introducing your Data",
    "section": "",
    "text": "Practice getting data into R from different sources\nLearn to identify common import errors\nGenerate summaries of imported data"
  },
  {
    "objectID": "lesson/04-lesson.html#a-note-about-project-management",
    "href": "lesson/04-lesson.html#a-note-about-project-management",
    "title": "Introducing your Data",
    "section": "A Note about Project Management",
    "text": "A Note about Project Management\nYou’ll notice that every repository we use for assignments in this class has a set folder structure (with data, docs, etc.). This helps ensure that once anyone has cloned the repository all of the paths to files, code, etc will be the same regardless of who’s running the code. For this lesson, we’ll be working within the data folder. You’ll notice that within the data folder, there is a subfolder for original and one for processed. The original folder is reserved for unmodified data. This could be your initial spreadsheet of data, the version of a dataset that you downloaded for an analysis, or any other file that you will eventually modify for your analysis. If you make any changes (rename variables, filter observations, modify values), those changes should be saved to an object in the processed folder\n\n\n\n\n\n\nWarning\n\n\n\nFor your analysis to be reproducible, any filtering, cleaning, or modification of that original data should be documented in your scripts or Quarto document and the outputs stored in the processed folder.\n\n\n\nLet’s load some packages\nBefore you get too far into this, it’s a good idea to load all of your packages.\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'ggplot2' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(tigris)\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\nCode\nlibrary(sf)\n\n\nWarning: package 'sf' was built under R version 4.4.1\n\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\n\n\nCommon Sources of Data\nThere are, in general, three types of data that you will encounter that meet our original data criteria. Data you’ve collected, data you’ve downloaded from somehere else, or data you’ve accessed via a package.\n\nIt’s yours!\nSo far, this is the type of data that most students bring into class. It’s usually some sort of spreadsheet that contains all of that hard-earned field data. Although there are R packages for dealing with Microsoft Excel spreadsheets, we won’t focus on those for two reasons: 1) Excel is a proprietary software and so may not be available to all users and 2) Excel makes a lot of formatting choices for you that may not actually be helpful in your analysis.\nInstead, we’ll focus on a more general idea, the “delimited” text file. A delimited text file is flat (i.e., there’s only one “sheet”) and uses a consistent character (like a , or a tab) to denote when column breaks should occur. Delimited text files can be created and read in a variety of free software making them more accessible to others. It’s also a fairly trivial exercise to save an Excel spreadsheet into a .csv.\nThere are a variety of functions in base R that will read delimited files (read.csv, read.table, and read.delim are just a few examples), but we’re going to use the readr package from the tidyverse because it will help you get used to some of the tidyverse conventions and because it automates more of the data import process.\nWe’ll talk more about the tidyverse next week, but for the time being it’s worth knowing that the general structure of tidyverse functions is to combine a verb with and object. So in order to read a delimited file, we might use the read_delim() function where read is the verb and delim is the object\nFor this example, I’ve downloaded a file from the Federal Elections Committee depicting the campaign contributions to one of our Congressman in the 2025-2026 fiscal year. It’s located in your data/original folder. You can read it into your environment using read_delim and specifying a , for the delim argument (if you have tab delimited .txt file you would use \\\\t). Because we’ll want to look at this data later, I’m assigning it to an object called election_data using the assignment operator &lt;-. If you look at the help file for read_delim (by typing ?read_delim), you’ll see that there are a variety of other options (like read_csv or read_tsv) that allow you to eliminate specifying the delim argument. I’ll demonstrate that here, too.\n\n\nCode\nelection_data_1 &lt;- read_delim(\"data/original/fec_reciepts.csv\", \n                            delim = \",\")\n\n\nRows: 233 Columns: 78\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (40): committee_id, committee_name, report_type, filing_form, line_numb...\ndbl   (9): report_year, image_number, link_id, file_number, contribution_rec...\nlgl  (27): contributor_prefix, recipient_committee_org_type, is_individual, ...\ndttm  (2): contribution_receipt_date, load_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nelection_data_2 &lt;- read_csv(\"data/original/fec_reciepts.csv\")\n\n\nRows: 233 Columns: 78\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (40): committee_id, committee_name, report_type, filing_form, line_numb...\ndbl   (9): report_year, image_number, link_id, file_number, contribution_rec...\nlgl  (27): contributor_prefix, recipient_committee_org_type, is_individual, ...\ndttm  (2): contribution_receipt_date, load_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nA Note About Parsing\nYou’ll notice that both read_ functions return a bit of output telling you the names and data types of each column in the dataset. This is one of the features of using readr::read_ - it attempts to guess what datatype each column should be based on the value you give to the guess_max argument. You can see from the helpfile that the default value for guess_max is min(1000, n_max) meaning that it will look at the first n_max or 1000 rows, whichever is smaller. This can be helpful for large datasets, but it can also introduce some challenges as the different versions of read_ assign column types a little differently. You can see this by running:\n\n\nCode\nidentical(election_data_1, election_data_2)\n\n\n[1] FALSE\n\n\nCode\nall.equal(election_data_1, election_data_2)\n\n\n[1] TRUE\n\n\nDespite the fact that the two objects were created from exactly the same file, identical returns FALSE while all.equal returns TRUE. This is an indication that while the data is exactly the same between both objects, there is something a little different about how R is storing the objects (identical is very strict). We don’t need to worry about that now, but I’m pointing it out as you may run into places where this causes errors that are difficult to interpret. For now, we’ll just be excited that the data is in R!\n\n\nYou download it\nOccasionally, you’ll find data that is directly downloadable from a webpage (meaning the webaddress points directly to a .csv or .txt file). When that’s the case, you can still use the read_ functions to download and assign the data to an object. Like this:\n\n\nCode\nelection_data_web &lt;- read_csv(\"https://raw.githubusercontent.com/BSU-Spatial-Data-In-R-Fall2025/inclass-04/refs/heads/main/data/original/fec_reciepts.csv\")\n\n\nRows: 233 Columns: 78\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (40): committee_id, committee_name, report_type, filing_form, line_numb...\ndbl   (9): report_year, image_number, link_id, file_number, contribution_rec...\nlgl  (27): contributor_prefix, recipient_committee_org_type, is_individual, ...\ndttm  (2): contribution_receipt_date, load_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen you call the webpage inside of read_ the data is not automatically saved. If you’ve assigned it to an object (like election_data_web) it will be stored there until you decide to save it (which we’ll do next week). If you haven’t assigned it to an object, the data will just be printed to the screen.\n\n\nThere are more complicated workflows for .zip files (using download.file) or Google Drive files (using the googledrive package) which we’ll introduce later in the course. Those approaches add a bit more syntax on the front-end to get the file into your data/original folder, but after that the read_ step is the same.\nThe more common way of downloading data from the web is via Application Programming Interfaces (APIs). Although there are lots of APIs in the world, the typical application for getting data is a web-service that expects a particular set of inputs and then returns (possibly for download) a set of outputs matching your query. For example, the US Census has an API that allows you to access all of the Decennial Census and American Community Survey data by providing the state, county, year, and dataset that your are interested in. There are a lot of R packages designed to make these API calls easier. For example, the tidycensus package in R allows easy downloading of Census data, the FedData package allows you to download a variety of federally created spatial datasets, and the elevatr package allows easy download of global elevation datasets. We’ll explore these more in the future, but for now, we’ll use a simple example with the tigris package. The tigris package is a means of accessing the US TIGER (Topologically Integrated Geographic and Referencing System) files. The TIGER datsets contains US roads, state and county boundaries, and a variety of other data related to the US Census. Here’s a simple bit of code to download Idaho county boundaries.\n\n\nCode\nid_counties &lt;- counties(state=\"ID\", year = 2024, progress_bar = FALSE)\n\nid_counties\n\n\nSimple feature collection with 44 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -117.243 ymin: 41.98818 xmax: -111.0435 ymax: 49.00085\nGeodetic CRS:  NAD83\nFirst 10 features:\n    STATEFP COUNTYFP COUNTYNS GEOID        GEOIDFQ       NAME          NAMELSAD\n63       16      051 00399751 16051 0500000US16051  Jefferson  Jefferson County\n150      16      055 00395661 16055 0500000US16055   Kootenai   Kootenai County\n190      16      045 00395442 16045 0500000US16045        Gem        Gem County\n228      16      041 00395585 16041 0500000US16041   Franklin   Franklin County\n234      16      065 00394803 16065 0500000US16065    Madison    Madison County\n258      16      033 00399755 16033 0500000US16033      Clark      Clark County\n306      16      053 00395662 16053 0500000US16053     Jerome     Jerome County\n309      16      037 00399758 16037 0500000US16037     Custer     Custer County\n457      16      019 00395407 16019 0500000US16019 Bonneville Bonneville County\n574      16      049 00395699 16049 0500000US16049      Idaho      Idaho County\n    LSAD CLASSFP MTFCC CSAFP CBSAFP METDIVFP FUNCSTAT       ALAND    AWATER\n63    06      H1 G4020   292  26820     &lt;NA&gt;        A  2832619467  31166194\n150   06      H1 G4020   518  17660     &lt;NA&gt;        A  3205836476 184444918\n190   06      H1 G4020   147  14260     &lt;NA&gt;        A  1449806864  12481361\n228   06      H1 G4020  &lt;NA&gt;  30860     &lt;NA&gt;        A  1717211138  12153472\n234   06      H1 G4020   292  39940     &lt;NA&gt;        A  1215396274  10500950\n258   06      H1 G4020  &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;        A  4566524479   2476453\n306   06      H1 G4020  &lt;NA&gt;  46300     &lt;NA&gt;        A  1547629732  12908802\n309   06      H1 G4020  &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;        A 12748378762  42527190\n457   06      H1 G4020   292  26820     &lt;NA&gt;        A  4832814957  88912977\n574   06      H1 G4020  &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;        A 21956620663  67532942\n       INTPTLAT     INTPTLON                       geometry\n63  +43.7969649 -112.3185879 MULTIPOLYGON (((-111.8045 4...\n150 +47.6759569 -116.6959192 MULTIPOLYGON (((-117.0423 4...\n190 +44.0614727 -116.3987839 MULTIPOLYGON (((-116.7124 4...\n228 +42.1736093 -111.8229653 MULTIPOLYGON (((-111.9336 4...\n234 +43.7886140 -111.6569925 MULTIPOLYGON (((-111.9835 4...\n258 +44.2902180 -112.3546128 MULTIPOLYGON (((-112.3085 4...\n306 +42.6913953 -114.2620858 MULTIPOLYGON (((-113.932 42...\n309 +44.2733510 -114.2522675 MULTIPOLYGON (((-115.305 44...\n457 +43.3951708 -111.6218783 MULTIPOLYGON (((-112.5201 4...\n574 +45.8496440 -115.4673371 MULTIPOLYGON (((-116.4809 4...\n\n\nHere we are providing the API with a state and a year which counties converts into an API call to the census page. There are more complicated versions of this that we’ll explore down the road once you’re more comfortable with the spatial packages.\n\n\nIt comes with a package\nOne final option for obtaining data is that it “ships” with a package. That is, when you install the package, you get the data along with the functions. You’re not likely to use this much for your own analysis, but it can be critical when you’re trying to get help with a coding problem. Most help sites (e.g., StackOverflow, Posit Community) require a minimally reproducible example. Minimally reproducible examples allow others to diagnose your coding problem without you having to share your dataset and without them needing to run all of the cleanup steps. You can type library(help = \"datasets\") to get a list of a variety of example datasets. We’ll load the iris dataset here just so you can see how it works should you need it.\n\n\nCode\ndata(iris)\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n\n\nChecking yourself (or at least R)\nOk, so you’ve got your data into R, the first thing you need to do is to make sure that the import was successful. Before you do anything in R it’s worth familiarizing yourself with the metadata (data about the data) and building your intuition for how the data should look. Let’s take a look at the page where the data was exported.\n\nDid you get it all?\nThe first thing we might want to check is whether we actually got all of the date. Based on a quick look at the data it appears that there are 233 observations and columns. We can check that our data matches that by using the dim function (short for “dimensions”).\n\n\nCode\ndim(election_data_1)\n\n\n[1] 233  78\n\n\nThis returns the number of rows and then columns. Based on this we can see that are 233 observations (rows) and 78(!) columns. Where did all of these columns come from?? It’s not immediately obvious. You’ll notice that if you click on an individual record there’s an option to “View Image”. If you do that, you’ll see the actual tax form that is entered into the database. That form actually has 78 boxes so it would appear that we’re good on that front.\n\n\nIs it meaningful?\nOne of the things that readr::read_ does is to try and parse the column names in your data and assign it to a particular data types. That doesn’t guarantee, however, that it got it right. We should inspect that. The first thing we might do is take a look at the column names (using the colnames function from base R).\n\n\nCode\ncolnames(election_data_1)\n\n\n [1] \"committee_id\"                         \n [2] \"committee_name\"                       \n [3] \"report_year\"                          \n [4] \"report_type\"                          \n [5] \"image_number\"                         \n [6] \"filing_form\"                          \n [7] \"link_id\"                              \n [8] \"line_number\"                          \n [9] \"transaction_id\"                       \n[10] \"file_number\"                          \n[11] \"entity_type\"                          \n[12] \"entity_type_desc\"                     \n[13] \"unused_contbr_id\"                     \n[14] \"contributor_prefix\"                   \n[15] \"contributor_name\"                     \n[16] \"recipient_committee_type\"             \n[17] \"recipient_committee_org_type\"         \n[18] \"recipient_committee_designation\"      \n[19] \"contributor_first_name\"               \n[20] \"contributor_middle_name\"              \n[21] \"contributor_last_name\"                \n[22] \"contributor_suffix\"                   \n[23] \"contributor_street_1\"                 \n[24] \"contributor_street_2\"                 \n[25] \"contributor_city\"                     \n[26] \"contributor_state\"                    \n[27] \"contributor_zip\"                      \n[28] \"contributor_employer\"                 \n[29] \"contributor_occupation\"               \n[30] \"contributor_id\"                       \n[31] \"is_individual\"                        \n[32] \"receipt_type\"                         \n[33] \"receipt_type_desc\"                    \n[34] \"receipt_type_full\"                    \n[35] \"memo_code\"                            \n[36] \"memo_code_full\"                       \n[37] \"memo_text\"                            \n[38] \"contribution_receipt_date\"            \n[39] \"contribution_receipt_amount\"          \n[40] \"contributor_aggregate_ytd\"            \n[41] \"candidate_id\"                         \n[42] \"candidate_name\"                       \n[43] \"candidate_first_name\"                 \n[44] \"candidate_last_name\"                  \n[45] \"candidate_middle_name\"                \n[46] \"candidate_prefix\"                     \n[47] \"candidate_suffix\"                     \n[48] \"candidate_office\"                     \n[49] \"candidate_office_full\"                \n[50] \"candidate_office_state\"               \n[51] \"candidate_office_state_full\"          \n[52] \"candidate_office_district\"            \n[53] \"conduit_committee_id\"                 \n[54] \"conduit_committee_name\"               \n[55] \"conduit_committee_street1\"            \n[56] \"conduit_committee_street2\"            \n[57] \"conduit_committee_city\"               \n[58] \"conduit_committee_state\"              \n[59] \"conduit_committee_zip\"                \n[60] \"donor_committee_name\"                 \n[61] \"national_committee_nonfederal_account\"\n[62] \"election_type\"                        \n[63] \"election_type_full\"                   \n[64] \"fec_election_type_desc\"               \n[65] \"fec_election_year\"                    \n[66] \"two_year_transaction_period\"          \n[67] \"amendment_indicator\"                  \n[68] \"amendment_indicator_desc\"             \n[69] \"schedule_type\"                        \n[70] \"schedule_type_full\"                   \n[71] \"increased_limit\"                      \n[72] \"load_date\"                            \n[73] \"sub_id\"                               \n[74] \"original_sub_id\"                      \n[75] \"back_reference_transaction_id\"        \n[76] \"back_reference_schedule_name\"         \n[77] \"pdf_url\"                              \n[78] \"line_number_label\"                    \n\n\nNothing seems obviously wrong here. The names all seem readable and broken into distinct categories which suggests taht the delimitter worked and we didn’t end up with oddball columns.\n\n\nDoes R recognize it?\nNow to take a look at whether readr::read_ correctly guessed the type of data contained in each column. We can use dplyr::glimpse from the tidyverse or str (short for structure) from base R to get a quick look at the data.\n\n\nCode\nglimpse(election_data_1)\n\n\nRows: 233\nColumns: 78\n$ committee_id                          &lt;chr&gt; \"C00331397\", \"C00331397\", \"C0033…\n$ committee_name                        &lt;chr&gt; \"SIMPSON FOR CONGRESS\", \"SIMPSON…\n$ report_year                           &lt;dbl&gt; 2025, 2025, 2025, 2025, 2025, 20…\n$ report_type                           &lt;chr&gt; \"Q1\", \"Q1\", \"Q1\", \"Q1\", \"Q2\", \"Q…\n$ image_number                          &lt;dbl&gt; 2.025041e+17, 2.025041e+17, 2.02…\n$ filing_form                           &lt;chr&gt; \"F3\", \"F3\", \"F3\", \"F3\", \"F3\", \"F…\n$ link_id                               &lt;dbl&gt; 4.04082e+18, 4.04082e+18, 4.0408…\n$ line_number                           &lt;chr&gt; \"11AI\", \"11AI\", \"11AI\", \"11AI\", …\n$ transaction_id                        &lt;chr&gt; \"AF23BB60932664D8EACA\", \"A7BD67B…\n$ file_number                           &lt;dbl&gt; 1884461, 1884461, 1884461, 18844…\n$ entity_type                           &lt;chr&gt; \"IND\", \"ORG\", \"IND\", \"ORG\", \"IND…\n$ entity_type_desc                      &lt;chr&gt; \"INDIVIDUAL\", \"ORGANIZATION\", \"I…\n$ unused_contbr_id                      &lt;chr&gt; \"C00694323\", NA, \"C00694323\", NA…\n$ contributor_prefix                    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ contributor_name                      &lt;chr&gt; \"BINGER, KEVIN\", \"WINRED\", \"SLAT…\n$ recipient_committee_type              &lt;chr&gt; \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H…\n$ recipient_committee_org_type          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ recipient_committee_designation       &lt;chr&gt; \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P…\n$ contributor_first_name                &lt;chr&gt; \"KEVIN\", NA, \"LINDSAY\", NA, \"MIT…\n$ contributor_middle_name               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ contributor_last_name                 &lt;chr&gt; \"BINGER\", NA, \"SLATER\", NA, \"BUT…\n$ contributor_suffix                    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ contributor_street_1                  &lt;chr&gt; \"12910 CREAMERY HILL DR\", \"PO BO…\n$ contributor_street_2                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ contributor_city                      &lt;chr&gt; \"GERMANTOWN\", \"ARLINGTON\", \"WASH…\n$ contributor_state                     &lt;chr&gt; \"MD\", \"VA\", \"DC\", \"VA\", \"MD\", \"V…\n$ contributor_zip                       &lt;chr&gt; \"208746338\", \"222191891\", \"20002…\n$ contributor_employer                  &lt;chr&gt; \"CASSIDY & ASSOCIATES\", NA, \"TRO…\n$ contributor_occupation                &lt;chr&gt; \"SENIOR VICE PRESIDENT\", NA, \"VI…\n$ contributor_id                        &lt;chr&gt; \"C00694323\", NA, \"C00694323\", NA…\n$ is_individual                         &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, TRUE, …\n$ receipt_type                          &lt;chr&gt; \"15E\", NA, \"15E\", NA, \"15E\", NA,…\n$ receipt_type_desc                     &lt;chr&gt; \"EARMARKED CONTRIBUTION\", NA, \"E…\n$ receipt_type_full                     &lt;chr&gt; \"EARMARKED (NON-DIRECTED) THROUG…\n$ memo_code                             &lt;chr&gt; NA, \"X\", NA, \"X\", NA, \"X\", NA, \"…\n$ memo_code_full                        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ memo_text                             &lt;chr&gt; NA, \"TOTAL EARMARKED THROUGH CON…\n$ contribution_receipt_date             &lt;dttm&gt; 2025-02-05, 2025-02-05, 2025-02…\n$ contribution_receipt_amount           &lt;dbl&gt; 250, 250, 250, 250, 250, 250, 25…\n$ contributor_aggregate_ytd             &lt;dbl&gt; 250, 31900, 250, 31900, 250, 554…\n$ candidate_id                          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candidate_name                        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candidate_first_name                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candidate_last_name                   &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candidate_middle_name                 &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candidate_prefix                      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candidate_suffix                      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candidate_office                      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candidate_office_full                 &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candidate_office_state                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candidate_office_state_full           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candidate_office_district             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conduit_committee_id                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conduit_committee_name                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conduit_committee_street1             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conduit_committee_street2             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conduit_committee_city                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conduit_committee_state               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conduit_committee_zip                 &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ donor_committee_name                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ national_committee_nonfederal_account &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ election_type                         &lt;chr&gt; \"P2026\", \"P2026\", \"P2026\", \"P202…\n$ election_type_full                    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ fec_election_type_desc                &lt;chr&gt; \"PRIMARY\", \"PRIMARY\", \"PRIMARY\",…\n$ fec_election_year                     &lt;dbl&gt; 2026, 2026, 2026, 2026, 2026, 20…\n$ two_year_transaction_period           &lt;dbl&gt; 2026, 2026, 2026, 2026, 2026, 20…\n$ amendment_indicator                   &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n$ amendment_indicator_desc              &lt;chr&gt; \"ADD\", \"ADD\", \"ADD\", \"ADD\", \"ADD…\n$ schedule_type                         &lt;chr&gt; \"SA\", \"SA\", \"SA\", \"SA\", \"SA\", \"S…\n$ schedule_type_full                    &lt;chr&gt; \"ITEMIZED RECEIPTS\", \"ITEMIZED R…\n$ increased_limit                       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ load_date                             &lt;dttm&gt; 2025-04-12 04:16:48, 2025-04-12…\n$ sub_id                                &lt;dbl&gt; 4.04112e+18, 4.04112e+18, 4.0411…\n$ original_sub_id                       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ back_reference_transaction_id         &lt;chr&gt; NA, \"AF23BB60932664D8EACA\", NA, …\n$ back_reference_schedule_name          &lt;chr&gt; NA, \"SA11AI\", NA, \"SA11AI\", NA, …\n$ pdf_url                               &lt;chr&gt; \"https://docquery.fec.gov/cgi-bi…\n$ line_number_label                     &lt;chr&gt; \"Contributions From Individuals/…\n\n\nYou’ll notice that each column shows up after the $ operator, followed by the datatype enclosed in &lt;&gt;, followed by the first few observations from the dataset. The nice part about glimpse is the colored highlighting of NA values which can help draw your attention to potential mistakes. For now, we’re going to focus on the data types. You’ll see &lt;chr&gt; for many of the columns indicating that the data in those columns is of the character data type, you’ll also see &lt;dbl&gt; indicating that the column is a numeric data type with a “double float” precision which refers to the number of decimal points R will track for a numeric value. You’ll also notice the &lt;lgl&gt; or logical datatype referring to data that is either TRUE/FALSE and the &lt;dttm&gt; for dates and times. One thing you might also notice is that for many columns the first set of entries that glimpse shows us are entirely NA. When you’re working with your own data, hopefully you’ll know whether or not NAs are appropriate, but here, because this is “found” data, it’s a good idea here to check a few of the images to see whether these NAs are expected. The other thing you’ll notice is that a variety of “ID” fields (e.g., sub_id, file_id, image_id) were parsed as a double data type. This could be a problem as id columns are often meant to denote individuals and so act as a label (i.e., character) rather than a number. If there are leading 0’s (i.e., 00134) R will drop those when converting it to a numeric data type. Because we’re just looking at the data, we won’t worry about it now, but I wanted to draw your attention to it.\n\n\n\nExploring your new data\nOkay, you’ve got the data into R and you’ve checked that things look correct. Now it’s time to get a sense for what the data actually has in it. We might want some information of the distribution of numeric values, frequencies of categorical values, and maybe to look for values that fall outside of the expected range.\n\nBasic stats\nOne quick (and ugly) way to get a sense for the range of values in your data (along with summary stats for numeric data) is to use the summary function.\n\n\nCode\nsummary(election_data_1)\n\n\n committee_id       committee_name      report_year   report_type       \n Length:233         Length:233         Min.   :2025   Length:233        \n Class :character   Class :character   1st Qu.:2025   Class :character  \n Mode  :character   Mode  :character   Median :2025   Mode  :character  \n                                       Mean   :2025                     \n                                       3rd Qu.:2025                     \n                                       Max.   :2025                     \n  image_number       filing_form           link_id          line_number       \n Min.   :2.025e+17   Length:233         Min.   :4.041e+18   Length:233        \n 1st Qu.:2.025e+17   Class :character   1st Qu.:4.041e+18   Class :character  \n Median :2.025e+17   Mode  :character   Median :4.041e+18   Mode  :character  \n Mean   :2.025e+17                      Mean   :4.053e+18                     \n 3rd Qu.:2.025e+17                      3rd Qu.:4.071e+18                     \n Max.   :2.025e+17                      Max.   :4.071e+18                     \n transaction_id      file_number      entity_type        entity_type_desc  \n Length:233         Min.   :1884461   Length:233         Length:233        \n Class :character   1st Qu.:1884461   Class :character   Class :character  \n Mode  :character   Median :1884461   Mode  :character   Mode  :character  \n                    Mean   :1891304                                        \n                    3rd Qu.:1901069                                        \n                    Max.   :1901069                                        \n unused_contbr_id   contributor_prefix contributor_name  \n Length:233         Mode:logical       Length:233        \n Class :character   NA's:233           Class :character  \n Mode  :character                      Mode  :character  \n                                                         \n                                                         \n                                                         \n recipient_committee_type recipient_committee_org_type\n Length:233               Mode:logical                \n Class :character         NA's:233                    \n Mode  :character                                     \n                                                      \n                                                      \n                                                      \n recipient_committee_designation contributor_first_name contributor_middle_name\n Length:233                      Length:233             Length:233             \n Class :character                Class :character       Class :character       \n Mode  :character                Mode  :character       Mode  :character       \n                                                                               \n                                                                               \n                                                                               \n contributor_last_name contributor_suffix contributor_street_1\n Length:233            Length:233         Length:233          \n Class :character      Class :character   Class :character    \n Mode  :character      Mode  :character   Mode  :character    \n                                                              \n                                                              \n                                                              \n contributor_street_2 contributor_city   contributor_state  contributor_zip   \n Length:233           Length:233         Length:233         Length:233        \n Class :character     Class :character   Class :character   Class :character  \n Mode  :character     Mode  :character   Mode  :character   Mode  :character  \n                                                                              \n                                                                              \n                                                                              \n contributor_employer contributor_occupation contributor_id     is_individual  \n Length:233           Length:233             Length:233         Mode :logical  \n Class :character     Class :character       Class :character   FALSE:147      \n Mode  :character     Mode  :character       Mode  :character   TRUE :86       \n                                                                               \n                                                                               \n                                                                               \n receipt_type       receipt_type_desc  receipt_type_full   memo_code        \n Length:233         Length:233         Length:233         Length:233        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n memo_code_full  memo_text         contribution_receipt_date       \n Mode:logical   Length:233         Min.   :2025-01-08 00:00:00.00  \n NA's:233       Class :character   1st Qu.:2025-03-10 00:00:00.00  \n                Mode  :character   Median :2025-03-29 00:00:00.00  \n                                   Mean   :2025-04-09 11:38:22.15  \n                                   3rd Qu.:2025-05-23 00:00:00.00  \n                                   Max.   :2025-06-30 00:00:00.00  \n contribution_receipt_amount contributor_aggregate_ytd candidate_id  \n Min.   : 250                Min.   :  250             Mode:logical  \n 1st Qu.: 500                1st Qu.: 1000             NA's:233      \n Median :1000                Median : 2000                           \n Mean   :1479                Mean   :10836                           \n 3rd Qu.:2000                3rd Qu.: 7500                           \n Max.   :5000                Max.   :55400                           \n candidate_name candidate_first_name candidate_last_name candidate_middle_name\n Mode:logical   Mode:logical         Mode:logical        Mode:logical         \n NA's:233       NA's:233             NA's:233            NA's:233             \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n candidate_prefix candidate_suffix candidate_office candidate_office_full\n Mode:logical     Mode:logical     Mode:logical     Mode:logical         \n NA's:233         NA's:233         NA's:233         NA's:233             \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n candidate_office_state candidate_office_state_full candidate_office_district\n Mode:logical           Mode:logical                Mode:logical             \n NA's:233               NA's:233                    NA's:233                 \n                                                                             \n                                                                             \n                                                                             \n                                                                             \n conduit_committee_id conduit_committee_name conduit_committee_street1\n Mode:logical         Mode:logical           Mode:logical             \n NA's:233             NA's:233               NA's:233                 \n                                                                      \n                                                                      \n                                                                      \n                                                                      \n conduit_committee_street2 conduit_committee_city conduit_committee_state\n Mode:logical              Mode:logical           Mode:logical           \n NA's:233                  NA's:233               NA's:233               \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n conduit_committee_zip donor_committee_name\n Mode:logical          Length:233          \n NA's:233              Class :character    \n                       Mode  :character    \n                                           \n                                           \n                                           \n national_committee_nonfederal_account election_type      election_type_full\n Mode:logical                          Length:233         Mode:logical      \n NA's:233                              Class :character   NA's:233          \n                                       Mode  :character                     \n                                                                            \n                                                                            \n                                                                            \n fec_election_type_desc fec_election_year two_year_transaction_period\n Length:233             Min.   :2026      Min.   :2026               \n Class :character       1st Qu.:2026      1st Qu.:2026               \n Mode  :character       Median :2026      Median :2026               \n                        Mean   :2026      Mean   :2026               \n                        3rd Qu.:2026      3rd Qu.:2026               \n                        Max.   :2026      Max.   :2026               \n amendment_indicator amendment_indicator_desc schedule_type     \n Length:233          Length:233               Length:233        \n Class :character    Class :character         Class :character  \n Mode  :character    Mode  :character         Mode  :character  \n                                                                \n                                                                \n                                                                \n schedule_type_full increased_limit   load_date                     \n Length:233         Mode:logical    Min.   :2025-04-12 04:16:48.00  \n Class :character   NA's:233        1st Qu.:2025-04-12 04:16:48.00  \n Mode  :character                   Median :2025-04-12 04:16:48.00  \n                                    Mean   :2025-05-21 17:29:56.39  \n                                    3rd Qu.:2025-07-17 04:06:49.00  \n                                    Max.   :2025-07-17 04:06:49.00  \n     sub_id          original_sub_id back_reference_transaction_id\n Min.   :4.041e+18   Mode:logical    Length:233                   \n 1st Qu.:4.041e+18   NA's:233        Class :character             \n Median :4.041e+18                   Mode  :character             \n Mean   :4.054e+18                                                \n 3rd Qu.:4.072e+18                                                \n Max.   :4.072e+18                                                \n back_reference_schedule_name   pdf_url          line_number_label \n Length:233                   Length:233         Length:233        \n Class :character             Class :character   Class :character  \n Mode  :character             Mode  :character   Mode  :character  \n                                                                   \n                                                                   \n                                                                   \n\n\nWe can look at the contribution_receipt_amount and the contributor_aggregate_ytd columns as those are the only true numeric values. We can see that the Max contribution amount for any single observation is 5000 whereas the Max aggregate amount is 55400. Looking at the aggregate value a bit more you’ll see that the Mean is actually larger than the 3rd quartile values which suggests that there are a handful of very larger contributors relative to the rest. If you were planning to analyze this data, you’ll have to think about how to deal with those observations carefully.\n\n\nOddballs\nThe other thing that summary can help with is identifying places where data is incorrect. For example, this should only be for this FY so the fact that all of the report_yearvalues are the same is not surprising. Another thing you might now is that individual contributions are capped at 5000, which we know to be true from the previous check. Finally, summary returns the number of NAs in each column.\n\n\nData frequency\nOne last thing we might check before we decide what we want to do with this data is to look at the frequency of different categories. We can use tableto look at a few of those things. We might take a look at how many of these contributions are from individuals.\n\n\nCode\ntable(election_data_1$is_individual)\n\n\n\nFALSE  TRUE \n  147    86 \n\n\nBased on this we can see that the number of contributions by groups outnumbers individuals by almost 2:1. What other categorical variables might you look at?\nThat’s all for now. We’ll learn more complicated ways to evaluate and modify data in the coming weeks, but this is a standard “gut-check” anytime you’re bringing data into R."
  },
  {
    "objectID": "lesson/01-lesson.html",
    "href": "lesson/01-lesson.html",
    "title": "Is this thing on?",
    "section": "",
    "text": "This class relies on several tools that you may not be used to: an online instance of RStudio, git, and GitHub classroom. We’ll talk more about about git and GitHub in our next class. For now the focus is on getting you familiar with the RStudio server and the way that GitHub classroom works."
  },
  {
    "objectID": "lesson/01-lesson.html#orientation-to-rstudio-and-our-rstudio-server",
    "href": "lesson/01-lesson.html#orientation-to-rstudio-and-our-rstudio-server",
    "title": "Is this thing on?",
    "section": "Orientation to RStudio and our RStudio server",
    "text": "Orientation to RStudio and our RStudio server\nRstudio is what’s called an Integrated Development Environment (IDE). That’s a fancy way of saying that you can write code, run code, and view the outputs of code all in one place. There are other options (VSCode seems to be a popular one), but this one is designed for use specifically with the R program language, is pretty stable, and has lots of other features that we might want to use. Also, it’s free!!\nRstudio requires that a working version of R (to actually execute the code) along with a program to process graphics (usually XQuartz) and a suite of compilers to allow for various packages and functions to work (these are often a part of Rtools or Xcode). You should be able to install all of these on your laptop for your own work. We are going to skip all of that and use an RStudio server. While that might rob you of the joy of installing R on your own (though you probably should do that anyway), it ensures that everyone is using the exact same version of packages, operating system, and underlying architecture. This, in turn, should make it easier for us to work together to troubleshoot when we run into problems.\n\nGetting credentials\nBecause our server runs on a Boise State instance of Amazon Web Services, you’ll need to have credentials to login. Because we may update packages or underlying data periodically throughout the course, your credentials may change in between logins. As such, you’ll need to go here before logging into the server to refresh your credentials and make sure you’re updating the current version.\n\n\nLogging into our Rstudio server\nYou should be able to get to our server by clicking this link or the  in the upper right corner of the webpage. Once you’ve clicked that you should see:  Assuming you followed the instructions in the previous step to get your credentials, you can enter them here and click Sign In. You should now see the main Rstudio environment. \nNow that you’re in Rstudio you’ll need to introduce yourself!\n\n\nIntroduce yourself to Git\nThere are lots of ways to do this (check out Jenny Bryan’s happygitwithr webpage if you want to learn more), but we’ll do this the easy way.\nFirst, you’ll need to load two packages. Packages are pre-compiled sets of functions designed to do a handful of related tasks. You’ll learn more about these in the coming weeks, but for now you just need to know that a) the packages should already be downloaded on the server, b) you can load them with the library() command (or you can call functions explicitly by using packagename::functionname), and c) you need to know the name. For this portion, you’ll need the usethis and gitcreds packages. Let’s load them now:\n\n\nCode\nlibrary(usethis)\nlibrary(gitcreds)\n\n\n\nYou’ll notice that nothing really happens. That’s ok! Now we’re going to introduce github account to Rstudio.\nThe first step is to include your username and email (this should be your GitHub username and the email associated with your GitHub account)\n\n\nCode\nuse_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\") # your info here\n#OR\nusethis::use_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\")\n\n\nOnce you do that, you’ll need to supply your personal access token (PAT). If you don’t have one, you can run the following command and it will take you to GitHub to generate a token. You need to save this somewhere that you’ll be able to access it again. If you don’t, you’ll be doing this step A LOT. If you already have a PAT, skip this step!\n\n\nCode\nusethis::create_github_token()\n\n\nOnce you’ve got your PAT (and copied it to a place that you won’t lose it), you can run the following to add it to your Rstudio session. Note that this will timeout after an hour unless you do a few more steps.\n\n\nCode\ngitcreds::gitcreds_set()\n\n\nFinally, you can make sure that everything is stored and ready to go by running:\n\n\nCode\ngitcreds::gitcreds_get()\n\n\nYou should see that you’ve successfully stored your credential.\n\n\nOptional: Cache your token for a longer period of time\nIn general, it’s better to rely on a secure password manager (like MacOS Keychain or 1Password) to hold onto your PAT. Unfortunately, Linux (which our server runs) doesn’t have one of these. As a workaround to prevent frustration with git errors you can do 1 of 2 things.\nThe first option is to change your global git preferences, which you can do by switching to the terminal tab in Rstudio and entering the following:\n\n\nCode\ngit config --global credential.helper 'cache --timeout=10000000'\n\n\n\nAlternatively, you can edit your .Renviron file to store your token there. You can open the file for editing by running:\n\n\nCode\nusethis::edit_r_environ()\n\n\nRunning this should open a new pane in Rstudio above the console with a blank txt file that you can edit: \nYou can then paste the following (with your PAT instead of the XXXs) into that file, add a line after, and save. Note: you have to add a newline after you enter your PAT\n\nMake sure you don’t upload this .Renviron file to a place where others can find/see it (like Google Drive, GitHub, or your Instagram)\n\n\n\nCode\nGITHUB_PAT=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
  },
  {
    "objectID": "lesson/01-lesson.html#github-classroom",
    "href": "lesson/01-lesson.html#github-classroom",
    "title": "Is this thing on?",
    "section": "GitHub classroom",
    "text": "GitHub classroom\nBecause version control is key to both coding efficiency and reproducible workflows, we are going to use GitHub classroom (GHC) to manage assignments in this class. GitHub classroom allows me to provide you with a repository that contains any necessary functions, data, and instructions that you can then clone, modify to complete the assignment, and then submit back to me. It can be a little clunky at first, but today is supposed to help get passed that initial frustration. **If you’re already used to GitHub, this should be pretty easy. If you’re not, we’ll do a quick walk-through today so you can get a sense of how it works.\nThe basic steps are: 1. I’ll share the link with you to the assignment. 2. GHC creates a clone of the assignment repo inside of your GitHub account 3. You’ll bring this repository into your Rstudio environment (we’ll do this Weds) 4. You’ll make changes, store them with commits (again, on Weds), and push them back up to your repository 5. I’ll be able to access all of your files and code for grading and code review."
  },
  {
    "objectID": "example/index.html",
    "href": "example/index.html",
    "title": "Examples",
    "section": "",
    "text": "This section has the code and description from the live-coding exercises that we’ll do in class."
  },
  {
    "objectID": "example/04-example.html",
    "href": "example/04-example.html",
    "title": "Meeting Your Data",
    "section": "",
    "text": "Join the in-class example repo from github classroom using the project setup we’ve introduced.\nRead the data into R and floow the steps in today’s lesson\nUse table() to identify the number of contributions from each state.\nPost a screeenhot of the result in slack."
  },
  {
    "objectID": "example/01-example.html",
    "href": "example/01-example.html",
    "title": "Testing GitHub Classroom",
    "section": "",
    "text": "Login to Rstudio server\nIntroduce yourself to git on your server instance\nUpdate your token cache\nClick this link to access the first in-class example\nTake a screenshot of your GitHub page with the example in it.\nPost it to our class slack channel"
  },
  {
    "objectID": "content/32-content.html",
    "href": "content/32-content.html",
    "title": "Conclusion",
    "section": "",
    "text": "You did it!! You made it all the way through and hopefully learned a few things along the way. Today we’ll do a brief wrapup of the course and answer any outstanding questions you may have.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/30-content.html",
    "href": "content/30-content.html",
    "title": "Data Visualization and Maps II",
    "section": "",
    "text": "R provides a number of different packages for generating plots of your data, but ggplot2 is probably the most common owing to its ability to use consistent syntax to produce a variety of different graphics. In addition to plots of data and model objects, ggplot2 can also be used with sf objects and some raster datasets to generate publication quality maps. We’ll also take a little more time to understand some of the options for building static maps in R and look at a few packages that can help you build publication-quality maps without having to move into a new software."
  },
  {
    "objectID": "content/30-content.html#resources",
    "href": "content/30-content.html#resources",
    "title": "Data Visualization and Maps II",
    "section": "Resources",
    "text": "Resources\n\n The Data Visualization: A Practical Introduction by (Healy 2018) provides a lot of examples of Tufte-style graphics built with ggplot2.\n Graphic design with ggplot2 is an entire course devoted to making beautiful visualizations with ggplot2. If nothing else, check out some of the examples!\n The patchwork package website provides a lot of examples of building complicated layouts with ggplot2 objects with intuitive syntax.\n The Drawing Beautiful Maps Programatically with R, sf, and ggplot2 chapter by Mel Moreno and Mathieu Basille provides a nice series of blogposts designed to help you build maps with sf objects.\n Creating beautiful demographic maps in R with the tidycensus and tmap packages by Zev Ross illustrates the simplicity of mapping with tmap.\n Displaying time series, spatial and space-time data with R by Oscar Perpiñán Lamigueiro has a bunch of interesting code for producing maps contained in the book by the same name."
  },
  {
    "objectID": "content/30-content.html#objectives",
    "href": "content/30-content.html#objectives",
    "title": "Data Visualization and Maps II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nUnderstand the relationship between the Grammar of Graphics and ggplot syntax\nDescribe the various options for customizing ggplots and their syntactic conventions\nGenerate complicated plot layouts without additional pre-processing\nConstruct a map using ggplot2 and tmap\nCombine vector and raster data in the same map\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nPanopto Video"
  },
  {
    "objectID": "content/26-content.html",
    "href": "content/26-content.html",
    "title": "Movement and Networks II",
    "section": "",
    "text": "Now that we’ve chatted briefly about what a network is and how we represent it for analysis, it’s time to take some first steps toward building spatial networks in R. The backbone of most network analysis is igraph, a package that is available for a number of programming languages. There are also a growing number of spatial network packages being developed for R. We’ll introduce a few of those today."
  },
  {
    "objectID": "content/26-content.html#resources",
    "href": "content/26-content.html#resources",
    "title": "Movement and Networks II",
    "section": "Resources",
    "text": "Resources\n\n Network Data in R and Spatial Networks from the Online Companion to Network Science in Archaelogy introduces a variety of approaches for handling and visualizing network data with R using examples from archaeology.\n CMRnet: An R package to derive networks of social interactions and movement from mark-recapture data by (Silk et al. 2021) describes a package for developing networks from common wildlife sampling techniques.\n The sfnetworks package github page provides a variety of vignettes for manipulating spatial network data within the sf framework for spatial objects."
  },
  {
    "objectID": "content/26-content.html#objectives",
    "href": "content/26-content.html#objectives",
    "title": "Movement and Networks II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nGenerate an adjacency matrix for network analysis\nCalculate network density, centrality, and other common measures\nGenerate landscape connectivity models using terra and gDistance\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nPanopto Video"
  },
  {
    "objectID": "content/24-content.html",
    "href": "content/24-content.html",
    "title": "Statistical Modelling III",
    "section": "",
    "text": "In our last class on multivariate analysis, we’ll take on one of the more underappreciated elements of modeling: understanding if your model is good enough for prediction or inference. We’ll spend a bit of time differentiating the uses of models as a means of understanding what it means to be a “good” model."
  },
  {
    "objectID": "content/24-content.html#resources",
    "href": "content/24-content.html#resources",
    "title": "Statistical Modelling III",
    "section": "Resources",
    "text": "Resources\n\n A practical guide to selecting models for exploration, inference, and prediction in ecology by (Tredennick et al. 2021) highlights the importance of understanding model performance before making inference on predictor effects.\n Model selection using information criteria, but is the “best” model any good? by (Mac Nally et al. 2018) highlights the importance of understanding model performance before making inference on predictor effects.\n Standards for distribution models in biodiversity assessments by (Araújo et al. 2019) highlights the importance of understanding model performance before making inference on predictor effects."
  },
  {
    "objectID": "content/24-content.html#objectives",
    "href": "content/24-content.html#objectives",
    "title": "Statistical Modelling III",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nArticulate three different reasons for modeling and how they link to assessments of fit\nDescribe and implement several test statistics for assessing model fit\nDescribe and implement several assessments of classification\nDescribe and implement resampling techniques to estimate predictive performance\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nPanopto Video"
  },
  {
    "objectID": "content/22-content.html",
    "href": "content/22-content.html",
    "title": "Statistical Modelling I",
    "section": "",
    "text": "Now that we’ve spent some time building dataframes and assessing the spatial correlation (or covariation) for different data, we can move beyond just describing the nature of the data we have or interpolating based on simple predictions. We’ll introduce two fairly simple spatial analysis approaches - overlays and logistic regression - and talk about some of the key assumptions and extensions of these approaches."
  },
  {
    "objectID": "content/22-content.html#resources",
    "href": "content/22-content.html#resources",
    "title": "Statistical Modelling I",
    "section": "Resources",
    "text": "Resources\n\n Overlay analysis provides an overview of the logic of overlay analysis.\n Predicting site location with simple additive raster sensitivity analysis using R from Ben Markwick has a complete example of using a weights of evidence approach to overlays.\n Logistic regression: a brief primer by (Stoltzfus 2011) is a nice introduction to logistic regression.\n Is my species distribution model fit for purpose? Matching data and models to applications by (Guillera-Arroita et al. 2015) is an excellent, concise description of the relations between data collection, statistical models, and inference.\n Predicting species distributions for conservation decisions by (Guisan et al. 2013) is a foundational paper describing some of the challenges with making conservation decisions based on the outcomes of species distribution models."
  },
  {
    "objectID": "content/22-content.html#objectives",
    "href": "content/22-content.html#objectives",
    "title": "Statistical Modelling I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nIdentify nearest neighbors based on distance\nDescribe and implement overlay analyses\nExtend overlay analysis to statistical modeling\nGenerate spatial predictions from statistical models\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Video"
  },
  {
    "objectID": "content/20-content.html",
    "href": "content/20-content.html",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "Last class we started to explore ways to leverage spatial autocorrelation as a means of using interpolation to generate values at unobserved locations. We’ll continue that discussion using variograms and kriging. We then move to a discussion of areal data and the need to identify “neighbors” as a means of understanding how to weight observations when the actual point location of the observation may be unknown or impossible to assign."
  },
  {
    "objectID": "content/20-content.html#resources",
    "href": "content/20-content.html#resources",
    "title": "Proximity and Areal Data",
    "section": "Resources",
    "text": "Resources\n\n Ch. 7: Spatial Neighborhood Matrices in from Paula Moraga’s new book Spatial Statistics for Data Science: Theory and Practice with R gives a little gentler introduction to spatial neighbors specifically in the context of statistical models.\n Chapter 14 Proximity and Areal Data in Spatial Data Science by Edzer Pebesma and Roger Bivand provides explanations of how the spdep package can be used to construct neighborhood weights."
  },
  {
    "objectID": "content/20-content.html#objectives",
    "href": "content/20-content.html#objectives",
    "title": "Proximity and Areal Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDescribe and implement statistical approaches to interpolation\nDescribe the case for identifying neighbors with areal data\nImplement contiguity-based neighborhood detection approaches\nImplement graph-based neighborhood detection approaches\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nPanopto Recording"
  },
  {
    "objectID": "content/18-content.html",
    "href": "content/18-content.html",
    "title": "Combining Data and Point Patterns",
    "section": "",
    "text": "Today we’ll finish up our example of combining data for analysis and introduce point process models as a first version of spatial analysis. We’ll need a few new packages here, but many of the key data management processes will remain the same."
  },
  {
    "objectID": "content/18-content.html#resources",
    "href": "content/18-content.html#resources",
    "title": "Combining Data and Point Patterns",
    "section": "Resources",
    "text": "Resources\n\n The Chapters 17 and 18 on Spatial Point Processes and the spatstat package in Paula Moraga’s book Spatial Statistics for Data Science: Theory and Practice with R.\n Rings, circles, and null-models for point pattern analysis in ecology by (Wiegand and A. Moloney 2004) provides an introduction to metrics for spatial clustering with applications in ecology.\n Improving the usability of spatial point process methodology: an interdisciplinary dialogue between statistics and ecology by Janine Illian (a major contributor to modern point pattern analyses) and David Burslem (a Scottish plant ecologist) (Illian and Burslem 2017) is a fairly modern take on the challenges associated with point process modeling in ecology.\n Chapter 11: Point Pattern Analysis in Manuel Gimond’s Introduction to GIS and Spatial Analysis bookdown project provides a nice (and free) introduction to some of these introductory point process methods."
  },
  {
    "objectID": "content/18-content.html#objectives",
    "href": "content/18-content.html#objectives",
    "title": "Combining Data and Point Patterns",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nComplete the creation of a dataset for analysis using vector and raster data\nDefine a point process and their utility for ecological applications\nDefine first and second-order Complete Spatial Randomness\nUse several common functions to explore point patterns"
  },
  {
    "objectID": "content/18-content.html#slides",
    "href": "content/18-content.html#slides",
    "title": "Combining Data and Point Patterns",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Video"
  },
  {
    "objectID": "content/16-content.html",
    "href": "content/16-content.html",
    "title": "Integrating Rasters and Vector Data",
    "section": "",
    "text": "The goal of much of our spatial data “munging” is to create a dataframe that can be used in subsequent statistical analyses. It can be difficult to link all of the steps of filtering, selecting, extracting, etc into a coherent problem when you are just being exposed to the syntax (as we discovered last week). Today, I’ll try to use a motivating example to help you see a path forward."
  },
  {
    "objectID": "content/16-content.html#resources",
    "href": "content/16-content.html#resources",
    "title": "Integrating Rasters and Vector Data",
    "section": "Resources",
    "text": "Resources\n\n The Spatial Data Operations Chapter in (Lovelace et al. 2019) makes the concepts of a network concrete (literally) by using a transportation route example to illustrate the various components of a network analysis in R.\n Chapter 3. Processing Tabular Data from Geographic Data Science with R by Michael C. Wimberly has a nice introduction to many of the dplyr verbs for manipulating tabular data.\n Chapter 9. Combining Vector Data with Continuous Raster Data from Geographic Data Science with R by Michael C. Wimberly introduces data extraction and zonal statistics for raster data.\n Chapter 10. Combining Vector Data with Discrete Raster Data from Geographic Data Science with R by Michael C. Wimberly extends Chapter 9 for discrete rasters, but also adds some additional buffering and data manipulation syntax."
  },
  {
    "objectID": "content/16-content.html#objectives",
    "href": "content/16-content.html#objectives",
    "title": "Integrating Rasters and Vector Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nUse dplyr with predicates and measures to subset and manipulate data\nUse extract to access raster data\nUse zonal to summarize access data\nJoin data into a single analyzable dataframe"
  },
  {
    "objectID": "content/16-content.html#slides",
    "href": "content/16-content.html#slides",
    "title": "Integrating Rasters and Vector Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Zoom Recording"
  },
  {
    "objectID": "content/14-content.html",
    "href": "content/14-content.html",
    "title": "Operations with Raster Data II",
    "section": "",
    "text": "Now that we’ve done some “global” transformations of raster data using terra, we’ll look at some of the options for cell-wise transformations. Rather than manipulating the extent, resolution, or CRS of the raster data; we’ll actually be using functions to change the values of the cells themselves."
  },
  {
    "objectID": "content/14-content.html#readings",
    "href": "content/14-content.html#readings",
    "title": "Operations with Raster Data II",
    "section": "Readings",
    "text": "Readings\n\n The terra package vignette describes the new raster functions available in terra, their relationship to those in the raster package, and the changes in syntax between the two.\n The Raster GIS Operations in R with terra chapter from Jasper Slingsby’s “A Minimal Introduction to GIS (in R)” bookdown project has worked examples of many of the operations we’ll learn today."
  },
  {
    "objectID": "content/14-content.html#objectives",
    "href": "content/14-content.html#objectives",
    "title": "Operations with Raster Data II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nUse moving windows as a means of smoothing raster data\nReclassify data using conditional statements and reclassification tables\nUse raster math as a means of creating new data based on an existing dataset."
  },
  {
    "objectID": "content/14-content.html#slides",
    "href": "content/14-content.html#slides",
    "title": "Operations with Raster Data II",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to today’s Panopto Slides"
  },
  {
    "objectID": "content/12-content.html",
    "href": "content/12-content.html",
    "title": "Operations With Vector Data II",
    "section": "",
    "text": "Now that you have the complete picture of predicates, measures, and transformers; it’s time to use them on some actual data. This lecture is meant to be the “practical” application of the ideas you’ve learned in our previous discussions of vector data and give you enough tools to begin to subset your data to the records and attributes of interest, calculate new spatial metrics, and generate new geometries based on existing data."
  },
  {
    "objectID": "content/12-content.html#readings",
    "href": "content/12-content.html#readings",
    "title": "Operations With Vector Data II",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Section 3.1 and 3.2 of Spatial Data Science, a bookdown project by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)."
  },
  {
    "objectID": "content/12-content.html#objectives",
    "href": "content/12-content.html#objectives",
    "title": "Operations With Vector Data II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nTranslate pseudocode commands into functional workflows\nArticulate the importance of key arguments to sf functions\nGenerate new attributes and geometries from existing data."
  },
  {
    "objectID": "content/12-content.html#slides",
    "href": "content/12-content.html#slides",
    "title": "Operations With Vector Data II",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "Introduction to Mapping Geographic Data",
    "section": "",
    "text": "Now that we’re getting into actual operations on spatial data and beginning to actually modify the geometries and attributes of spatial data, it’ll be important for you to be able to visualize the results. At this point, we’ll be focusing on rough visualization as a way of “gut-checking” the outcomes of your code. We’ll focus more on creating informative, aesthetically pleasing, publication quality visualizations."
  },
  {
    "objectID": "content/10-content.html#readings",
    "href": "content/10-content.html#readings",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Readings",
    "text": "Readings\n\n Ch.3 Tmap in a nutshell from “Elegant and informative maps with tmap” by Martijn Tennekes and Jakub Nowosad provides a great “quick start” for using the tmap package for visualizing spatial data.\n Making maps with R by (Lovelace et al. 2019) introduces the tmap package for making nice maps with relatively minimal syntax.\n Making maps in R by Emily Burchfield illustrates some quick mapping syntax with base plot, ggplot, and tmap. For now, just focus on the base plot and tmap sections as we’ll take on the ggplot stuff later in the course."
  },
  {
    "objectID": "content/10-content.html#objectives",
    "href": "content/10-content.html#objectives",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDescribe the basic components of data visualization as a foundation for mapping syntax\nUnderstand layering in both base plot and tmap\nMake basic plots of multiple spatial data objects"
  },
  {
    "objectID": "content/10-content.html#slides",
    "href": "content/10-content.html#slides",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to today’s Panopto Slides"
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "Areal Data: Vectors",
    "section": "",
    "text": "Now that you have started working with the various components of coordinates and coordinate reference systems, it’s time to start learning the fundamental aspects of working with vector data in sf and R. The syntax is a little confusing at first, but once you’ve gotten a sense for the logic behind it you should be able to start piecing together the functions necessary to implement the pseudocode you write for an analysis. We’ll spend more time on vector manipulation in the coming weeks so you’ll get plenty of practice with the ideas we introduce today."
  },
  {
    "objectID": "content/08-content.html#readings",
    "href": "content/08-content.html#readings",
    "title": "Areal Data: Vectors",
    "section": "Readings",
    "text": "Readings\nSame as last class really, but hopefully you’ll begin to understand them better…\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Chapter 2, Sections 1-3 and Chapter 3, Section 1 of Spatial Data Science by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)"
  },
  {
    "objectID": "content/08-content.html#objectives",
    "href": "content/08-content.html#objectives",
    "title": "Areal Data: Vectors",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nImplement approaches for checking and reparing geometries in R\nUnderstand predicates and measures in the context of spatial operations in sf\nUse st_* to evaluate attributes of geometries and calculate measurements"
  },
  {
    "objectID": "content/08-content.html#slides",
    "href": "content/08-content.html#slides",
    "title": "Areal Data: Vectors",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Recording"
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "",
    "text": "Today we’ll focus on some of the tools for reproducible workflows using R. We’ll introduce Quarto as a means of authoring different kinds of documents. We’ll talk about literate programming and leaving breadcrumbs for yourself (and others). Finally, we’ll begin to work through the ideas of workflow planning"
  },
  {
    "objectID": "content/06-content.html#readings",
    "href": "content/06-content.html#readings",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Readings",
    "text": "Readings\n\n Authoring in Quarto - an intro to Quarto for developing different kinds of documents. Lots of other resources linked here!!\n Pseudocode: what it is and how to write it - A nice blogpost by Sara Metawalli the sketches out the logic of pseudocode and why it can be helpful.\n The Whole Game - from Wickham et al., R for Data Science (Wickham and Grolemund 2016). Focus on the sections that begin with “Workflow” to get a sense for how we’ll start putting the pieces together.\n Scripts, algorithms, and functions - chapter 11 in in Lovelace et al., Geocomputation with R (Lovelace et al. 2019) introduces some concepts behind geospatial programming. A few of these pieces will make more sense in the next few weeks, but the general advice on constructing code and planning analyses is useful now."
  },
  {
    "objectID": "content/06-content.html#objectives",
    "href": "content/06-content.html#objectives",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDevelop basic docs with Quarto\nUnderstand the basics of creating readable code\nUse pseudocode to sketch out a computational problem"
  },
  {
    "objectID": "content/06-content.html#slides",
    "href": "content/06-content.html#slides",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "Meeting Your Data",
    "section": "",
    "text": "Now that you know a bit about the “why” and “how” of this course, it’s time to start actually working with data. We’ll spend a bit of time thinking about the ‘nature’ of data, the qualities of “good” data, and the responsibilities of data creators and users. We’ll also practice some generic approaches for getting your own data into R and accessing data via APIs and functions."
  },
  {
    "objectID": "content/04-content.html#readings",
    "href": "content/04-content.html#readings",
    "title": "Meeting Your Data",
    "section": "Readings",
    "text": "Readings\n\nSetting the Stage\n Geographies of conservation II: Technology, surveillance and conservation by algorithm by Adams (2019) provides a critical perspective on the role of new data-sensing technology in the environment.\n The ethics of big data as a public good: Which public? Whose good? by Taylor (2016) highlights some of the difficulties that arise when “big data” is largely owned and created by private companies.\n A Survey of Data Quality Requirements That Matter in ML Development Pipelines by Priestley et al. (2023) provides a practical discussion of the attributes of “good” data particularly in the context of applied machine learning. While a bit broader in focus, many of the 4 dimensions of data quality are directly relevant to the discussions in the other articles.\n Data justice and biodiversity conservation by Pritchard et al. (2022) provides an accessible introduction to the concept of data justice and frameworks available for achieving it.\nThe CARE Principles for Indigenous Data Governance by (2023) outlines a framework for data justice in an Indigenous context.\n\n\nTechnical Details\nTidy Data by (2014) provides the foundations for thinking about tidy data. What it is, why it’s helpful, etc.\nThe “Data in R” section of Introduction to R by (Douglas et al. 2022) gives an important overview of the different data types in R and how they are represented as objects within the R environment."
  },
  {
    "objectID": "content/04-content.html#objectives",
    "href": "content/04-content.html#objectives",
    "title": "Meeting Your Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nRecognize the role that data selection and documentation plays in reproducible workflows\nSummarize key debates surrounding the role of (spatial) data in solving environmental problems\nDescribe FAIR and CARE principles for data and their relationship to existing debates\nRead data into your R environment.\nInspect the data and summarize it using tables and simple plots"
  },
  {
    "objectID": "content/04-content.html#slides",
    "href": "content/04-content.html#slides",
    "title": "Meeting Your Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "What Is This Class?",
    "section": "",
    "text": "Today we’ll focus on getting to know each other and the course structure. Before we get inundated with technical details and syntax, I think it’s important to remind ourselves why we need geographical analysis. We’ll spend some time on the various conceptualizations of place and space, how those things show up in geographic data, and their implications for the kinds of science we can do when we’re using geographic data. This session is meant to provide a bit of philosophical foundation for you to keep in mind as you work through various parts of the analytic pipeline."
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "What Is This Class?",
    "section": "Readings",
    "text": "Readings\n\nSetting the Stage\n Conservation biogeography: assessment and prospects by Whitaker et al. (2005) provides an overview of the of geography in understanding ecosystem function and shaping conservation strategies.\n Economic geography, politics, and policy by Rickard (2020) provides a review of the role of geography in understanding responses to globalization.\n Revolutionary and counter revolutionary theory in geography and the problem of ghetto formation by David Harvey (1972) offers a scathing critique of quantitative geography (though Harvey was one of the founders of the field). See (Barnes 2009) for a relatively recent attempt to reconcile these views.\n\n\nTechnical Details\n\nThe syllabus, content, examples, and assignments pages for this class\n Happy Git and GitHub for the useR - all you really need to know to be a proficient user of git for version control and reproducible workflows.\n\n\n\nThinking Deeper\n Does scale exist? An epistemological scale continuum for complex human–environment systems by Steven Manson (2008) is one of my favorite summaries of the various definitions and confusion surrounding scale as a concept invoked in many disciplines.\n Spatial Scaling in Ecology by John Wiens (1989) describes the fundamental challenges of scale in Ecology."
  },
  {
    "objectID": "content/01-content.html#objectives",
    "href": "content/01-content.html#objectives",
    "title": "What Is This Class?",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDescribe the roles that geographic analysis in understanding the world.\nDefine what we mean by description, explanation, and prediction in geography.\nUnderstand the organization of the course, the approach to grading, and the requirements for the final project\nAccess the course examples and assignments and know the process for submitting assignments"
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "What Is This Class?",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "assignment/self-eval2.html",
    "href": "assignment/self-eval2.html",
    "title": "Self-reflection 2",
    "section": "",
    "text": "This is the second of three self-reflections that you’ll complete during the course. We’ll revisit your objectives and check-in on what I can do to help you get the most out of the second half of the course. This is our mid-semester “adjustment”. As such, I’m asking that you complete this in a timely fashion and turn it in by Oct 21. You’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/self-eval2.html#instructions",
    "href": "assignment/self-eval2.html#instructions",
    "title": "Self-reflection 2",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-2-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-2-xxx.qmd and give it a title (like M Williamson Self-Reflection 2). Make sure that you select the html output option.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-2-xx.qmd, and self-reflection-2-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Assignments",
    "section": "",
    "text": "The main goals of this class is class is to get you comfortable with the manipulation, analysis, and visualization of spatial data using the R computing environment. These assignments are designed to help you practice those skills and reflect on your progress. All of the assignments have their own repository in our GitHub classroom, so you’ll submit them there. I’ll transfer grades over to Canvas so you’ll have an idea where you’re at in the course, but we won’t use Canvas for much else."
  },
  {
    "objectID": "assignment/index.html#self-reflections",
    "href": "assignment/index.html#self-reflections",
    "title": "Assignments",
    "section": "Self-reflections",
    "text": "Self-reflections\nThis course is collaborative. I’m hoping to provide a broad suite of information that can help you analyze spatial data for your graduate research and beyond. That said, you know more than I do about your personal and professional objectives. During the course of the semester, I’ll ask you to submit 2 self-reflections. The first should help me get to know you and get us on the same page with respect to your goals for the course. The last provides you with an opportunity to reflect on your progress in the course relative to your own objectives and give me feedback on how I can improve the course. These self-reflections are the foundation of how you’ll be ‘graded’ in this course, so they are mandatory and need to be submitted by the due date"
  },
  {
    "objectID": "assignment/index.html#problem-sets",
    "href": "assignment/index.html#problem-sets",
    "title": "Assignments",
    "section": "Problem sets",
    "text": "Problem sets\nI’ve created ten assignments to practice outlining a workflow, writing R code, and troubleshooting errors. The assignments rely on these datatasetsare designed to provide “bite-sized” practice to help cement the topics covered in the (generally 2) lectures that each applies. That said, they may take some and I’d encourage you to start on them early so that we have time to work on any questions you have. Moreover, long coding days are the worst so working in bite size chunks can help keep things fun! Finally, working incrementally will also to encourage you to get in the habit of using git to keep track of your progress.\nI’ll be grading these according to: * Please Resubmit: This indicates that either your code does not run as written (i.e., your quarto document will not compile on my computer), you did not use Git as instructed, and/or that your responses to the questions I posed indicate that you do not quite understand the material as well as I would like. You’ll need to schedule an appointment to talk with me and we’ll work out what you need to do to get credit for the assignment. Although there isn’t a hard deadline for this resubmission, the assignments build on each other so it’s in your best interest to complete the resubmission before the next assignment. Failure to resubmit will result in no credit for the assignment.\n\nResubmit If You Like: This indicates that all of the code works as written and that you used Git, but that you may have missed some important concepts. Your are welcome to resubmit the assignment and address my comments to help polish the final product, but it is not required for you to get credit for the assignment.\nGood To Go: All of your code works, you completed the necessary Git steps, and all of the pieces are there and polished. I may have some minor comments, but I don’t need you to address them for this assignment.\n\n\n\n\n\n\n\nTip\n\n\n\nLate Work: There is no such thing as ‘late work’ with these assignments. Life happens, sometimes things take longer to finish than you expect. If you turn it in, I’ll give you feedback. That said, the assignments build on each other so it’s probably best to avoid falling too far behind."
  },
  {
    "objectID": "assignment/index.html#final-project",
    "href": "assignment/index.html#final-project",
    "title": "Assignments",
    "section": "Final project",
    "text": "Final project\nAt the end of the course, you will demonstrate your knowledge of spatial analysis workflows through a final project that requires you to integrate a variety of spatial datasets, analyze the data with respect to a question of interest, and create visuals that help you interpret the data.\nComplete details for the final project are here.\nThere is no final exam. This project is your final exam."
  },
  {
    "objectID": "assignment/data.html",
    "href": "assignment/data.html",
    "title": "Assignment Datasets",
    "section": "",
    "text": "In order to reduce the fatigue of learning new datasets for each assignment, we’ll try to keep things limited to a handful of point, vector, and raster datasets. I’m going to simplify them a bit for the sake of loading times, but you can take a look at the webpages for each dataset to get a sense for what the originals look like."
  },
  {
    "objectID": "assignment/data.html#fire-occurrence-data",
    "href": "assignment/data.html#fire-occurrence-data",
    "title": "Assignment Datasets",
    "section": "Fire Occurrence Data",
    "text": "Fire Occurrence Data\nThe US Forest Service maintains a geospatial archive of historic and ongoing fire occurrence ignition locations. It’s a massive dataset, but you can play around with a web-viewer to see the sorts of things they report."
  },
  {
    "objectID": "assignment/data.html#wildfire-risk-to-communities",
    "href": "assignment/data.html#wildfire-risk-to-communities",
    "title": "Assignment Datasets",
    "section": "Wildfire Risk to Communities",
    "text": "Wildfire Risk to Communities\nThis is a recent product that is being used to determine where priorities for fire mitigation and restoration should be located. They’ve been the tool underlying the most recent round of Community Wildfire Defense Grants."
  },
  {
    "objectID": "assignment/data.html#climate-and-economic-justice-screening-tool",
    "href": "assignment/data.html#climate-and-economic-justice-screening-tool",
    "title": "Assignment Datasets",
    "section": "Climate and Economic Justice Screening Tool",
    "text": "Climate and Economic Justice Screening Tool\nThe CEJST combines a number of interesting datasets to identify locations where environmental justice concerns are likely to be highest. We’ll generally be using the underlying data (not the classification itself), but it’s worth checking out just to get a sense for what’s there.\nIn general, I’ll do my best to keep us using these datasets and give you a heads up and description if we need to deviate (e.g., for the networks unit)."
  },
  {
    "objectID": "assignment/12-nets.html",
    "href": "assignment/12-nets.html",
    "title": "Assignment 10: Movement and Networks",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of working in R, developing programming workflows, and getting help. These topics are important for building robust spatial workflows, but their utility extends beyond that to most things you’ll do in R during the course of your graduate research. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/12-nets.html#instructions",
    "href": "assignment/12-nets.html#instructions",
    "title": "Assignment 10: Movement and Networks",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/10-secondrevision.html",
    "href": "assignment/10-secondrevision.html",
    "title": "Assignment 10: Revisiting your code (part 2)",
    "section": "",
    "text": "This is your second opportunity to reconsider your answers to the middle four assignments and evaluate what you might have done differently now that you’ve had a little more practice. I’ve also asked some specific questions based on common mistakes across the assignments. You’ll still be using Quarto to complete This homework."
  },
  {
    "objectID": "assignment/09-pointpatterns.html",
    "href": "assignment/09-pointpatterns.html",
    "title": "Assignment 8: Fitting Models",
    "section": "",
    "text": "You built a dataframe for analysis in Assignment 6 using the National Forest boundaries that we used in class (I’ve written a function for you to download and unzip this in the /code/ folder), a dataset describing the cost of natural disasters fromm 1999-2020 (described in this paper and available at /assignment07/ics209-plus-wf_incidents_1999to2020.csv folder. a land use dataset (/assignment07/land_use_pnw.tif), and the wildfire hazard dataset (/assignment07/wildfire_hazard_agg.tif). Now it’s time to fit some models to explain the cost of natural disasters. By the end of this assignment you should be able to:"
  },
  {
    "objectID": "assignment/09-pointpatterns.html#instructions",
    "href": "assignment/09-pointpatterns.html#instructions",
    "title": "Assignment 8: Fitting Models",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-8-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-8-xxx.qmd and give it a title (like M Williamson Assignment 8). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-8-xx.qmd, and assignment-8-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions arehere"
  },
  {
    "objectID": "assignment/08-combinations.html",
    "href": "assignment/08-combinations.html",
    "title": "Assignment 7: Autocorrelation and Interpolation",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of working in R, developing programming workflows, and getting help. These topics are important for building robust spatial workflows, but their utility extends beyond that to most things you’ll do in R during the course of your graduate research. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/08-combinations.html#instructions",
    "href": "assignment/08-combinations.html#instructions",
    "title": "Assignment 7: Autocorrelation and Interpolation",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here"
  },
  {
    "objectID": "assignment/07-rasterops.html",
    "href": "assignment/07-rasterops.html",
    "title": "Assignment 7: Building spatial databases",
    "section": "",
    "text": "It’s time to put together the various vector, raster, and tabular code we’ve learned in order to build a dataframe that you can use for a subsequent statistical analysis. For this assignment we’ll be using the National Forest boundaries that we used in class (I’ve written a function for you to download and unzip this in the /code/ folder), a dataset describing the cost of natural disasters fromm 1999-2020 (described in this paper and available at /assignment07/ics209-plus-wf_incidents_1999to2020.csv folder. a land use dataset (/assignment07/land_use_pnw.tif), and the wildfire hazard dataset (/assignment07/wildfire_hazard_agg.tif). Your goal is to create a dataframe that includes the total cost of all disasters that have occurred within a National Forest Boundary along with several social and ecological variables that might help explain the difference in dollars expended to contain the hazard (typically fire). By the end of this assignment you should be able to:"
  },
  {
    "objectID": "assignment/07-rasterops.html#instructions",
    "href": "assignment/07-rasterops.html#instructions",
    "title": "Assignment 7: Building spatial databases",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-7-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-7-xxx.qmd and give it a title (like M Williamson Assignment 7). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-7-xx.qmd, and assignment-7-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/07-rasterops.html#the-assignment",
    "href": "assignment/07-rasterops.html#the-assignment",
    "title": "Assignment 7: Building spatial databases",
    "section": "The Assignment",
    "text": "The Assignment\n\nYou’ll need to download the FS boundary shapefiles then load in all of the different spatial and tabular datasets.\nValidate your geometries and make sure all of your data is in the same CRS.\nSmooth the wildfire hazard and land use datasets using a 5s5 moving window; use the mean for the continuous dataset and the mode for the categorical dataset.\nEstimate the total cost of the incidents within each forest (PROJECTED_FINAL_IM_COST contains this value for each incident).\nNext join 3 attributes of your choosing from the CEJST and the extracted fire and land cover values to your dataframe.\nMake a set of maps that shows the Forest-level values for all of your selected variables.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions arehere"
  },
  {
    "objectID": "assignment/06-vectorops.html",
    "href": "assignment/06-vectorops.html",
    "title": "Assignment 6: Vector Operations",
    "section": "",
    "text": "Now that you’ve been introduced to predicates, measures, and transformers in the sf package. You should be able complete a relatively simple workflow for a spatial analysis. We’ll build on this again with raster data (using terra) next week and then integrate both data models the week after that. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/06-vectorops.html#instructions",
    "href": "assignment/06-vectorops.html#instructions",
    "title": "Assignment 6: Vector Operations",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-6-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-6-xxx.qmd and give it a title (like M Williamson Assignment 6). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-6-xx.qmd, and assignment-6-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com.\n\nSolutions are here"
  },
  {
    "objectID": "assignment/05-assignmentrev.html",
    "href": "assignment/05-assignmentrev.html",
    "title": "Assignment 5: Predicting Spatial Patterns",
    "section": "",
    "text": "Objectives\n\n\nInstructions\n\n\nQuestions to Answer"
  },
  {
    "objectID": "assignment/04-mapssolutions.html",
    "href": "assignment/04-mapssolutions.html",
    "title": "Assignment 4 Solutions: Predicates and Measures",
    "section": "",
    "text": "1. Load the cejst_nw.shp use the correct predicates to determine whether the geometries are valid and to check for empty geometries. If there are empty geometries, determine which rows have empty geometries (show your code). \n\nRemember that predicates return logical (i.e. TRUE or FALSE) answers so we are looking for functions with st_is_* to look for valid or empty geometries. We wrap those in the all() or any() function calls so that we get a single TRUE or FALSE for the entire geometry collection rather than returning the value for each individual observation. While those can be useful for figuring out if the entire dataset meets our criteria (i.e., all are valid or any have empty geometries), identifying which records have empty geometries takes an additional step. We use which() to return the row index of each record that returns a TRUE for st_is_empty() and then subset the original data using the [] notation keeping only the rows with empty geometries and all other columns.\n\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(terra)\n\ncejst.nw &lt;- read_sf(\"data/opt/data/2023/assignment04/cejst_nw.shp\")\nall(st_is_valid(cejst.nw))\nany(st_is_empty(cejst.nw))\nwhich(st_is_empty(cejst.nw))\n\ncejst.nw[which(st_is_empty(cejst.nw)),]\n\n 2. Load the landmarks_ID.csv table and convert it to an sf object. Now filter to just the hospital records (MTFCC == \"K1231\") and calculate the distance between all of the hospitals in Idaho. Note that you’ll have to figure out the CRS for the landmarks dataset… \n\nHere we are interested in distance which is a measure (not a predicate or transformer), but to get there we need to take a few extra steps. First, we read in the csv file and convert it to coordinates (using st_as_sf, a transformer). Then we use dplyr::filter to retain only the hospitals in the dataset. Finally, because this is a lat/long dataset, we assume a geodetic projection of WGS84 and assign it to the filtered object. Once we’ve gotten all that squared away, it’s just a matter of using the st_distance function to return the distance matrix for all objects in the dataset.\n\n\nhospitals.id &lt;- read_csv(\"data/opt/data/2023/assignment04/landmarks_ID.csv\") %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"lattitude\")) %&gt;% \n  filter(., MTFCC == \"K1231\")\nst_crs(hospitals.id) &lt;- 4269\n\ndist.hospital &lt;- st_distance(hospitals.id)\n\ndist.hospital[1:5, 1:5]\n\n 3. Filter the cejst_nw.shp to just those records from Ada County. Then filter again to return the row with the highest annual loss rate for agriculture (2 hints: you’ll need to look at the columns.csv file in the data folder to figure out which column is the expected agricultural loss rate and you’ll need to set na.rm=TRUEwhen looking for the maximum value). Calculate the area of the resulting polygon.\n\nThis one should be relatively straightforward. We start with another call to dplyr::filter to get down to just the tracts in Ada County (note the use of the & to combine two logical calls). Then we use a second filter to return the row with the max value for agricultural loss. Note that we have to use the na.rm=TRUE argument to avoid having the NA values force the function to return NA.\n\n\nada.cejst &lt;- cejst.nw %&gt;% \n  filter(., SF == \"Idaho\" & CF == \"Ada County\") \n\nada.max.EALR &lt;- ada.cejst %&gt;%  \n  filter(., EALR_PFS == max(EALR_PFS, na.rm = TRUE))\n  \nada.max.EALR[, c(\"SF\", \"CF\", \"EALR_PFS\")]\n\n 4. Finally, look at the helpfile for the terra::adjacent command. How do you specify which cells you’d like to get the adjacency matrix for? How do you return only the cells touching your cells of interest? Use the example in the helpfile to illustrate how you’d do this on a toy dataset - this will help you learn to ask minimally reproducible examples.\n\nWe can access the helpfile for adjacent by using ?terra::adjacent (I won’t do that here because I don’t want to print the entire helpfile). From that we can see that the cells argument is the place to specify which cells we are interested in. also see that the directions argument allows us to specify whether we want “rook”, “bishop”, or “queen” neighbors. Finally, we see that if we want to exclude the focal cell itself, we have to set include to FALSE. By plotting the map with the cell numbers, we can see that cells 1 and 5 are on th top row of the raster and thus do not have any neighbors for for the upper 3 categories whereas cell 55 hase all 8 neighbors. If you choose cells that are in the center of the raster, you get all neighbors\n\n\nr &lt;- rast(nrows=10, ncols=10)\ncellnum &lt;- cells(r)\nr[] &lt;- cellnum\nplot(r)\nadjacent(r, cells=c(1, 5, 55), directions=\"queen\", include=FALSE)\n\nadjacent(r, cells=c(51, 52, 55), directions=\"queen\", include=FALSE)"
  },
  {
    "objectID": "assignment/04-assignmentrev.html",
    "href": "assignment/04-assignmentrev.html",
    "title": "Assignment 4: Explaining Spatial Patterns",
    "section": "",
    "text": "Objectives\n\n\nInstructions\n\n\nQuestions to Answer"
  },
  {
    "objectID": "assignment/03-vectorsolutions.html",
    "href": "assignment/03-vectorsolutions.html",
    "title": "Assignment 3 Solutions: Coordinates and Geometries",
    "section": "",
    "text": "1. Write out the pseudocode that you would use to set up an analysis of the spatial correlations between chronic asthma risk, exposure to PM2.5, and wildfire. You don’t have to write functions or any actual code. Just write the steps and insert named code blocks for each step.\n\nThis one is probably a little tricky if you haven’t taken the time to check out the attributes of the data (which you should always do). That said, some pretty generic steps would be:\n\n\n1. Load each dataset\n2. Check geometry validity\n3. Align CRS\n4. Run Correlation\n5. Print Results\n\n\nThere are two key steps here, that you’ll repeat for any/all spatial analyses that you do: 1) checking for valid geometries and 2) making sure the data are aligned in a sensible CRS. I can add a code chunk for each now.\n\n\n1. Load each dataset\n\n\n2. Check geometry validity\n\n\n3. Align CRS\n\n\n4. Run Correlation\n\n\n5. Print Results\n\n2. Read in the cdc_nw.shp, pm_nw.shp, and wildfire_hazard_agg.tif files and print the coordinate reference system for each object. Do they match?\n\nHere I’m going to combine the load portion of my pseudocode with the validity since I can do that without creating additional object. I use the str() function to get a sense for what the data looks like and to understand what data classes I’m working with. Then, I use the all() function to make sure that all of the results of st_is_valid() are true. I don’t need to do that with the raster file as the geometry is implicit which means that it has to be topologically valid (this doesn’t mean that the numbers are accurate, it just means that the dataset conforms to the data model R expects). Then I’ll add another code to check the CRS of the different objects.\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\n\ncdc.nw &lt;- read_sf(\"data/opt/data/2023/assignment03/cdc_nw.shp\")\nstr(cdc.nw)\nall(st_is_valid(cdc.nw))\npm.nw &lt;- read_sf(\"data/opt/data/2023/assignment03/pm_nw.shp\")\nstr(pm.nw)\nall(st_is_valid(pm.nw))\n\nwildfire.haz &lt;- rast(\"data/opt/data/2023/assignment03/wildfire_hazard_agg.tif\")\nstr(wildfire.haz)\n\n\nNow that I’ve gotten the data into my environment, I need to make sure that the CRS are alligned. I’ll demonstrate that with a few different approaches. You can use the logical == or the identical function to check, but remember that these fnctions are not specific to spatial objects, they evaluate things very literally. So even if the CRS is the same, if st_crs returns the CRS in one format (WKT) and crs returns it in another, you’ll get FALSE even if they are actually the same CRS - pay attention to that. You’ll notice that they aren’t identical; we’ll deal with that in the next question.\n\n\nst_crs(cdc.nw)\nst_crs(pm.nw)\ncrs(wildfire.haz)\n\nidentical(st_crs(cdc.nw), st_crs(pm.nw))\nst_crs(cdc.nw) == st_crs(pm.nw)\n\n3. Re-project the cdc_nw.shp and pm_nw.shp shapefiles so that they have the same CRS as the wildfire_hazard_agg.tfi file. Verify that all the files have the same projection.\n\nNow we’ll use st_transform to get the two shapefiles aligned with the raster (because we generally want to avoid projecting rasters if we can). We can then use the same steps above to see if they’re aligned. Note that I’m using the terra::crs() function to make sure that the output is printed in exactly the same format\n\n\ncdc.nw.proj &lt;- cdc.nw %&gt;% st_transform(., crs=crs(wildfire.haz))\npm.nw.proj &lt;- pm.nw %&gt;% st_transform(., crs=crs(wildfire.haz))\n\nidentical(crs(cdc.nw.proj), crs(wildfire.haz))\nidentical(crs(pm.nw.proj), crs(wildfire.haz))\n\n4. How does reprojecting change the coordinates of the bounding box for the two shapefiles? Show your code\n\nNow we just want to look at the bounding box of the data before and after it was projected. We can do this using st_bbox. One of the most obvious changes is that the units for cdc.nw have changed from degrees to meters (as evidenced by the much larger numbers). For the pm.nw object we can see that the raw coordinates indicate a shift to the west; however, because the origin for this crs has also changed, the states still show up in the correct place.\n\n\nst_bbox(cdc.nw)\n\nst_bbox(cdc.nw.proj)\n\nst_bbox(pm.nw)\n\nst_bbox(pm.nw.proj)\n\n5. What class of geometry does the pm_nw.shp have (show your code)? Now filter the pm_nw.shp file so that only the records from Ada County, Idaho are showing. Find the record with the lowest value for PM25. How many coordinates are associated with that geometry?\n\nThis one was probably a little tricky. First, to check the geometry type, we use st_geometry_type setting by_geometry to FALSE means we get the geometry type for the entire object instead of each observation. We then use a series of filter commands to get the records from Idaho and Ada county. Once we’ve narrowed the data to our correct region, we can filter again to find the row with the minimum value of PM25 (note that we have to set na.rm=TRUE so that we ignore the NA values). Then we just take the number of rows (nrow) of the result of st_coordinates to get the number of coordinates associated with that geometry.\n\n\nst_geometry_type(pm.nw, by_geometry = FALSE)\n\nada.pm &lt;- pm.nw %&gt;% \n  filter(STATE_NAME==\"Idaho\" & CNTY_NAME==\"Ada\") %&gt;% \n  filter(PM25 == min(PM25, na.rm = TRUE))\n\nada.pm\n\nnrow(st_coordinates(ada.pm))"
  },
  {
    "objectID": "assignment/03-assignmentrev.html",
    "href": "assignment/03-assignmentrev.html",
    "title": "Assignment 3: Describing Spatial Patterns",
    "section": "",
    "text": "Objectives\n\n\nInstructions\n\n\nQuestions to Answer"
  },
  {
    "objectID": "assignment/02-introspatialsolutions.html",
    "href": "assignment/02-introspatialsolutions.html",
    "title": "Assignment 2 Solutions: Intro to Spatial Data",
    "section": "",
    "text": "Find a figure that you’d like to mimic with your research. The figure should be from a manuscript or report and present the results of a quantitative aaalysis (i.e., not a conceptual model or an image). Once you’ve found one you should:\n1. Create a section called “Introduction” in your Quarto document. In that section, you should give me the citation for the article and a brief description (similar to the caption) of the figure.\n\nIn order to do this, you’ll need to start a new Quarto document (File -&gt; New File -&gt; Quarto Document). Once you’ve done that Rstudio will open up a Quarto document with the yaml header already in place and a bunch of example text and code. Delete that. Then use Markdown syntax to specify headings (# for top level headings, ## for second level headings, etc.) So in this case, once you’ve gotten all of the example stuff deleted, you can use # Introduction to create a section header with the correct title. Adding citations requires you to create a separate, .bib file that lives in the same directory as your document and has your citation info in BIBTEX format. You can find more on that here.\n\n2. Create a second section called “Methods” and write out the steps necessary to create the figure. These should be similar to the pseudocode we discussed in class (e.g., “Load Data”, “Summarize by county”, “Run linear regression”, “Build Figure”). The methods section of the manuscript you’ve chosen should provide you with enough information to begin sketching this out. Don’t worry if you don’t know all of the steps, the goal is to get you thinking about the “mile markers” along the way to creating the figure.\n\nNow I’ll add # Methods as my next header and start to write out the steps I’d like the analysis to follow. There are lots of ways you might do this, depending on whether you want the pseudocode to be part of your final product. For now we’ll keep it simple and just use numbered steps like:\n\n\nLoad data\nFilter the correct rows\nSelect the right variables\nModel y as a function of x1, x2, x3…\nPlot\n\n3. Add in code blocks for each step in your pseudocode. Give each block a name that corresponds to your pseudocode steps.\n\nAdding in code blocks and giving is accomplished by setting up your code fence (```), identifying the language you want to use ({r}), and then setting code options with the hash-pipe (#|). You can see my example here\n\n4. Based on the webpage linked above and the “Execution Options” section linked there, add execution options to each block that ensure that the code block will be printed, but not evaluated.\n\nIn order to ensure that the code prints, you need to add the #| echo: true execution option. In order to prevent it from running, you want to set #| eval: false. You can see this in my example.\n\n5. Add a “Results” section and use the markdown command to include an image of the figure from the manuscript you chose. \n\nYou can add images in markdown by using the ![]() syntax where the file location is pasted in the parentheses and any caption is placed in the brackets."
  },
  {
    "objectID": "assignment/02-example.html",
    "href": "assignment/02-example.html",
    "title": "Quarto Example",
    "section": "",
    "text": "Introduction\nI want to reproduce the figure from (Dash Nelson 2016) depicting commute networks in the United States based on US Census data.\n\n\nMethods\nSome pseudocode:\n\n1. Retrieve ACS commute data\n2. Identify the source and destination networks\n3. Calculate the edge density for each source-destination pair\n4. Thin to a manageable number of nodes based on edge densities\n5. plot\n\nas code chunks\n\n```{r}\n#| eval: false\n#| label: getacs\n\n1. Retrieve ACS commute data\n```\n\n\n```{r}\n#| eval: false\n#| label: buildnet\n\n2. Identify the source and destination networks\n```\n\n\n```{r}\n#| eval: false\n#| label: edgedens\n\n3. Calculate the edge density for each source-destination pair\n```\n\n\n```{r}\n#| eval: false\n#| label: thinnodes\n\n4. Thin to a manageable number of nodes based on edge densities\n```\n\n\n```{r}\n#| eval: false\n#| label: buildplot\n\n5. Build plot\n```\n\n\n\nResults\n\n\n\nCommute Networks from Dash Nelson and Rae 2016\n\n\n\n\n\n\n\nReferences\n\nDash Nelson, A., Garrett AND Rae. 2016. An economic geography of the united states: From commutes to megaregions. PLOS ONE 11:1–23."
  },
  {
    "objectID": "assignment/02-assignment.html",
    "href": "assignment/02-assignment.html",
    "title": "Assignment 2: Fundamentals of Spatial Data",
    "section": "",
    "text": "Objectives\n\n\nInstructions\n\n\nQuestions to Answer"
  },
  {
    "objectID": "assignment/01-intro.html",
    "href": "assignment/01-intro.html",
    "title": "Assignment 1: Introductory material",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of geographic thought, core technical details of working with spatial data, and introduce R as a tool for end-to-end spatial workflows. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. There are additional step-by-step guides in the By the end of this assignment you should be able to:"
  },
  {
    "objectID": "assignment/01-intro.html#instructions",
    "href": "assignment/01-intro.html#instructions",
    "title": "Assignment 1: Introductory material",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 2 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/01-intro.html#questions-for-the-assignment",
    "href": "assignment/01-intro.html#questions-for-the-assignment",
    "title": "Assignment 1: Introductory material",
    "section": "Questions for the Assignment",
    "text": "Questions for the Assignment\n\nHow does geographic analysis fit into your goals for your research? Given our discussion of the aims and limitations of geographic analysis, are there particular issues that you would like to know more about or guard against?\nWhat are the primary components that describe spatial data?\nWhat is the coordinate reference system and why is it important\nFind two maps of the same area in different projections? How does the projection affect your perception of the data being displayed?\nRead in the cejst.shp file in the assignment01 folder. How many attributes describe each object? How many unique geometries are there? What is the coordinate reference system?\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here"
  },
  {
    "objectID": "assignment/01-assignment.html",
    "href": "assignment/01-assignment.html",
    "title": "Assignment 1: Getting Started",
    "section": "",
    "text": "Objectives\n\n\nInstructions\n\n\nQuestions to Answer"
  },
  {
    "objectID": "assignment/01-assignmentrev.html",
    "href": "assignment/01-assignmentrev.html",
    "title": "Assignment 1: Getting Started",
    "section": "",
    "text": "Objectives\n\n\nInstructions\n\n\nQuestions to Answer"
  },
  {
    "objectID": "assignment/01-introsolutions.html",
    "href": "assignment/01-introsolutions.html",
    "title": "Assignment 1 Solutions: Introductory material",
    "section": "",
    "text": "How does geographic analysis fit into your goals for your research? Given our discussion of the aims and limitations of geographic analysis, are there particular issues that you would like to know more about or guard against?\n\nAlmost all of my research has to do with geographic analysis and basic geographic concepts. I suppose that’s why I teach this course. For me, one of the biggest challenges is figuring out how to a) choose a scale of analysis that matches the scale of the phenomena that I’m interested in and b) reconcile the fact that most of the data I have access to was rarely collected at that specific scale. I’m constantly looking for new methods to try and determine if and when my results are dependent upon some arbitrary choice of extent and resolution.\n\nWhat are the primary components that describe spatial data?\n\nI would say that the primary components are the coordinate reference system (because it helps us understand where we actually are on Earth), the extent of the data (because that helps me know what scale we’re working with and the size of the computational probelem), the resolution (same reason as extent), the geometry, and spatial support. I don’t think about this last one often enough, but it really is the key to honest interpretation of the spatial data that you have.\n\nWhat is the coordinate reference system and why is it important\n\nThe CRS consists of the information necessary to locate points in 2 or 3 dimensional space. Coordinates are only meaningful in the context of a CRS (i.e., (2,2) could describe any number of places in the world - we need to know the origin and the datum to actually know where that is). The CRS becomes particularly important when we need to align datasets that were not collected in the same CRS originally or when we need to transfer locations from the globe to a flat surface (e.g., map, screen, etc).\n\nFind two maps of the same area in different projections? How does the projection affect your perception of the data being displayed?\nHere’s a fun article on projections that shows what i’m talking about!\nRead in the cejst.shp file in the assignment01 folder. How many attributes describe each object? How many unique geometries are there? What is the coordinate reference system?\nI can read in the data using st_read or read_sf\n\n```{r}\n#| message: false\nlibrary(sf)\n\ncejst.sf &lt;- read_sf(\"data/opt/data/2023/assignment01/cejst_nw.shp\")\ncejst.st &lt;- st_read(\"data/opt/data/2023/assignment01/cejst_nw.shp\")\n```\n\nYou can inspect the differences between the resulting object classes by calling class\n\n```{r}\n#| message: false\nlibrary(sf)\n\nclass(cejst.sf)\nclass(cejst.st)\n```\n\nYou’ll notice that using st_read assigns the object to an sf and data.frame class meaning that functions defined for those two classes will work. Alternatively, read_sf assigns the object to sf, tbl_df, tbl, and data.frame classes meaning that a much broader set of functions can be run on the cejst.sf object.\nBecause the data are in wide format, we can assume that there is only 1 observation for each location (because sf requires that there is a geometry entry for every observation (even if it’s empty)). Probably the easiest way to get the number of observations is:\n\n```{r}\nnrow(cejst.sf)\n```\n\nSimilarly, if we wanted to know how many attributes are collected for each observation we could use ncol:\n\n```{r}\nncol(cejst.sf)\n```\n\nNote that these are really only approximate estimates. There’s usually a lot of extra ID-style columns in spatial data such that the number of columns with useful information is less than the total number of columns, but we won’t worry about that for now."
  },
  {
    "objectID": "assignment/02-assignmentrev.html",
    "href": "assignment/02-assignmentrev.html",
    "title": "Assignment 2: Fundamentals of Spatial Data",
    "section": "",
    "text": "Objectives\n\n\nInstructions\n\n\nQuestions to Answer"
  },
  {
    "objectID": "assignment/02-introspatial.html",
    "href": "assignment/02-introspatial.html",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "",
    "text": "This is the second assignment of the semester for HES 505.\nFor the rest of the course, I’ll be asking you to use pseudocode to plan your analysis steps before you start using any functions (or writing your own). Pseudocode allows you to think about the important steps of your process and identify your desired results before your start down the path of coding. You can think of pseudocode as an outline for syntax, much like the one you might use for writing an manuscript or report. Quarto documents are designed to let you both outline your report and plan your analysis all in the same place! This assingment is meant to give you some practice setting up your outlines before you start coding. By the end of this assignment you should be able to:"
  },
  {
    "objectID": "assignment/02-introspatial.html#instructions",
    "href": "assignment/02-introspatial.html#instructions",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-2-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-2-xxx.qmd and give it a title (like M Williamson Assignment 2). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-2-xx.qmd, and assignment-2-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/02-introspatial.html#the-assignment",
    "href": "assignment/02-introspatial.html#the-assignment",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "The Assignment",
    "text": "The Assignment\nFind a figure that you’d like to mimic with your research. The figure should be from a manuscript or report and present the results of a quantitative aaalysis (i.e., not a conceptual model or an image). Once you’ve found one you should:\n\nCreate a section called “Introduction” in your Quarto document. In that section, you should give me the citation for the article and a brief description (similar to the caption) of the figure.\nCreate a second section called “Methods” and write out the steps necessary to create the figure. These should be similar to the pseudocode we discussed in clase (e.g., “Load Data”, “Summarize by county”, “Run linear regression”, “Build Figure”). The methods section of the manuscript you’ve chosen should provide you with enough information to begin sketching this out. Don’t worry if you don’t know all of the steps, the goal is to get you thinking about the “mile markers” along the way to creating the figure.\nAdd in code blocks for each step in your pseudocode. Give each block a name that corresponds to your pseudocode steps.\nBased on the webpage linked above and the “Execution Options” section linked there, add execution options to each block that ensure that the code block will be printed, but not evaluated.\nAdd a “Results” section and use the markdown command to include an image of the figure from the manuscript you chose.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here"
  },
  {
    "objectID": "assignment/03-assignment.html",
    "href": "assignment/03-assignment.html",
    "title": "Assignment 3: Describing Spatial Patterns",
    "section": "",
    "text": "Objectives\n\n\nInstructions\n\n\nQuestions to Answer"
  },
  {
    "objectID": "assignment/03-vector.html",
    "href": "assignment/03-vector.html",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "",
    "text": "This is the third assignment of the semester for HES 505. The last few lectures have focused on coordinates and gemoetries. In this assignment, we’ll ude the different functions for accessing and transforming the crs of different spatial objects. We’ll also use a little of the tidyverse to subset the data and access some of the geometry information for one of the observations in our dataset. You’ll need to use both the lectures and the recorded examples (or check out the tidyverse tutorials linked in the lectures). This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/03-vector.html#instructions",
    "href": "assignment/03-vector.html#instructions",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-3-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-3-xxx.qmd and give it a title (like M Williamson Assignment 3). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-3-xx.qmd, and assignment-3-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/03-vector.html#the-data",
    "href": "assignment/03-vector.html#the-data",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "The Data",
    "text": "The Data\nFor this assignment, you’ll be lookcing at 3 different datasets. One from the Center for Disease Control’s PLACES data describing the distribution of chronic health risks, one from the EPA describing exposure to PM2.5 (an important air pollutant), and one describing wildfire risk. You might imagine that as we become increasingly concerned with the evironmental justice concerns associated with fire, we might be concerned about whether more smoke increases the risk of chronic respiratory diseases. We won’t totally answer that question this week, but you’ll start to develop the workflow necessary to move towards that type of analysis. All of the data are on the server in the opt/data/2023/assignment03/ folder.\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here"
  },
  {
    "objectID": "assignment/04-assignment.html",
    "href": "assignment/04-assignment.html",
    "title": "Assignment 4: Explaining Spatial Patterns",
    "section": "",
    "text": "Objectives\n\n\nInstructions\n\n\nQuestions to Answer"
  },
  {
    "objectID": "assignment/04-maps.html",
    "href": "assignment/04-maps.html",
    "title": "Assignment 4: Predicates and Measures",
    "section": "",
    "text": "This is the fourth assignment of the semester for HES 505.\nNow that you’ve learned a bit about predicates and measures it’s time to practice on some vector and raster data. By the end of this assignment, you should be able to:"
  },
  {
    "objectID": "assignment/04-maps.html#instructions",
    "href": "assignment/04-maps.html#instructions",
    "title": "Assignment 4: Predicates and Measures",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-4-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-4-xxx.qmd and give it a title (like M Williamson Assignment 4). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-4-xx.qmd, and assignment-4-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/04-maps.html#the-data",
    "href": "assignment/04-maps.html#the-data",
    "title": "Assignment 4: Predicates and Measures",
    "section": "The Data",
    "text": "The Data\nWe will be using the landmarks data table, the shapefile from the Climate and Economic Justice Screening Tool, and the wildfire hazard raster from the previous two assignments. I’ve moved the versions for this assignment into the opt/data/2023/assignment04/ folder to make life easier."
  },
  {
    "objectID": "assignment/04-maps.html#the-assignment",
    "href": "assignment/04-maps.html#the-assignment",
    "title": "Assignment 4: Predicates and Measures",
    "section": "The Assignment",
    "text": "The Assignment\n\nLoad the cejst_pnw.shp use the correct predicates to determine whether the geometries are valid and to check for empty geometries. If there are empty geometries, determine which rows have empty geometries (show your code).\nLoad the landmarks_ID.csv table and convert it to an sf object. Now filter to just the hospital records (MTFCC == \"K1231\") and calculate the distance between all of the hospitals in Idaho. Note that you’ll have to figure out the CRS for the landmarks dataset…\nFilter the cejst_pnw.shp to just those records from Ada County. Then filter again to return the row with the highest annual loss rate for agriculture (2 hints: you’ll need to look at the columns.csv file in the data folder to figure out which column is the expected agricultural loss rate and you’ll need to set na.rm=TRUEwhen looking for the maximum value). Calculate the area of the resulting polygon.\nFinally, look at the helpfile for the terra::adjacent command. How do you specify which cells you’d like to get the adjacency matrix for? How do you return only the cells touching your cells of interest? Use the example in the helpfile to illustrate how you’d do this on a toy dataset - this will help you learn to ask minimally reproducible examples.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here"
  },
  {
    "objectID": "assignment/05-assignment.html",
    "href": "assignment/05-assignment.html",
    "title": "Assignment 5: Predicting Spatial Patterns",
    "section": "",
    "text": "Objectives\n\n\nInstructions\n\n\nQuestions to Answer"
  },
  {
    "objectID": "assignment/05-firstrevision.html",
    "href": "assignment/05-firstrevision.html",
    "title": "Assignment 5: Revisiting your code (part 1)",
    "section": "",
    "text": "Now that you’ve had some practice with R and the format of the course, it’s time to pause and take a moment to check in on what you’ve learned. Because we haven’t had a ton of coding yet, this review is a little more conceptual (rather than focusing on particular pieces you may have done incorrectly or inefficiently). My soluionts (or suggestions) for how I’d approach the first four assignments are posted (at the end of each assignment page). Your task here is to review those solutions and your own code and answer a few questions to demonstrate what you’ve learned so far and where I need to be more clear. You’ll still be using Quarto to complete This homework."
  },
  {
    "objectID": "assignment/05-firstrevision.html#instructions",
    "href": "assignment/05-firstrevision.html#instructions",
    "title": "Assignment 5: Revisiting your code (part 1)",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-revision-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file ar1_xxx.qmd and give it a title (like M Williamson Assignment Revision 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions in the assignment into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 3 commits is part of the assignment).\nRender the document (by clicking the “Render” button in RStudio) to html (you should now have at least 3 files in the repository: Readme.md, ar1_xx.qmd, and ar1_xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/06-vectoropssolutions.html",
    "href": "assignment/06-vectoropssolutions.html",
    "title": "Assignment 6 Solutions: Vector Operations",
    "section": "",
    "text": "We want to begin to assess the role of distance from schools in determining the education outcomes for Idahoans. We’ll use the landmarks_pnw.csv and cejst_pnw.shp datasets as the basis for this assignment. You’ll need to load the csv and convert it to an sf object. We want to compare the percentage of individuals age 25 or over with less than a high school degree (HSEF in the cejst dataset) for of counties within 50km of a school (MTFCC == K2543) to those that are more than 50km. \nYou’ll need to follow many of the same operations in the video example from class. Your assignment is:\n1. Write out the pseudocode for your analysis\n\nWe’ll need to do a few things here including load the data, find the tracts within 50km of a school, and then compare the cejst results. Breaking that into pseudocode would look like:\n\n\n1. Load each dataset\n2. Subset to schools\n3. Check geometry validity\n4. Align CRS\n5. Find tracts within 50km\n6. Make Maps\n\n\nNote that my fift step (find tracts within 50km) is a little vague. There are lots of ways I could do this. It might be more helpful to add some specificity here like:\n\n\n1. Load each dataset\n2. Subset to schools\n3. Check geometry validity\n4. Align CRS\n5. Buffer schools by 50km\n6. Select tracts within the buffer and attribute\n7. Make Maps\n\n\nThere are other ways to do this too (like calculating the distance), but those are likely to be more computationally intensive so I’ll leave it at this.\n\n2. Translate the pseudocode into code chunks and create the necessary code (You’ll need to use things like st_distance, st_buffer, st_sym_difference)\n\nLoading the data should be pretty straightforward for you by now. We use read_sf for the shapefile and read_csv for the landmarks.csv. We then filter the data here so that we aren’t working with the entire landmarks dataset.\n\n\nlibrary(sf)\nlibrary(tidyverse, quietly = TRUE)\nlibrary(tmap, quietly = TRUE)\ncejst.pnw &lt;- read_sf(\"data/opt/data/2023/assignment06/cejst_pnw.shp\")\nlandmarks.pnw &lt;- read_csv(\"data/opt/data/2023/assignment06/landmarks_pnw.csv\") %&gt;% \n  filter(., MTFCC == \"K2543\")\n\n\nWe know that one of the datasets are still in long/lat form so we’ll need to make it a sf object before checking the geometry makes any sense. We’ll also assign the crs here by adding it to the st_as_sf call. We also need to make sure that there aren’t any empty geometries as that will cause problems for mapping later.\n\n\nlandmarks.sf &lt;- landmarks.pnw %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"latitude\"), crs=4269)\nall(st_is_valid(cejst.pnw))\nall(st_is_valid(landmarks.sf))\n\nany(st_is_empty(cejst.pnw))\nany(st_is_empty(landmarks.sf))\n\n\nLooks like all the geometries are valid, but there are some empty geometries in the cejst dataset. We will drop those by using filter combined with the negation operator (!) and st_is_empty to return the rows where st_is_empty is not equal to TRUE. Then we can move forward with making sure the two datasets are aligned by using st_transform to change the CRS. We can verify the alignment using a simple call to the plot function.\n\n\ncejst.pnw &lt;- cejst.pnw %&gt;% \n  filter(., !st_is_empty(.))\nlandmarks.proj &lt;- landmarks.sf %&gt;% \n  st_transform(., crs=st_crs(cejst.pnw))\nplot(st_geometry(cejst.pnw))\nplot(st_geometry(landmarks.proj), add=TRUE, col=\"red\")\n\n\nNow it’s time to find the tracts that are within 50km of a school. We can do this a few ways. First, we’ll use the st_buffer approach. We can also calculate the distance matrix between the schools and the tracts using st_distance. This adds a little more complexity as we have to then find the values that are greater than 50km. You’ll notice that st_distance returns a matrix with a row for each school and a column for each tract. This is a little clumsier to deal with, but more precise than a simple buffer.\n\n\nschool.buf &lt;- landmarks.proj %&gt;% \n  st_buffer(., dist=50000) \n\nschool.dist &lt;- st_distance(landmarks.proj, cejst.pnw)\ndim(school.dist)\n\n\nOnce we have the buffered “footprint” of the school we can use st_filter (which filters using topological relations) combined with the st_covered_by predicate to find all of the cejst.pnw tracts that are covered by the buffer. Notice that if we use the typical [] subset we get over 200 more records. This is because the latter takes all tracts with an intersection (rather than using our covered by criteria.). We can alter this to achieve the same result as the st_filter by adding the op= argument. Using the distance to the points themselves can be a more conservative way of calculating this, but takes a little more work. First we have to get a list of all of the tracts that fall within 50km (using st_is_within_distance), then identify which of those list elements are empty (i.e., no schools are within 50km), then set that as an index to subset our cejst data.\n\n\nschool.tracts.stf &lt;- cejst.pnw %&gt;% \n  st_filter(x =., y = school.buf, .predicate = st_covered_by)\nschool.tracts.sbst &lt;- cejst.pnw[school.buf,]\nschool.tracts.sbst2 &lt;- cejst.pnw[school.buf,, op=st_covered_by]\n\nnrow(school.tracts.stf)\nnrow(school.tracts.sbst)\nnrow(school.tracts.sbst2)\n\nidentical(school.tracts.stf, school.tracts.sbst2)\n\nwithin50 &lt;- st_is_within_distance(cejst.pnw, landmarks.proj, dist=50000, sparse = TRUE)\nwithin50.idx &lt;- lengths(within50) &gt; 0\nschool.tracts.sbst3 &lt;- cejst.pnw[within50.idx,]\nnrow(school.tracts.sbst3)\n\n3. Make a map for both the percentage of individuals with less than a high school degree in counties within 50km and beyond 50km (i.e. make 2 maps)\n\nWe now have the full cejst dataset and a dataset that is subsetted to the tracts within 50km of a school. Plotting the HSEF values for the tracts within 50km of a school is easy enough. Just map a layer that contains all of the tracts and set it’s color to gray. Then layer the subsetted features on top. Plotting the values of HSEF for the tracts beyond 50km is a little trickier (because we haven’t created that dataset yet). We can use the index we created in the previous step to do that here. We can also use the mutate function to create an indicator variable using our index and then create a “small multiples” style map that plots the two side by side. We’ll learn more about “prettying” up these maps in the later parts of the course.\n\n\ntm_shape(cejst.pnw) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(school.tracts.sbst3) +\n  tm_fill(col=\"HSEF\")\n\nnoschool.tracts &lt;- cejst.pnw[!within50.idx,]\ntm_shape(cejst.pnw) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(noschool.tracts) +\n  tm_fill(col=\"HSEF\")\n\nschool.combined &lt;- cejst.pnw %&gt;% \n  mutate(., indist = if_else(lengths(within50) &gt; 0, \"within50km\", \"notWithin50km\"))\n\n\ntm_shape(cejst.pnw) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(school.combined) +\n  tm_fill(col=\"HSEF\") +\n  tm_facets(by = c(\"indist\"), nrow = 1)"
  },
  {
    "objectID": "assignment/07-rasteropssolutions.html",
    "href": "assignment/07-rasteropssolutions.html",
    "title": "Assignment 7 Solutions: Building Spatial Databases",
    "section": "",
    "text": "1. You’ll need to download the FS boundary shapefiles then load in all of the different spatial and tabular datasets. &gt;The trickiest part here is just downloading the FS data (made more difficult because they updated the data on 26 Nov 2023). Once you load the function, you should be able to run it using the current link to the Administrative Forest Boundaries data. Because we already know there are empty geometries in the cejst dataset, we go ahead and drop those here.\n\nlibrary(sf)\nlibrary(tidyverse, quietly = TRUE)\nlibrary(terra)\nlibrary(tmap, quietly = TRUE)\n\n download_unzip_read &lt;- function(link){\n  tmp &lt;- tempfile()\n  download.file(link, tmp)\n  tmp2 &lt;- tempfile()\n  unzip(zipfile=tmp, exdir=tmp2)\n  shapefile.sf &lt;- read_sf(tmp2)\n  }\n\nfs.shapefile &lt;- download_unzip_read(link = \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.AdministrativeForest.zip\")\n\ncejst.pnw &lt;- read_sf(\"data/opt/data/2023/assignment07/cejst_pnw.shp\")%&gt;% \n  filter(., !st_is_empty(.))\n\nincidents.csv &lt;- read_csv(\"data/opt/data/2023/assignment07/ics209-plus-wf_incidents_1999to2020.csv\")\n\nland.use &lt;- rast(\"data/opt/data/2023/assignment07/land_use_pnw.tif\")\nfire.haz &lt;- rast(\"data/opt/data/2023/assignment07/wildfire_hazard_agg.tif\")\n\n2. Validate your geometries and make sure all of your data is in the same CRS. &gt;We start by checking to make sure the rasters have the same CRS (because we’d like to avoid projecting rasters). They don’t so we’ll use the land use data as the template as the interpolation for continuous data is a little less error prone. We use the terra::project function to reproject the rasters and then use sf::st_transform to reproject all of the vector data. You’ll notice that some of the coordinates for the incidents are NA we need to get rid of those before we can convert to an sf object. We do that with a call to filter before using st_as_sf.\n\ncrs(land.use) \ncrs(fire.haz)\n\nfire.haz.proj &lt;- project(fire.haz, land.use)\ncrs(fire.haz.proj) == crs(land.use)\n\nfs.shapefile.proj &lt;- fs.shapefile %&gt;% \n  st_transform(., crs=crs(land.use))\n\ncejst.proj &lt;- cejst.pnw %&gt;% \n  st_transform(., crs=crs(land.use))\n\nincidents.proj &lt;- incidents.csv %&gt;% \n  filter(., !is.na(POO_LONGITUDE) | !is.na(POO_LATITUDE) ) %&gt;% \n  st_as_sf(., coords = c(\"POO_LONGITUDE\", \"POO_LATITUDE\"), crs= 4269) %&gt;% \n  st_transform(., crs=crs(land.use))\n\n\nOnce we’ve gotten everything into the same CRS, we can check for valid geometries. Unfortunately, the FS boundary file has some invalid geometries, so we’ll fix those with st_make_valid.\n\n\nall(st_is_valid(fs.shapefile.proj))\nall(st_is_valid(cejst.proj))\nall(st_is_valid(incidents.proj))\n\nfs.shapefile.proj.valid &lt;- st_make_valid(fs.shapefile.proj)\nall(st_is_valid(fs.shapefile.proj.valid))\n\n3. Smooth the wildfire hazard and land use datasets using a 5s5 moving window; use the mean for the continuous dataset and the mode for the categorical dataset.\n\nSmoothing the two rasters is relatively straightforward using the focal function. We set the w argument to be 5 so that we get the 5x5 moving window and then specify the function for each.\n\n\nhazard.smooth &lt;- focal(fire.haz.proj, w=5, fun=\"mean\")\nland.use.smooth &lt;- focal(land.use, w=5, fun=\"modal\")\nlevels(land.use.smooth) &lt;- levels(land.use)\n\n4. Estimate the total cost of the incidents within each forest (PROJECTED_FINAL_IM_COST contains this value for each incident).\n\nThis one is a little trickier. First, we need to restrict the incidents and forests to the area for which we have data. Then, we’ve got to join the incident to the appropriate forest. Finally, we’ve got to summarize the incident cost by each forest. We use st_crop so that we can cut off the datasets at the actual bounding boxes (rather than looking for the observations that intersect). Once we do that, we need to join the incidents to the forests (using st_join). Once we’ve got the incidents linked to forests, we can use group_by, summarise, and sum to create a new, summary level variable that is the total cost. The last step is to join that value back to the original forest geometries.\n\n\nincidents.pnw &lt;- st_crop(incidents.proj, st_bbox(cejst.proj))\nforest.pnw &lt;- st_crop(fs.shapefile.proj.valid, st_bbox(cejst.proj))\n\nincidents.forest &lt;- incidents.pnw %&gt;% \n  st_join(x=., forest.pnw, join=st_within, left=FALSE)\n\nincidents.summary &lt;- incidents.forest %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(FORESTNAME) %&gt;% \n  summarise(., totcost = sum(PROJECTED_FINAL_IM_COST, na.rm=TRUE))\n\nforest.join &lt;- forest.pnw %&gt;% \n  left_join(., incidents.summary)\n\n5. Next join 3 attributes of your choosing from the CEJST and the extracted fire and land cover values to your dataframe.\n\nThis step asks you to do 2 different things: join some tabular elements from the CEJST dataset to your forest datasets and extract values from the raster datasets. We first use select to keep only the total population (TPF), housing burden (HBF_PFS), and percent of population &gt;200% below the poverty level (P200_I_PFS). Then, we can use the st_join function again to attribute our forest data with the appropriate CEJST data and summarise it to the forest level. Once we’ve got our vector data attributed, we can use it to extract the appropriate values from the rasters. Because terra::extract returns the values in the order they are listed in the data, we just use cbind to join the columns to our forest data and then use rename to make the columns are little more sensible.\n\n\ncejst.select &lt;- cejst.proj %&gt;% \n  select(., c(TPF, HBF_PFS, P200_I_PFS))\n\nforest.cejst &lt;- forest.join %&gt;% \n  st_join(., y=cejst.select, join=st_intersects) %&gt;% \n  group_by(FORESTNAME) %&gt;% \n  summarise_at(vars(totcost:P200_I_PFS), mean, na.rm=TRUE)\n\n\nforest.landuse.ext &lt;- terra::extract(x=land.use.smooth, y = vect(forest.cejst), fun=\"modal\", na.rm=TRUE)\n\nforest.firehaz.ext &lt;- terra::extract(x= hazard.smooth, y = vect(forest.cejst), fun=\"mean\", na.rm=TRUE)\n\nforest.cejst.join &lt;- cbind(forest.cejst,forest.landuse.ext$category, forest.firehaz.ext$focal_mean) %&gt;% \n  rename(category = \"forest.landuse.ext.category\", hazard = \"forest.firehaz.ext.focal_mean\")\n\n6. Make a set of maps that shows the Forest-level values for all of your selected variables.\n\nYou could certainly make a map for each variable individually, but that is long and tedious. Here, we’ll use tm_facets again to create maps of all of our variables. To do that, we need the data in long format so we use pivot_longer to create a column with the variable name (taken from the column) and a column with the numeric value (take from the actual observation). Note that because a column cannot have mixed datatypes, the landcover data has to remain as the numeric code. We could fuss with that more, but for now all of the focal landcover values are forest so there’s not a ton of reason to do this. Once we’ve got the data in long format, it’s a simple call to tmap to make make all of our maps.\n\n\nforest.cejst.long &lt;- forest.cejst.join %&gt;% \n  pivot_longer(., cols =totcost:hazard, names_to=\"variable\", values_to = \"value\")\n\ntm_shape(cejst.proj) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(forest.cejst.long) +\n  tm_fill(col=\"value\") +\n  tm_facets(by = c(\"variable\"), free.scales.fill = TRUE)"
  },
  {
    "objectID": "assignment/08-combinationssolutions.html",
    "href": "assignment/08-combinationssolutions.html",
    "title": "Assignment 8 Solutions: Autocorrelation and Interpolation",
    "section": "",
    "text": "Read in the disasters dataset, convert it to points, filter it to those disasters in Idaho, and select any relevant columns. You will also need to use tigris::county() to download a county shapefile for the region. Make sure your data are projected correctly\n\nI start by loading the packages necessary for the entire analysis. Then, I use the tigris package to get my county files. We need this for two reasons: to subset the disaster data into our region of interest. Note that I used the entire region because it’s possible that there is information to be learned from the data on the borders of Idaho that don’t conform to the state boundaries. I then load the disaster dataset, select a handful of columns that I’m interest in, drop any records that are missing their coordinates, convert the csv to a sf object, project it to the same CRS as the county dataset, and then keep only the distinct point locations. This last step is important because having multiple events in the exact same location creates issues for calculating our spatial autocorrelation estimates (because the distance is exactly zero making it difficult to determine which event is the “parent”).\n\n\nlibrary(sf)\nlibrary(tidyverse, quietly = TRUE)\nlibrary(spdep)\nlibrary(spatstat)\nlibrary(sp)\nlibrary(terra)\nlibrary(tmap)\n\ncty &lt;- tigris::counties(state = c(\"ID\", \"WA\", \"OR\"), progress_bar=FALSE)\n\n\n\ndisast.sf &lt;- read_csv(\"data/opt/data/2023/assignment07/ics209-plus-wf_incidents_1999to2020.csv\") %&gt;% \n  filter(., START_YEAR &gt;= 2000 & START_YEAR &lt;= 2017) %&gt;% \n  select(INCIDENT_ID, , POO_STATE, POO_LATITUDE, POO_LONGITUDE, FATALITIES, PROJECTED_FINAL_IM_COST, STR_DESTROYED_TOTAL, PEAK_EVACUATIONS) %&gt;% \n  drop_na(c(POO_LATITUDE, POO_LONGITUDE)) %&gt;% \n  st_as_sf(coords = c(\"POO_LONGITUDE\", \"POO_LATITUDE\"), crs= 4326) %&gt;% \n  st_transform(., st_crs(cty)) %&gt;% \n  distinct(., geometry, .keep_all=TRUE)\n  \ndisast.sf &lt;- disast.sf[cty,]\n\nGenerate the Ripley’s K curves for the disaster dataset. What do you think? Is there evidence that the data is spatially autocorrelated? &gt;We use the same code from class here to estimate the Ripley’s K function. We first select the variable we’re interested in (STR_DESTROYED_TOTAL in my case), transform the CRS to a planar coordinate system, and convert it to a ppp object for spdep. We use the envelope function with Kest to calculate several theoretical values for Ripley’s K under complete spatial randomness. Comparing the K_{obs} to the envelope of theoretical values suggests that there is more aggregation in the data than would be predicted under CSR.\n\nkf.env &lt;- envelope(as.ppp(st_transform(select(disast.sf, STR_DESTROYED_TOTAL), crs=8826)), Kest, correction = \"translation\",  nsim= 1000, envelope = TRUE, verbose = FALSE)\n\nplot(kf.env)\n\nUse the nearest-neighbor approach that we used in class to estimate the lagged values for the disaster dataset and estimate the slope of the line describing Moran’s I statistic.\n\nWe begin by finding the nearest neighbor for each observation using the knearneigh function which finds the k closest neighbors for each point. Because we only want the nearest neighbor, we set k=1. We need to convert this to a neighbor list (class(geog.nearnb) = nb) and do this by wrapping the output of knearneigh inside of knn2nb which converts knn objects to nb objects. We then need to estimate the distance to each neighbor (using dnearneigh) and convert it to a spatial weights matrix (using nb2listw). Finally, we convert this weight’s matrix into a vector of the same number of rows as our disaster dataset using lag.listw. This function creates a new estimate of STR_DESTROYED_TOTAL for each row based on the spatially weighted value of the nearest neighbor. Finally, we fit a simple linear regression to the data and see that there is a slight positive slope to the line suggesting that there is some autocorrelation (remember, the slope of this simple linear model is the Moran’s I coefficient).\n\n\ngeog.nearnb &lt;- knn2nb(knearneigh(disast.sf, k = 1), row.names = disast.sf$INCIDENT_ID, sym=TRUE); #estimate distance to first neareset neighbor\nnb.nearest &lt;- dnearneigh(disast.sf, 0,  max( unlist(nbdists(geog.nearnb, disast.sf))));\nlw.nearest &lt;- nb2listw(nb.nearest, style=\"W\")\nbldg.lag &lt;- lag.listw(lw.nearest, disast.sf$STR_DESTROYED_TOTAL)\nM &lt;- lm(bldg.lag ~ disast.sf$STR_DESTROYED_TOTAL)\nsummary(M)\nplot(bldg.lag ~ disast.sf$STR_DESTROYED_TOTAL, xlim=c(0,20))\nabline(M, col=\"red\")\n\nNow use the permutation approach to compare your measured value to one generated from multiple simulations. Generate the plot of the data. Do you see more evidence of spatial autocorrelation?\n\nWe can verify this by using a Monte Carlo permutation approach. We use a for loop to “shuffle” the data (using sample), but keep the same neighbor structure (using our same lw.nearest spatial weights matrix). We then fit a linear model to the reshuffled data and estimate the slope to see what values are plausible under complete spatial randomness (which we achieve by shuffling the data independent of their location). Run this loop 1000 times (set by n &lt;- 1000L) and you’ll generate a distribution of plausible values. Based on the distribution and our actual value (in red), we can see that this value for Moran’s I is generally larger than we’d expect under CSR, but not terribly so.\n\n\nn &lt;- 1000L   # Define the number of simulations\nI.r &lt;- vector(length=n)  # Create an empty vector\n\nfor (i in 1:n){\n  # Randomly shuffle income values\n  x &lt;- sample(disast.sf$STR_DESTROYED_TOTAL, replace=FALSE)\n  # Compute new set of lagged values\n  x.lag &lt;- lag.listw(lw.nearest, x)\n  # Compute the regression slope and store its value\n  M.r    &lt;- lm(x.lag ~ x)\n  I.r[i] &lt;- coef(M.r)[2]\n}\n\nhist(I.r, main=NULL, xlab=\"Moran's I\", las=1)\nabline(v=coef(M)[2], col=\"red\")\n\nGenerate the 0th, 1st, and 2nd order spatial trend surfaces for the data. Is there evidence for a second order trend? How can you tell?\n\nIn order to generate a spatial trend surface, we need to predict values across a uniform grid that covers the study region. We initialize that grid by using our county dataset and drawing 15000 random sample points across the region. We then create a series of formula object depicting the 0th, 1st (linear), and 2nd (quadratic) models to the data where the predictors are just the X and Y coordinates. We fit each of the models using lm, convert them into a SpatialGridDataFrame from the sp package, and then convert them to a raster to make plotting easier. Based on the curvature we see in the 2nd order trend surface, there is an indication of a 2nd order trend, though it is not super strong. We’ll use that model for kriging in the subsequent steps.\n\n\ngrd &lt;- as.data.frame(spsample(as(cty, \"Spatial\"), \"regular\", n=15000))\nnames(grd)       &lt;- c(\"X\", \"Y\")\ncoordinates(grd) &lt;- c(\"X\", \"Y\")\ngridded(grd)     &lt;- TRUE  # Create SpatialPixel object\nfullgrid(grd)    &lt;- TRUE  # Create SpatialGrid object\nproj4string(grd) &lt;- proj4string(as(disast.sf, \"Spatial\"))\n\nf.0  &lt;- as.formula(PROJECTED_FINAL_IM_COST ~ 1)\n\n# Run the regression model\nlm.0 &lt;- lm( f.0 , data=disast.sf)\n\n# Use the regression model output to interpolate the surface\ndat.0th &lt;- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.0, newdata=grd)))\nr   &lt;- rast(dat.0th)\nr.m0 &lt;- mask(r, st_as_sf(cty))\n\nf.1  &lt;- as.formula(STR_DESTROYED_TOTAL ~ X + Y)\n\ndisast.sf$X &lt;- st_coordinates(disast.sf)[,1]\ndisast.sf$Y &lt;- st_coordinates(disast.sf)[,2]\n\n# Run the regression model\nlm.1 &lt;- lm( f.1 , data=disast.sf)\n\n# Use the regression model output to interpolate the surface\ndat.1st &lt;- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.1, newdata=grd)))\n\n# Convert to raster object to take advantage of rasterVis' imaging\n# environment\nr   &lt;- rast(dat.1st)\nr.m1 &lt;- mask(r, st_as_sf(cty))\n\nf.2 &lt;- as.formula(STR_DESTROYED_TOTAL ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))\n\n# Run the regression model\nlm.2 &lt;- lm( f.2, data=disast.sf)\n\n# Use the regression model output to interpolate the surface\ndat.2nd &lt;- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.2, newdata=grd))) \n\nr   &lt;- rast(dat.2nd)\nr.m2 &lt;- mask(r, st_as_sf(cty))\nrst.stk &lt;- c(r.m0, r.m1, r.m2)\nnames(rst.stk) &lt;- c(\"zeroOrder\", \"firstOrder\", \"secondOrder\")\nplot(rst.stk)\n\nNow use the spatial trend surface to perform some ordinary krigging. You’ll want to have a grid of 15,000 points, fit 3 different experimental variogram functions (see the vgm function helpfile to learn more about the shapes available to you). Plot your variogram fits. Which one would you choose? Why?\n\nA variogram simply plots the relationship between distance and the residuals of a model. We first assign those residuals to our disaster dataset. We then generate a cloud-style variogram for data without eliminating the spatial trend. As you can see, there are some strange bands that show up in the data likely due to the second order effects we saw in the model previously.\n\n\ndisast.sf$res &lt;- lm.2$residuals\n\nvar.cld  &lt;- gstat::variogram(res ~ 1, disast.sf, cloud = TRUE)\nvar.df  &lt;- as.data.frame(var.cld)\n\n\nOP &lt;- par( mar=c(4,6,1,1))\nplot(var.cld$dist/1000 , var.cld$gamma, col=\"grey\", \n     xlab = \"Distance between point pairs (km)\",\n     ylab = expression( frac((res[2] - res[1])^2 , 2)) )\npar(OP)\n\n\nWe then fit a variogram to the detrended data (this is the sample variogram) by passing our f.2 formula to the variogram function. We take the mean values of the pairwise differences and plot them in bins on top of the original data. As you can see, this reduces a considerable amount of noise and the shape of the variogram begins to materialize.\n\n\nvar.smpl &lt;- gstat::variogram(f.2, disast.sf, cloud = FALSE)\n\nbins.ct &lt;- c(0, var.smpl$dist , max(var.cld$dist) )\nbins &lt;- vector()\nfor (i in 1: (length(bins.ct) - 1) ){\n  bins[i] &lt;- mean(bins.ct[ seq(i,i+1, length.out=2)] ) \n}\nbins[length(bins)] &lt;- max(var.cld$dist)\nvar.bins &lt;- findInterval(var.cld$dist, bins)\nvar.cld2 &lt;- var.cld[var.cld$gamma &lt; 500,]\nOP &lt;- par( mar = c(5,6,1,1))\nplot(var.cld2$gamma ~ eval(var.cld2$dist/1000), col=rgb(0,0,0,0.2), pch=16, cex=0.7,\n     xlab = \"Distance between point pairs (km)\",\n     ylab = expression( gamma ) )\npoints( var.smpl$dist/1000, var.smpl$gamma, pch=21, col=\"black\", bg=\"red\", cex=1.3)\nabline(v=bins/1000, col=\"red\", lty=2)\npar(OP)\n\n\nYou can use vgm() to see potential shapes of the different variograms that we can fit using gstat. I chose linear, Gaussian, and spherical as they are common choices and because the initial rise to the sill seemed consistent with those shapes. Note that the semivariance begins to increase again you get further out. This might suggest that we need a more complicated de-trending model, but we won’t worry about that now. The 3 forms seem to fit the data in similar ways, but the spherical form has a slightly more gradual rise to the sill. I choose that one as that may help smooth a bit more than the other 2.\n\n\n# Compute the variogram model by passing the nugget, sill and range values\n# to fit.variogram() via the vgm() function.\ndat.fit.lin  &lt;- gstat::fit.variogram(var.smpl, gstat::vgm(psill= 50, model=\"Lin\"))\ndat.fit.gau  &lt;- gstat::fit.variogram(var.smpl, gstat::vgm(psill=50, model=\"Gau\"))\ndat.fit.sph  &lt;- gstat::fit.variogram(var.smpl, gstat::vgm(psill=50, model=\"Sph\"))\n\n# The following plot allows us to gauge the fit\nplot(var.smpl, dat.fit.lin, main = \"Linear variogram\")\nplot(var.smpl, dat.fit.gau, main = \"Gaussian variogram\")\nplot(var.smpl, dat.fit.sph, main = \"Spherical variogram\")\n\nUsing your spatial trend model and your fitted variogram, krige the data and generate a map of the interpolated value and a map of the error.\n\nNow that we’ve got our Spherical variogram estimated on the detrended data, we can use the krige function to generate spatial predictions across the grid. We can also access the variance resulting from that model. We do that, convert them to rasters and plot the two outcomes. We can see that we’ve eliminated the bulk of spatial patterns in the residuals (as evidenced by light colors on the predicted residual map); however, the predictions for the western cost are much less stable (as evidenced by the variance map).\n\n\ndat.krg &lt;- gstat::krige( res~1, as(disast.sf, \"Spatial\"), grd, dat.fit.sph)\n\nr &lt;- rast(dat.krg)$var1.pred\nr.m.pred &lt;- mask(r, st_as_sf(cty))\ntm_shape(r.m.pred) + tm_raster(n=10, palette=\"RdBu\", title=\"Predicted residual \\nstructures destroyed\")  +\n  tm_legend(legend.outside=TRUE)\n\nr &lt;- rast(dat.krg)$var1.var\nr.m.var &lt;- mask(r, st_as_sf(cty))\ntm_shape(r.m.var) + tm_raster(n=7, palette =\"Reds\", ,title=\"Variance map \") +\n  tm_legend(legend.outside=TRUE)"
  },
  {
    "objectID": "assignment/09-pointpatternssolutions.html",
    "href": "assignment/09-pointpatternssolutions.html",
    "title": "Assignment 9 Solutions: Fitting models to your dataframe",
    "section": "",
    "text": "1. Use the variables that you chose from assignment 6 along with the wildfire hazard and land use dataset to attribute each disaster in the disaster dataset.\n\nHere I am following the same procedure from assignment 7 for creating the spatial database. The only real change is that we are attributing point data (in the incidents dataset) instead of summarizing to polygons (like we did with the Forest Service data). We drop any incomplete cases to avoid problems with NAs (though this may not be the best thing to do in practice) and store that data for later. We then set up our model dataframe by making sure that the cost variable is an integer (for Poisson modeling) and that we drop levels from the land use dataset that don’t appear in our incident locations.\n\n\nlibrary(sf)\nlibrary(tidyverse, quietly = TRUE)\nlibrary(terra)\nlibrary(tmap, quietly = TRUE)\nlibrary(caret)\n\n \ncejst.pnw &lt;- read_sf(\"data/opt/data/2023/assignment07/cejst_pnw.shp\")%&gt;% \n  filter(., !st_is_empty(.))\n\nincidents.csv &lt;- read_csv(\"data/opt/data/2023/assignment07/ics209-plus-wf_incidents_1999to2020.csv\")\n\nland.use &lt;- rast(\"data/opt/data/2023/assignment07/land_use_pnw.tif\")\nfire.haz &lt;- rast(\"data/opt/data/2023/assignment07/wildfire_hazard_agg.tif\")\n\n\nfire.haz.proj &lt;- project(fire.haz, land.use)\n\n\ncejst.proj &lt;- cejst.pnw %&gt;% \n  st_transform(., crs=crs(land.use))\n\nincidents.proj &lt;- incidents.csv %&gt;% \n  filter(., !is.na(POO_LONGITUDE) | !is.na(POO_LATITUDE) ) %&gt;% \n  st_as_sf(., coords = c(\"POO_LONGITUDE\", \"POO_LATITUDE\"), crs= 4269) %&gt;% \n  st_transform(., crs=crs(land.use))\nincidents.pnw &lt;- st_crop(incidents.proj, st_bbox(cejst.proj))\n\nhazard.smooth &lt;- focal(fire.haz.proj, w=5, fun=\"mean\")\nland.use.smooth &lt;- focal(land.use, w=5, fun=\"modal\")\nlevels(land.use.smooth) &lt;- levels(land.use)\n\ncejst.select &lt;- cejst.proj %&gt;% \n  select(., c(TPF, HBF_PFS, P200_I_PFS))\n\nincident.cejst &lt;- incidents.pnw %&gt;% \n  st_join(., y=cejst.select, join=st_within) \n\nincident.landuse.ext &lt;- terra::extract(x=land.use.smooth, y = vect(incident.cejst), fun=\"modal\", na.rm=TRUE)\n\nincident.firehaz.ext &lt;- terra::extract(x= hazard.smooth, y = vect(incident.cejst), fun=\"mean\", na.rm=TRUE)\n\nincident.cejst.join &lt;- cbind(incident.cejst,incident.landuse.ext$category, incident.firehaz.ext$focal_mean) %&gt;% \n  rename(category = \"incident.landuse.ext.category\", hazard = \"incident.firehaz.ext.focal_mean\")\n\nincident.cejst.prep &lt;- incident.cejst.join %&gt;% \n  select(., PROJECTED_FINAL_IM_COST, TPF, HBF_PFS, P200_I_PFS, hazard, category,) %&gt;% \n  st_drop_geometry(.) %&gt;% \n  filter(., complete.cases(.))\n\nincident.cejst.model &lt;- incident.cejst.prep  %&gt;% \n  mutate(across(TPF:hazard, ~ as.numeric(scale(.x))),\n         category=droplevels(category),\n         cost = as.integer(floor(PROJECTED_FINAL_IM_COST))) %&gt;% \n  select(-PROJECTED_FINAL_IM_COST)\n\n2. Fit a Poisson regression using your covariates and the cost of the incident data (using glm with family=poisson())\n\nNow that we have our data, it’s time to set up some models. We take advantage of the caret package to split the data into a training and testing set using the category variable to make sure we have representation of all the cateogries. We then set up our trainControl options to use cross validation as a means of adjusting tuning parameters and tell R to only save the best model once the tuning is complete. Finally, we use the train function from caret to fit our first model. For a simple Poisson regression, we can rely on the glm method with the family set to poisson. Note that because this is not binary data, our ROC metric doesn’t work as a means of evaluating the performance of the different tuning parameters. Instead, we use something called the Root-Mean Squared Error (RMSE).The RMSE is a measure of the difference between the fitted value and the observed value (in this case, for the cross-validation data within the model training). Larger values indicate poorer fits.\n\n\nset.seed(998)\ninTraining &lt;- createDataPartition(incident.cejst.model$category, p = .8, list = FALSE)\ntraining &lt;- incident.cejst.model[ inTraining,]\ntesting  &lt;- incident.cejst.model[-inTraining,]\n\nfitControl &lt;- trainControl(\n   method = \"cv\",  # k-fold cross validation\n   number = 10,  # 10 folds\n   savePredictions = \"final\"       # save predictions for the optimal tuning parameter\n)\n\nPoisFit &lt;- train( cost ~ ., data = training, \n                 method = \"glm\", \n                 family = poisson,\n                 trControl = fitControl,\n                 metric=\"RMSE\"\n                 )\n\n3. Fit a regression tree using your covariates and the cost of the incident data (using caret package method=rpart`)\n\nWe use similar syntax to fit the regression tree to the data, but make a few changes. First, we set cost as.numeric() to ensure that this is a regression tree (because our data do not reflect categories). We then set the method to rpart. Because rpart has a complexity parameter, there is a bit of tuning to be done. We tell R that we’re willing to look at 20 different values of this complexity parameter. We can use plot and rpart.plot to inspect the results.\n\n\nRtFit &lt;- train(as.numeric(cost) ~ ., data = training, \n                 method = \"rpart\",\n                 trControl = fitControl,\n                 metric = \"RMSE\",\n                tuneLength = 20,  \n                 )\nplot(RtFit)\nrpart.plot::rpart.plot(RtFit$finalModel, type=4)\n\n4. Fit a random forest model using your covariates and the cost of the incident data (using caret package method= 'rf')\n\nThe syntax is similar to the previous models, but with method=rf to signal that we want to use the rf package to fit the Random Forest. Here, the tuning parameter is the number of variables to include in the tree. We’ve only got 7 variables so we’ll set the tuneLength to the maximum number of variables.\n\n\nRFFit &lt;- train(as.numeric(cost) ~ ., data = training, \n                 method = \"rf\",\n                 trControl = fitControl, \n               tuneLength=7\n                 )\nplot(RFFit)\nplot(RFFit$finalModel)\n\n5. Use cross-validation to identify the best performing model of the 3 that you fit\n\nNow that we have three different models, let’s see how well they do predicting the testing dataset. We first generate predictions using the predict function and supplying the model object and the newdata. In this case, our new data is the five covariate columns from the testing partition. Once we have the predictions, we can calculate the RMSE. Based on RMSE values the regression tree and Random Forest model seem to be the better performers.\n\n\npois.pred &lt;- predict(PoisFit, newdata = testing[,1:5])\nrmse.pois &lt;- sqrt(sum(pois.pred - testing$cost)^2/length(pois.pred))\nrt.pred &lt;- predict(RtFit, newdata = testing[,1:5])\nrmse.rt &lt;- sqrt(sum(rt.pred - testing$cost)^2/length(rt.pred))\nrf.pred &lt;- predict(RFFit, newdata = testing[,1:5])\nrmse.rf &lt;- sqrt(sum(rf.pred - testing$cost)^2/length(rf.pred))\n\n6. Convert all of your predictors into rasters of the same resolution and generate a spatial prediction based on your model\n\nNow that we’ve identified the models we want to use to generate our spatial surface, we need to prepare all of the input rasters. We use rasterize to create the cejst variables. These are on original scale of the data and so we need to rescale them to the same range of our modeled datasets. Here, we can’t use scale because the mean of the total dataset would differ from the mean that we used for the incidents-only data so we have to manually set up the scale. Lastly, we have to drop the levels from the land use raster that weren’t present in the incident dataset. We do that with the subst call from terra. Once we’ve got our rasters set up, we can just use predict.\n\n\nTPF.rast &lt;- (rasterize(cejst.select, hazard.smooth, field=\"TPF\") - mean(incident.cejst.prep$TPF,na.rm=TRUE))/sd(incident.cejst.prep$TPF)\nHBF_PFS.rast &lt;- (rasterize(cejst.select, hazard.smooth, field=\"HBF_PFS\")- mean(incident.cejst.prep$HBF_PFS,na.rm=TRUE))/sd(incident.cejst.prep$HBF_PFS)\nP200_I_PFS.rast &lt;- (rasterize(cejst.select, hazard.smooth, field=\"P200_I_PFS\")- mean(incident.cejst.prep$P200_I_PFS,na.rm=TRUE))/sd(incident.cejst.prep$P200_I_PFS)\nland.use.smooth &lt;- subst(land.use.smooth, from=c(\"Non-Forest Wetland\",\"Non-Processing Area Mask\"), to=c(NA, NA))\nhazard.smooth.scl &lt;- (hazard.smooth - mean(incident.cejst.prep$hazard))/sd(incident.cejst.prep$hazard)\npred.rast &lt;- c(TPF.rast, HBF_PFS.rast, P200_I_PFS.rast, land.use.smooth, hazard.smooth.scl)\nnames(pred.rast)[5] &lt;- \"hazard\"\n\n\nrt.spatial &lt;- terra :: predict(pred.rast, RtFit, na.rm=TRUE) \nrf.spatial &lt;- terra :: predict(pred.rast, RFFit, na.rm=TRUE) \n\n\nIf you looked at the RMSE values for our initial models, you’ll notice that they were quite high and the models weren’t particularly interesting. Because the cost data ranges over several orders of magnitude, we might try log-transforming them and fitting a linear model (because the data are no longer integers) along with the other two models. We do that here following the syntax above. When calculating the RMSE, we have to remember to log-transform the cost variable in the testing dataset to make sure that the predictions are comperable. Again, the regression tree and Random Forest are the better performers, but the RMSE suggests that we are doing considerably better (\\(10^{2}=100\\) as opposed to the 100,000s we were getting before).\n\n\ntraining.log &lt;- training %&gt;% \n  mutate(cost = log(cost, 10))\n\nLinFit &lt;- train( cost ~ ., data = training.log, \n                 method = \"lm\", \n                 trControl = fitControl,\n                 metric=\"RMSE\"\n                 )\n\nRtFit.log &lt;- train(cost ~ ., data = training.log, \n                 method = \"rpart\",\n                 trControl = fitControl,\n                 metric = \"RMSE\",\n                tuneLength = 20,  \n                 )\n\nRFFit.log &lt;- train(cost ~ ., data = training.log, \n                 method = \"rf\",\n                 trControl = fitControl\n                 )\n\nlin.pred &lt;- predict(LinFit, newdata = testing[,1:5])\nrmse.lin &lt;- sqrt(sum(lin.pred - log(testing$cost,10))^2/length(pois.pred))\nrt.pred &lt;- predict(RtFit.log, newdata = testing[,1:5])\nrmse.rt &lt;- sqrt(sum(rt.pred - log(testing$cost,10))^2/length(rt.pred))\nrf.pred &lt;- predict(RFFit.log, newdata = testing[,1:5])\nrmse.rf &lt;- sqrt(sum(rf.pred - log(testing$cost,10))^2/length(rf.pred))\n\n7. Plot your result &gt;We use the par argument to set up a 2x2 layout and print all 4 plots.\n\nrt.spatial.log &lt;- terra :: predict(pred.rast, RtFit.log, na.rm=TRUE) \nrf.spatial.log &lt;- terra :: predict(pred.rast, RFFit.log, na.rm=TRUE) \n\npar(mfrow=c(2,2))\nplot(rt.spatial, main=\"Regression Tree Classifier\")\nplot(rf.spatial, main=\"Random Forest Classifier\")\nplot(rt.spatial.log, main=\"Regression Tree Classifier (log)\")\nplot(rf.spatial.log, main=\"Random Forest Classifier(log)\")\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "assignment/11-statmod.html",
    "href": "assignment/11-statmod.html",
    "title": "Assignment 9: Statistical Analyses",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of working in R, developing programming workflows, and getting help. These topics are important for building robust spatial workflows, but their utility extends beyond that to most things you’ll do in R during the course of your graduate research. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/11-statmod.html#instructions",
    "href": "assignment/11-statmod.html#instructions",
    "title": "Assignment 9: Statistical Analyses",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/13-thirdrevision.html",
    "href": "assignment/13-thirdrevision.html",
    "title": "Assignment 13: Revisiting your code (part 3)",
    "section": "",
    "text": "This is your final opportunity to reconsider your answers to the last few assignments and evaluate what you might have done differently now that you’ve had a little more practice. I’ve also asked some specific questions based on common mistakes across the assignments. You’ll still be using Quarto to complete This homework."
  },
  {
    "objectID": "assignment/final-proj.html",
    "href": "assignment/final-proj.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project is an opportunity to bring all of the things you’ve learned in the course into a single reproducible workflow to answer a question of your choosing. Because each of you are in different stages of collecting your own data and because confronting datasets that aren’t yours can help clarify important concepts and design elements, I’m asking you to develop an analysis of data that isn’t yours. This final project should help you demonstrate:\n\nProper data management, cleaning, and manipulation technques\nThe ability to summarize spatial data and apply different statistical analyses to it\nThe ability to evaluate the performance of statistical models\nThe ability to generate visualizations that support your analyses\nThe ability to integrate code, analysis, and visualization with text descrbing your approach and discussing your results"
  },
  {
    "objectID": "assignment/final-proj.html#overview",
    "href": "assignment/final-proj.html#overview",
    "title": "Final Project",
    "section": "",
    "text": "The final project is an opportunity to bring all of the things you’ve learned in the course into a single reproducible workflow to answer a question of your choosing. Because each of you are in different stages of collecting your own data and because confronting datasets that aren’t yours can help clarify important concepts and design elements, I’m asking you to develop an analysis of data that isn’t yours. This final project should help you demonstrate:\n\nProper data management, cleaning, and manipulation technques\nThe ability to summarize spatial data and apply different statistical analyses to it\nThe ability to evaluate the performance of statistical models\nThe ability to generate visualizations that support your analyses\nThe ability to integrate code, analysis, and visualization with text descrbing your approach and discussing your results"
  },
  {
    "objectID": "assignment/final-proj.html#requirements",
    "href": "assignment/final-proj.html#requirements",
    "title": "Final Project",
    "section": "Requirements",
    "text": "Requirements\nDatasets. The ability to manipulate and integrate a variety of data types, resolutions, and formats is a key component of this course. Your analysis should incorporate at least 5 datasets. The ultimate compostion of your database is up to you, but I’d like you to include 1 tabular dataset, 1 vector dataset, and 1 raster dataset. You should choose the other 2 (or more) to give you practice with the data types that are most relevant to your objectives and/or research.\nAnalyses. You’ve learned several classes of analyses (e.g., overlays, point-pattern, multivariate regression, and statistical learning). Apply at least one (preferably the one most tied to your own objectives and research) of these analyses to address your question. In the course of doing so, you’ll need to justify your choice, assess whether your data meets appropriate assumptions, and evaluate the implications of key assumptions you make. For example, if you’re conducting an overlay analysis, how does your choice of threshold affect the ultimate result? If you’ve fit a statistical model based on summary statistics (e.g, mean, median), how well does the model fit? How does the model change if you use different slices of the data?\nVisualizations. You should produce a minimum of 3 visualizations to accompany your analysis. One of these should be a publication quality location map. The others are up to you, but should a) help you tell the story of your analysis and b) help you meet your objectives for the course and your own research. These can be additional maps of results, figures that summarize your data or results in non-spatial ways, or interactive graphics that allow you to explore parts of your analysis.\nReporting You can generate a ‘manuscript’ style document (using Quarto) or a flexdashboard (using Quarto and shiny) as the final product. Your report should include:\n\nA brief (1-2 paragraphs) description of your question and why you’re interested in it.\nA Methods section with subsections describing the data sources, any processing steps you took and why, and your process for the analysis. Show your code and provide annotation to describe what you are attempting do with the various steps.\nA Results section that includes tabular results as well as any relevant visualizations that describe your data and analysis.\nA Discussion of your results that puts your results in the context of your question, considers alternative analysis strategies and why they may or may not be better than the approach you chose, describes additional data that might be important for your question, and considers the role of extent and resolution in your analysis."
  },
  {
    "objectID": "assignment/final-proj.html#assessment",
    "href": "assignment/final-proj.html#assessment",
    "title": "Final Project",
    "section": "Assessment",
    "text": "Assessment\nThis is the final project, but there are several steps along the way! First, you’ll need to submit objectives (docs/grading_contract.qmd) which form the basis for assessment of the project. This is due October 1. Then, you’ll need to submit a brief description of your project (docs/project_description.qmd) on November 1.\n\n\n\n\n\n\nImportant\n\n\n\nYou’ll submit each of these documents by making commits prior to the deadline. After the deadline passes I’ll add my comments to the documents.\n\n\nYou’ll submit a draft of the final report (docs/main_project_doc.qmd) on December 9. I’ll give you feedback based on your project and on your objectives for the course. You’ll then have a chance to address my feedback before turning in your final draft on December 18. Your final self-assessment will ask you to reflect on your objectives for the course and evaluate the degree to which your final project demonstrates that you achieved your objectives. Thus, when you are designing your project, make sure that you have your initial objectives in mind.\nWe’ll use a form of contract grading to determine your grades on the final. Contract grading allows us to have a conversation about what you want out of the course, what you expect to put into it, and what I think you need to be successful in deploying the skills we learn here. Based on your goals for course, we’ll sign a contract that instantiates your objectives into the grade you’ll receive for the final. Complete the final and meet your objectives and you’ll get the grade you chose.\nScoring for the final project will be based on the percentage of the objectives you achieve (i.e., meet 9 out of 10 of your objectives means you get 90% of the points)\n\n\n\n\n\n\nNote\n\n\n\nA note on grades: You will be responsible for assessing how well your assignment demonstrates that you achieved your objectives. I reserve the right to change the grade you’ve given yourself, but will provide clear justification for why I’m doing that. Without a completed self-assessment there is no grade for your final project, so please make sure you complete it."
  },
  {
    "objectID": "assignment/self-eval1.html",
    "href": "assignment/self-eval1.html",
    "title": "Self-reflection 1",
    "section": "",
    "text": "This is the first of three self-reflections that you’ll complete during the course. These provide an opportunity for me to learn more about you, check-in on how the course is going, and make sure that getting what you need from the course materials. This first reflection also helps us set the standards for your assessment in the course. As such, I’m asking that you complete this in a timely fashion and turn it in by Aug. 30. You’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/self-eval1.html#instructions",
    "href": "assignment/self-eval1.html#instructions",
    "title": "Self-reflection 1",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-1-xxx.qmd and give it a title (like M Williamson Self-Reflection 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 2 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-1-xx.qmd, and self-reflection-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/self-eval3.html",
    "href": "assignment/self-eval3.html",
    "title": "Self-reflection 3",
    "section": "",
    "text": "This is the final self-reflection for the course and provides a way of evaluating your final project relative to your course learning objectives. This final self-reflection is critical for assigning your grades on the final project and the course, in general. As such, I’m asking that you complete this in a timely fashion and turn it in by Dec. 16. You’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/self-eval3.html#instructions",
    "href": "assignment/self-eval3.html#instructions",
    "title": "Self-reflection 3",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-1-xxx.qmd and give it a title (like M Williamson Self-Reflection 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-1-xx.qmd, and self-reflection-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Tools of the Trade",
    "section": "",
    "text": "Today we’ll be talking about reproducible workflows, why they’re important (generally) and some considerations for working with geographic data. We’ll also get introduced to version control and it’s the practice of open, reproducible research. Finally, you’ll meet Quarto our go-to tool to try and keep all of the pieces of the data analysis pipeline together."
  },
  {
    "objectID": "content/02-content.html#readings",
    "href": "content/02-content.html#readings",
    "title": "Tools of the Trade",
    "section": "Readings",
    "text": "Readings\nThe following readings are intended to give you some sense of the discussion surrounding the role of spatial data in understanding the world. They are a mix of old favorites and relatively recent reviews. You don’t need to read all of them or memorize them, but they are worth a skim. I bet you’ll find something interesting.\n\nSetting the Stage\n\n Open science, reproducibility, and transparency in ecology by Powers and Hampton - discusses the importance of open science for ecologists.\n Practical Reproducibility in Geography and Geosciences by Nilst and Pebesma - describes the importance of reproducibility for geospatial analysis.\n The Whole Game - from Wickham et al., R for Data Science (Wickham and Grolemund 2016). Focus on the sections that begin with “Workflow” to get a sense for how we’ll start putting the pieces together.\nThe FAIR Guiding Principles for scientific data management and stewardship by (2016) provides an overview of the FAIR framework for accessible, reproducible data.\n\n\n\nTechnical Details\n\n Authoring in Quarto - an intro to Quarto for developing different kinds of documents. Lots of other resources linked here!!\n\n\n\nCoding Help\n\n Chapter 1 - 6 in Venables et al., An Introduction to R (Venables et al. 2009) - for a quick refresher on data types in R (it’s only 30 pages)\n Chapters 1-2 in Douglas et al., An Introduction to R - provides another intro to R that’s been updated and is an open-source book.\n This RStudio Education page has a lot of additional tutorials to help you get started with R."
  },
  {
    "objectID": "content/02-content.html#objectives",
    "href": "content/02-content.html#objectives",
    "title": "Tools of the Trade",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDescribe the benefits of reproducible data analysis workflows\nExplain the benefits of version control\nGenerate a Quarto document and render it into .html\nExecute your first commit in GitHub classroom"
  },
  {
    "objectID": "content/02-content.html#slides",
    "href": "content/02-content.html#slides",
    "title": "Tools of the Trade",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "Data Manipulation with the tidyverse",
    "section": "",
    "text": "We’ve established some of the reasons why data reproducibility and transparency are important. We’ve even gotten to the point where you can bring data into R from a variety of sources. Now we’ve actually got to start wrangling it into the sorts of things we can use for analysis. For a variety of reasons, this course relies on the tidyverse family of packages and its extensions into the spatial realm. Today, we’ll talk about why and begin to develop some intuition for building an analysis using the functions contained in the tidyverse family of packages."
  },
  {
    "objectID": "content/05-content.html#readings",
    "href": "content/05-content.html#readings",
    "title": "Data Manipulation with the tidyverse",
    "section": "Readings",
    "text": "Readings\n\nSetting the Stage\n Good enough practices in scientific computing by Wilson et al. (2017). Provides some helpful guidance on organizing projects for people that aren’t necessarily computer scientists.\n Pseudocode: what it is and how to write it - A nice blogpost by Sara Metawalli the sketches out the logic of pseudocode and why it can be helpful.\n\n\nTechnical Details\n Data Tidying from R. for Data Sciencefrom Wickham (2016) provides the logic behind tidy data and how it works in the tidyverse.\n Data Transformation from the same book introduces dplyr and the various functions for altering, summarizing, and reshaping data.\n Joins introduces relational databases and a tidy approach for merging datasets. This one will prove to be important when we start working with spatial data."
  },
  {
    "objectID": "content/05-content.html#objectives",
    "href": "content/05-content.html#objectives",
    "title": "Data Manipulation with the tidyverse",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nExplain the importance of readable code.\nArticulate the structure of readable scripts\nUtilize the verb, object, helper syntax of the tidyverse to modify your data in a reproducible way"
  },
  {
    "objectID": "content/05-content.html#slides",
    "href": "content/05-content.html#slides",
    "title": "Data Manipulation with the tidyverse",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "",
    "text": "Now that you have a bit of the fundamentals of geographic data and have had a chance to start using R, it’s time to get into more complicated workflows. To do that, you’ll have to have a bit more of a foundation in coordinates, coordinate reference systems, and geometries and how to access those in R. We’ll start there today and move into functions that change or relate geometries in the next few classes."
  },
  {
    "objectID": "content/07-content.html#readings",
    "href": "content/07-content.html#readings",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Chapter 2, Sections 1-3 and Chapter 3, Section 1 of Spatial Data Science by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)"
  },
  {
    "objectID": "content/07-content.html#objectives",
    "href": "content/07-content.html#objectives",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDefine coordinate, coordinate system, datum, and coordinate reference system\nAccess coordinate and geometry information for simple features in R\nUnderstand the rules for simple feature geometries\nAccess and transform the coordinate reference system for vector and raster data in R"
  },
  {
    "objectID": "content/07-content.html#slides",
    "href": "content/07-content.html#slides",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Recording"
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "Areal Data: Rasters",
    "section": "",
    "text": "Now that we’ve learned a bit about how to assess some of the important quantities of vector-based spatial data, we’ll try to apply a bit of the same logic to raster data. We’ll be using the terra package for the majority of raster options in this course primarily because it of its speed. That said, it is not a tidyverse package and so some of the intuition we used to organize the sf functions will be a little harder to extend here. I’ll do my best to help you make the links!!"
  },
  {
    "objectID": "content/09-content.html#readings",
    "href": "content/09-content.html#readings",
    "title": "Areal Data: Rasters",
    "section": "Readings",
    "text": "Readings\n\n Chapter 4 from Paula Moraga’s Spatial Statistics for Data Science: Theory and Practice with R provides a quick intro to using terra for raster and vector data.\n The terra reference page provides a brief overview of all of the functions and their categories. We’ll only focus on the SpatRaster methods.\n Raster Data Manipulation from the Spatial Data Science with R and terra ebook provides some nice examples of terra functions in the context of spatial workflows."
  },
  {
    "objectID": "content/09-content.html#objectives",
    "href": "content/09-content.html#objectives",
    "title": "Areal Data: Rasters",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nAccess the elements that define a raster\nBuild rasters from scratch using matrix operations and terra\nEvaluate logical conditions with raster data\nCalculate different measures of raster data"
  },
  {
    "objectID": "content/09-content.html#slides",
    "href": "content/09-content.html#slides",
    "title": "Areal Data: Rasters",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/11-content.html",
    "href": "content/11-content.html",
    "title": "Operations With Vector Data I",
    "section": "",
    "text": "Now that we’ve spent some time getting used to the syntax of the sf package and used it to assess some of the characteristics of vector objects (e.g., through predicates and measures), we’ll move into transformations. Transformations allow you to actually manipulate the geometries of a vector object (without necessarily changing the attributes themselves) and are a powerful tool for geting disparate data into some logical alignment. That said, transforming geometries can be complicated and often has some unanticipated consequences. That’s why we spent a little bit of time learning the mapping syntax as a means for you to be able to check yourself."
  },
  {
    "objectID": "content/11-content.html#readings",
    "href": "content/11-content.html#readings",
    "title": "Operations With Vector Data I",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Section 3.1 and 3.2 of Spatial Data Science, a bookdown project by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)."
  },
  {
    "objectID": "content/11-content.html#objectives",
    "href": "content/11-content.html#objectives",
    "title": "Operations With Vector Data I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nRecognize the unary, binary, and n-ary transformers\nArticulate common uses for unary and binary transformers\nUse unary transformations to fix invalid geometries\nImplement common binary transformers to align and combine data"
  },
  {
    "objectID": "content/11-content.html#slides",
    "href": "content/11-content.html#slides",
    "title": "Operations With Vector Data I",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\nPanopto Vidoe Here"
  },
  {
    "objectID": "content/13-content.html",
    "href": "content/13-content.html",
    "title": "Operations with Raster Data I",
    "section": "",
    "text": "Now that we’ve learned about predicates and measures with raster data, it’s time to learn more about some of the transformations that we can conduct with terra. We’ll start with some of the basic transformations that operate on the entire dataset then move to some of the important cell-wise operations."
  },
  {
    "objectID": "content/13-content.html#readings",
    "href": "content/13-content.html#readings",
    "title": "Operations with Raster Data I",
    "section": "Readings",
    "text": "Readings\n\n The terra package vignette describes the new raster functions available in terra, their relationship to those in the raster package, and the changes in syntax between the two.\n The Raster GIS Operations in R with terra chapter from Jasper Slingsby’s “A Minimal Introduction to GIS (in R)” bookdown project has worked examples of many of the operations we’ll learn today."
  },
  {
    "objectID": "content/13-content.html#objectives",
    "href": "content/13-content.html#objectives",
    "title": "Operations with Raster Data I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nAlign rasters for spatial processing\nAdjust the resolution of raster data\nCombine (or reduce) rasters to match the extent of your analysis"
  },
  {
    "objectID": "content/13-content.html#slides",
    "href": "content/13-content.html#slides",
    "title": "Operations with Raster Data I",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to today’s Panopto Slides"
  },
  {
    "objectID": "content/15-content.html",
    "href": "content/15-content.html",
    "title": "Combining Spatial and Tabular Data",
    "section": "",
    "text": "Today we’ll begin exploring typical workflows for spatial analysis by working with attribute data. Attributes generally provide additional information about a location that we can use for visualization and analysis. Unlike spatial operations that we’ll explore next week, attribute data do not all require geographic information (but they do need some means of relating to a geography). These chapters are not ‘prerequisite’ reading for the week, but provide a lot of helpful background for attribute operations in R."
  },
  {
    "objectID": "content/15-content.html#resources",
    "href": "content/15-content.html#resources",
    "title": "Combining Spatial and Tabular Data",
    "section": "Resources",
    "text": "Resources\n\n The Tidy Data and Relational Data sections from R For Data Science (Wickham and Grolemund 2016) provide a great overview to data cleaning and manipulation functions available in the tidyverse.\n Doing things with multiple tables has a lot of nice visual examples of for using the _join functions in dplyr.\n This article (Di Minin et al. 2021) provides a recent recap of a variety of reasons why we may need to combine data from multiple, often disparate, sources.\n The Spatial Data Operations Chapter in (Lovelace et al. 2019) makes the concepts of a network concrete (literally) by using a transportation route example to illustrate the various components of a network analysis in R."
  },
  {
    "objectID": "content/15-content.html#objectives",
    "href": "content/15-content.html#objectives",
    "title": "Combining Spatial and Tabular Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDefine spatial analysis\nDescribe the steps in planning a spatial analysis\nUnderstand the structure of relational databases\nUse attributes and topology to subset data\nGenerate new features using geographic data\nJoin data based on attributes and location"
  },
  {
    "objectID": "content/15-content.html#slides",
    "href": "content/15-content.html#slides",
    "title": "Combining Spatial and Tabular Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/17-content.html",
    "href": "content/17-content.html",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "",
    "text": "Note that the last 3 sections needed a bit of reorganization. I’ve moved to session 18 to keep the webpage aligned with the schedule."
  },
  {
    "objectID": "content/17-content.html#resources",
    "href": "content/17-content.html#resources",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "Resources",
    "text": "Resources"
  },
  {
    "objectID": "content/17-content.html#objectives",
    "href": "content/17-content.html#objectives",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:"
  },
  {
    "objectID": "content/17-content.html#slides",
    "href": "content/17-content.html#slides",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/19-content.html",
    "href": "content/19-content.html",
    "title": "Interpolation",
    "section": "",
    "text": "Point patterns give us the foundation for beginning geostatistical analyses. In geostatistical analyses, we have observations or a spatial process from a limited sample of locations, but would like to be able to infer the values of that process across the entire study region (or at least an area larger than we initially sampled). Interpolation provides one simple way of doing this that relies on the notion that we can learn something about the process simply from our measurements and the location those measurements were taken. We can extend these approaches by adding additional covariates and model structures, but we’ll start simple for now."
  },
  {
    "objectID": "content/19-content.html#resources",
    "href": "content/19-content.html#resources",
    "title": "Interpolation",
    "section": "Resources",
    "text": "Resources\n\n Chapter 2: Scale in (Fletcher and Fortin 2018) provides a thorough introduction to the ecologist’s conceptualization of scale with R examples.\n This article by Steven Manson (Manson 2008) provides a more comprehensive view of conceptualizations of scale.\n The Hypothesis Testing and Autocorrelation chapters of Manuel Gimond’s Introduction to GIS and Spatial Analysis bookdown project provide concrete examples of attempts to find process from spatial patterns.\n Chapter 12: Spatial Interpolation in Spatial Data Science by Edzer Pebesma and Roger Bivand provides examples of different types of kriging and interpolation using sf and stars."
  },
  {
    "objectID": "content/19-content.html#objectives",
    "href": "content/19-content.html#objectives",
    "title": "Interpolation",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDistinguish deterministic and stochastic processes\nDefine autocorrelation and describe its estimation\nArticulate the benefits and drawbacks of autocorrelation\nLeverage point patterns and autocorrelation to interpolate missing data\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Recording"
  },
  {
    "objectID": "content/21-content.html",
    "href": "content/21-content.html",
    "title": "Spatial Autocorrelation",
    "section": "",
    "text": "Now that we’ve learned about the power of spatial autocorrelation for interpolation from point data, it’s time to explore methods for spatial autocorrelation with areal data. We’ll have to define neighbors because distance is a little more ambiguous here and then look at some global and local measures of autocorrelation."
  },
  {
    "objectID": "content/21-content.html#resources",
    "href": "content/21-content.html#resources",
    "title": "Spatial Autocorrelation",
    "section": "Resources",
    "text": "Resources\n\n Ch. 7: Spatial Neighborhood Matrices in from Paula Moraga’s new book Spatial Statistics for Data Science: Theory and Practice with R gives a little gentler introduction to spatial neighbors specifically in the context of statistical models.\n Chapter 14 Proximity and Areal Data in Spatial Data Science by Edzer Pebesma and Roger Bivand provides explanations of how the spdep package can be used to construct neighborhood weights.\n\n Spatial Autocorrelation in R provides some easy code for working through neighbors with areal data and calculating spatial autocorrelation measures."
  },
  {
    "objectID": "content/21-content.html#objectives",
    "href": "content/21-content.html#objectives",
    "title": "Spatial Autocorrelation",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nUse the spdep package to identify the neighbors of a given polygon based on proximity, distance, and minimum number\nUnderstand the underlying mechanics of Moran’s I and calculate it for various neighbors\nDistinguish between global and local measures of spatial autocorrelation\nVisualize neighbors and clusters\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Video"
  },
  {
    "objectID": "content/23-content.html",
    "href": "content/23-content.html",
    "title": "Statistical Modelling II",
    "section": "",
    "text": "Last class we spent some time extending the idea of Favorability to build a foundation for treating overlay analysis as a logistic regression. Although logistic regression has a number of properties that make it desirable for inference, a number of recently developed statistical learning approaches have greatly improved our ability to take advantage a wide variety of available data and generate spatially explicit predictions. These methods may make interpretation and inference more challenging, but can improve the predictive ability of your models. We’ll explore some of those today."
  },
  {
    "objectID": "content/23-content.html#resources",
    "href": "content/23-content.html#resources",
    "title": "Statistical Modelling II",
    "section": "Resources",
    "text": "Resources\n\n An Introduction to Statistical Learning by (James et al. 2021) is a comprehensive introduction to a number of statistical learning techniques with examples in R. Although these examples are not necessarily spatial, the chapters provide a lot of the background necessary for understanding what the models are doing.\n A statistical explanation of MaxEnt for ecologists by (Elith et al. 2011) provides a relatively accessible description of the details of MaxEnt species distribution modeling.\n Random forests for Classification in Ecology by (Cutler et al. 2007) provides an introduction to the utility of Random Forests for ecologists."
  },
  {
    "objectID": "content/23-content.html#objectives",
    "href": "content/23-content.html#objectives",
    "title": "Statistical Modelling II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nArticulate the differences between statistical learning classifiers and logistic regression\nDescribe several classification trees and their relationship to Random Forests\nDescribe MaxEnt models for presence-only data\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nPanopto Video"
  },
  {
    "objectID": "content/25-content.html",
    "href": "content/25-content.html",
    "title": "Movement and Networks I",
    "section": "",
    "text": "Today we’re going to finish up some of the model evaluation and prediction pieces from last week and then take a brief detour into the use of spatial information in networks. To do that, we’ll need to introduce some basic concepts of networks before getting too far down the road of the syntax."
  },
  {
    "objectID": "content/25-content.html#resources",
    "href": "content/25-content.html#resources",
    "title": "Movement and Networks I",
    "section": "Resources",
    "text": "Resources\n\n Landscape connectivity: A graph-theoretic perspective by (Urban and Keitt 2001) introduces the notion of using networks and graphs to understand ecological connectivity.\n Connectivity for conservation: a framework to classify network measures by (Rayfield et al. 2011) helps simplify the (often overwhelming nature) of terminology and network metrics.\n Graphs and network science: and introduction provides a little simpler, but less spatially informed introduction to the tasks of network analysis."
  },
  {
    "objectID": "content/25-content.html#objectives",
    "href": "content/25-content.html#objectives",
    "title": "Movement and Networks I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nUse cross-validation to evaluate your models\nDefine a network and it’s key components\nIdentify major questions that we can address with networks\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nPanopto Video"
  },
  {
    "objectID": "content/29-content.html",
    "href": "content/29-content.html",
    "title": "Data Visualization and Maps I",
    "section": "",
    "text": "We’ve spent the last few weeks learning about operations to compile geographic information into databases for visualization and analysis. Because analysis requires you to know something about your data and because visualization is a great way to explore your data (especially when there’s a lot of it), we’ll turn to that next. For the next few weeks, we’ll be looking at different ways to visualize spatial data and the associated approaches in R. Note that this could be an entire course by itself, but hopefully you’ll get enough to get started making publication quality maps by the time we’re done"
  },
  {
    "objectID": "content/29-content.html#resources",
    "href": "content/29-content.html#resources",
    "title": "Data Visualization and Maps I",
    "section": "Resources",
    "text": "Resources\n\n The Introduction and Visualizing Geospatial Data chapters Principles of Figure Design section in (Wilke 2019) provide a useful set of general introductions to data visualization principles and practce that is “platform agnostic” (though much of Wilke’s work is done in R).\n The Look at Data and Draw Maps chapters in (Healy 2018) revisits many of the same ideas, but focuses specifically on R and ggplot2.\n This post on making maps people want to look at from ESRI is a nice, concise depiction of some core principles for planning a cartographic project.\n Making maps with R by (Lovelace et al. 2019) introduces the tmap package for making nice maps with relatively minimal syntax."
  },
  {
    "objectID": "content/29-content.html#objectives",
    "href": "content/29-content.html#objectives",
    "title": "Data Visualization and Maps I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDescribe some basic principles of data visualization\nExtend principles of data visualization to the development of maps\nDistinguish between several common types of spatial data visualization\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nPanopto Video"
  },
  {
    "objectID": "content/31-content.html",
    "href": "content/31-content.html",
    "title": "Introduction to Interactive Maps",
    "section": "",
    "text": "Now that you’ve had a chance to practice building a few maps and learning some of the core ideas behind the Grammar of Graphics, we can extend those ideas into the development of interactive webmaps and more expansive data visualizations that can be served on the internet and accessed by collaborators and members of the public. Like the previous unit on static maps, this could be a course unto itself, but we should be able to introduce you to enough ideas to get started."
  },
  {
    "objectID": "content/31-content.html#resources",
    "href": "content/31-content.html#resources",
    "title": "Introduction to Interactive Maps",
    "section": "Resources",
    "text": "Resources\n\n The Web-mapping section from the University Consortium for Geographic Information Science’s GIS & Technology Body of Knowledge has a nice overview of the topic and it’s origins.\n This post on User-Centered Design from Adobe provides a concise, general introduction to the core elements of User-Centered Design.\n The Maps chapter in (Sievert 2020) gives a nice demonstration of using plotly to build interactive maps. More importantly, the book provides a comprehensive resource for building interactive web-based visualization in R."
  },
  {
    "objectID": "content/31-content.html#objectives",
    "href": "content/31-content.html#objectives",
    "title": "Introduction to Interactive Maps",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDefine an API and their use in interactive visualization\nObtain a token for common mapping APIs\nBuild interactive maps using common packages\nRecognize other opportunities for interactive visuals with R\n\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Readings and slides",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before watching the lecture. On each class session page you’ll see buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:\n\n View all slides in new window  Download PDF of all slides\n\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes)."
  },
  {
    "objectID": "example/02-example.html",
    "href": "example/02-example.html",
    "title": "Your First Quarto doc",
    "section": "",
    "text": "Click this link to create the example repository in your github account.\nLog in to Rstudio Server and clone the repository to your Rstudio session using the instructions in the server.\nFollow the instructions in inclass02.qmd making sure to commit your changes as instructed.\nPush your changes to the remote repository\nTake a screenshot of your commit history and post it to our slack channel!"
  },
  {
    "objectID": "example/getting-setup.html",
    "href": "example/getting-setup.html",
    "title": "Getting Setup",
    "section": "",
    "text": "We are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\n\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account.\n\n\n\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\n\nCode\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\n\nCode\nusethis::create_github_token()\n\n\n\nStore your credential for use in RStudio\n\n\n\nCode\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\n\nVerify that Rstudio has saved your credential\n\n\n\nCode\ngitcreds_get()\n\n\nR should return something that looks like this:\n\n\n\n\n\n\n\n\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\n\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:\n\n\n\n\n\n\n\n\n\nEverytime you begin working on code, make sure you “Pull” from the remote repository to make sure you have the most recent version of things (this is especially important when you are collaborating with people).\nMake some changes to code\nSave those changes\n“Commit” those changes - Think of commits as ‘breadcrumbs’ they help you remember where you were in the coding process in case you need to revert back to a previous version. Your commit messages should help you remember what was ‘happening’ in the code when you made the commit. In general, you should save and commit fairly frequently and especially everytime you do something ‘consequential’. Git allows you to ‘turn back time’, but that’s only useful if you left enough information to get back to where you want to be.\nPush your work to the remote - when you’re done working on the project for the day, push your local changes to the remote. This will ensure that if you switch computers or if someone else is going to work on the project, you (or they) will have the most recent version. Plus, if you don’t do this, step 1 will really mess you up."
  },
  {
    "objectID": "example/getting-setup.html#lets-git-started",
    "href": "example/getting-setup.html#lets-git-started",
    "title": "Getting Setup",
    "section": "",
    "text": "We are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\n\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account.\n\n\n\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\n\nCode\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\n\nCode\nusethis::create_github_token()\n\n\n\nStore your credential for use in RStudio\n\n\n\nCode\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\n\nVerify that Rstudio has saved your credential\n\n\n\nCode\ngitcreds_get()\n\n\nR should return something that looks like this:\n\n\n\n\n\n\n\n\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\n\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:\n\n\n\n\n\n\n\n\n\nEverytime you begin working on code, make sure you “Pull” from the remote repository to make sure you have the most recent version of things (this is especially important when you are collaborating with people).\nMake some changes to code\nSave those changes\n“Commit” those changes - Think of commits as ‘breadcrumbs’ they help you remember where you were in the coding process in case you need to revert back to a previous version. Your commit messages should help you remember what was ‘happening’ in the code when you made the commit. In general, you should save and commit fairly frequently and especially everytime you do something ‘consequential’. Git allows you to ‘turn back time’, but that’s only useful if you left enough information to get back to where you want to be.\nPush your work to the remote - when you’re done working on the project for the day, push your local changes to the remote. This will ensure that if you switch computers or if someone else is going to work on the project, you (or they) will have the most recent version. Plus, if you don’t do this, step 1 will really mess you up."
  },
  {
    "objectID": "example/getting-setup.html#quarto",
    "href": "example/getting-setup.html#quarto",
    "title": "Getting Setup",
    "section": "Quarto",
    "text": "Quarto\nThis is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\n\n\n\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!"
  },
  {
    "objectID": "example/getting-setup.html#the-example",
    "href": "example/getting-setup.html#the-example",
    "title": "Getting Setup",
    "section": "The Example",
    "text": "The Example\n\nSetup\nThe University of Exeter has been conducting an ongoing survey to understand the age at which the belief in Santa Claus begins to drop off. A sample of the data is located in your assignment01 folder. Our task is to bring the data into R, conduct some preliminary exploration of the data, and then fit a model to the data to see if age predicts belief in Santa. We’ll start by branching off of the master Quarto doc in our GitHub repo and then work through the steps together.\n\n\nPseudocode\nBefore we get started, let’s sketch out the steps in our analysis using pseudocode. If you take a look at the tasks I’ve outlined above, you might construct your pseudocode like this:\n\n\nCode\nLOAD: all packages that we need for the analysis\nREAD: Data located in isthereasanta.txt\nCHECK: Data structure and values\nCLEAN: Are there odd values?\nPLOT: Age vs Belief\nMODEL: GLM of Age vs. belief\n\n\n\n\nProgramming\nNow that we have the basic steps in place, let’s transform the pseudocode into a repeatable Quarto document that explains what we’re doing, why, and what we found.\n\nLoad the packages\nPart of what makes R so powerful for data analysis is the number of ready-made functions and packages that are designed for all the things. That said, you can’t take advantage of that power if you don’t load them into your session so that their functions become available. In general, it’s best to do that first thing your document so that other folks can see what packages are necessary before you start running analyses. If you pay attention when these packages load, you may see warnings that a function is masked. This happens because two (or more) packages have functions with the same name. We can be explicit about which version we want by using packagename::functionname(). You’ll see that more later this semester.\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'ggplot2' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nRead the Data\nBased on our pseudocode our first step is the read the data. We can create headings in Quarto using different numbers of # symbols to keep things organized. The code below uses ``` to create the code chunk and then {r} to tell Quarto which environment to use when running it. I’m specifying a filepath because I’m not working within our git repo, this isn’t great practice, but it’s necessary for the webpage to render correctly. We use paste0 to combine the filepath with the file name (isthereasanta.txt) then read in the data using read_table.\n\n\nCode\nfilepath &lt;- \"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2022/assignment01/\"\n#READ\nsanta &lt;- read_table(paste0(filepath, \"isthereasanta.txt\"))\n\n\n\n\nCheck out the Data\nNow that we’ve got the data loaded and assigned it to the santa object. It’s always a good idea to take a look and make sure things look the way you expect, check for NAs, and get a basic understanding of the way your data is being represented by R. This process will get more involved once we start working with spatial data, but it’s good to get in the habit now. We’ll start by looking at the first few rows (using head()), then get a sense for the classes of data using str(), and check for any NAs.\n\n\nCode\nhead(santa)\nstr(santa)\nany(is.na(santa))\n\n\nYou’ll notice a few things. First, because we read this in using the read_table function, the result is a tibble. As such, head() returns both the data and the classes. This makes the result of str() largely redundant (note that if santa were a data.frame this would not be true). The combination of any() with is.na() asks whether any of the cells in santa have an NA value. You can see that there are NAs. Most statistical modeling functions in R don’t like NAs so we’ll try to clean those up here. Before we clean them, let’s try to learn what they are. We can use which() to identify the locations of the NAs.\n\n\nCode\nwhich(is.na(santa), arr.ind = TRUE)\n\n\nWe see that all of them are in the age column (our key predictor variable!). We could also have discovered this using summary().\n\n\nCode\nsummary(santa)\n\n\n\n\nClean the data\nDeciding how to clean NAs is an important decision. Many people choose to drop any incomplete records. We can do that with complete.cases() and see that the resulting object now has only 47 rows.\n\n\nCode\nsanta_complete_cases &lt;- santa[complete.cases(santa),]\n\n\nDropping the incomplete cases may seem like a “safe” approach, but what if there is some systematic reason for the data to be incomplete. Maybe older people are less likely to provide their age? If that’s the case, then dropping these cases may bias our dataset and the models that result. In that case, we may decide to “impute” values for the NAs based on some principled approach. We’ll talk more about what it means to take a principled approach to imputation later in this class. For now, let’s just try to strategies: 1 where we assign the mean() value of age and one where we assign the max() value (to reflect our hypothesis that older people may not provide their age). We’ll do this by using the ifelse() function. Note that we can only do this because all of the NAs are in a single column.\n\n\nCode\nsanta_mean &lt;- santa\nsanta_mean$Age &lt;- ifelse(is.na(santa_mean$Age), round(mean(santa_mean$Age, na.rm=TRUE),digits=0), santa_mean$Age)\n\nsanta_max &lt;- santa\nsanta_max$Age &lt;- ifelse(is.na(santa_max$Age), max(santa_max$Age, na.rm=TRUE), santa_max$Age)\n\n\n\n\nPlot the Data\nNow that we have a few clean datasets, let’s just take a quick look to see if our intuition is correct about the relationship between age and belief in santa. The idea isn’t so much to “prove” your hypothesis, but rather to get to know your data better as a means of identifying potential outliers and thinking about the distribution of your data.\n\n\nCode\nplot(Believe ~ Age, data=santa_complete_cases, main=\"Age vs. Belief in Santa (complete cases)\")\n\nplot(Believe ~ Age, data=santa_mean, main=\"Age vs. Belief in Santa (Age at mean)\")\n\nplot(Believe ~ Age, data=santa_max, main=\"Age vs. Belief in Santa (Age at max)\")\n\n\nThese plots highlight two things. First, because Believe is a logical variable, the only possible outcomes are 0 and 1. This means we can’t fit a typical linear regression (we’ll use a logistic regression instead). Also, we notice that our choice of imputation strategy makes a difference! Let’s fit some models and see what kind of difference it makes.\n\n\nFit Some Models\nWe’ll be using a generalized linear model for this analysis. The details will come up later, but for now, let’s keep it simple. The syntax for the glm() function is relatively straightforward. First we specify the model Believe ~ Age, then we tell it what family binomial(link=\"logit\"), then we remind R of the data. We use the binomial family because there are only 2 possible outcomes (TRUE and FALSE).\n\n\nCode\nfit_complete_cases &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_complete_cases)\nfit_mean &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_mean)\nfit_max &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_max)\n\nsummary(fit_complete_cases)$coef\nsummary(fit_mean)$coef\nsummary(fit_max)$coef\n\n\nWe see the older a person is, the less likely they are to believe in Santa! We also see that the choice of how we handle NAs affects the size of the effect, but not the direction. In class, we’ll write a function to simulate some new data based on this model and see if our results are robust to different assumptions.\n\n\n\nRendering the document\nWhen you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Intro to Spatial Data in R\n        ",
    "section": "",
    "text": "Intro to Spatial Data in R\n        \n        \n            Use R to load, visualize, and analyze spatial data\n        \n        \n            HES 505 • Fall 2025Human-Environment SystemsBoise State University\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\nInstructor\n\n   Dr. Matt Williamson\n   4125 Environmental Research Building\n   mattwilliamson@boisestate.edu\n   Schedule an appointment\n\n\n\nCourse details\n\n   Mondays and Wednesdays\n   August 25–December 12, 2025\n   1:30–2:45 PM\n   ILC 404\n   Slack\n\n\n\nContacting me\nE-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 48 hours (really), but also remember that life can be busy and chaotic for everyone (including me!), so if I don’t respond right away, don’t worry!"
  },
  {
    "objectID": "lesson/02-lesson.html",
    "href": "lesson/02-lesson.html",
    "title": "Introducing Git and Quarto",
    "section": "",
    "text": "Successfully clone a repository into your RStudio Session\nEdit a Quarto document, save the changes, and commit them to git\nRender a Quarto document into a final html page"
  },
  {
    "objectID": "lesson/02-lesson.html#objectives",
    "href": "lesson/02-lesson.html#objectives",
    "title": "Introducing Git and Quarto",
    "section": "",
    "text": "Successfully clone a repository into your RStudio Session\nEdit a Quarto document, save the changes, and commit them to git\nRender a Quarto document into a final html page"
  },
  {
    "objectID": "lesson/02-lesson.html#gitting-started",
    "href": "lesson/02-lesson.html#gitting-started",
    "title": "Introducing Git and Quarto",
    "section": "Gitting started",
    "text": "Gitting started\n\nCloning a repository to RStudio\nEach assignment in Github Classroom creates a repository for you in your Github account. To work on it; however, you’ll have to get it into Rstudio. I’ll show you how to do that now.\nStart by creating a new project. Click on the little cube in the top right of RStudio, it should give you a dropdown that looks like this:\n\nClick on the New project button it should bring up a window that looks like:\n\nClick on the version control option which will bring up\n\nSelect the Git option which will open:\n\nNow go back to the assignment page in GitHub and click the Code dropdown button (green in the topright)\n\nCopy the link that appears under the HTTPS option and pasted it in the repository URL in the RStudio Git interface.\nNow you should be able to click “Create Project” and clone the assignment into your RStudio server session. If you were successful, your RStudio screen should look something like this:\n\nYou’ll see a “Git” tab in the same pane as your “Environment” pane, you’ll see all of the files and folders associated with the webpage in the “Files” pane, you’ll see the project name in the top right corner next to the R box, and you see the branch you’re working on in the “Git” tab of your environment pane.\nNow that you’ve successfully linked RStudio to the repository (and pulled all of the current files). It’s time to start working on a document!"
  },
  {
    "objectID": "lesson/02-lesson.html#getting-to-know-quarto",
    "href": "lesson/02-lesson.html#getting-to-know-quarto",
    "title": "Introducing Git and Quarto",
    "section": "Getting to know Quarto",
    "text": "Getting to know Quarto\nWithin your project folder, you should see a /docs folder with a file called inclass02.qmd. Open that by clicking on it.This should automatically open the file in a “Source” pane above the “Console” pane.\n\nQuarto document structure\nAll Quarto documents start with a yaml header that provides a number of formatting options for Pandoc (the program that translates Markdown into the final format). This is the section of the document enclosed in the --- fence. You can find a description of options you can include in the Quarto documentation.\n\nFor this example, I want you to add your Name and Affilation to the document by including it in the yaml header. Based on the onling help, we can do that like this:\n\n\n\nCommitting your changes\nOnce you’ve made the change, save your document. When you do that you’ll notice that the file now shows up in the git tab of your environment pane:\n\nThis area is called the “Staging” area. It shows files that have been modified since you’re last commit and allows you to commit those changes to the version history in git, ignore the changes and stop tracking the file, or revert back to a previous version of the file. The “M” next to the file means the file has been modified and has changes to commit. The “?” next to the other file names means that the file is not currently tracked (meaning that it is the first version of the file and needs to be commited before it will be tracked.)\nAs noted by the “M”, we’ve made a change to the file that should be tracked in our version history. That means we need to commit it to the version history. To do that, click the box next to the assignment name and press the commit button. This will open a separate windoe called the “dif” (i.e. the differences between the previous version and the one you are committing) window. You’ll see the additions to the file highlighted in green and any subtractions highlighted in red.\n In the top-right corner, you’ll see a box for the “Commit message”. Each commit should have a short message that will remind you what you did in the step that you are committing. In this case, you changed the author information so let’s use that for our commit message. Type “change author” in the commit message window and press the “Commit” button. Once you do, you should see a window that looks like this:\n\nWhen you close the dif viewer, you’ll see that your git tab in the Environment pane has a note that you are 1 commit ahead of the remote repository, meaning that you have local changes that haven’t been included in the online version of your assignment.\n\n\n\nEditing text in Quarto\nThe next part of the example asks you to write some text and add color and formatting. There are a couple ways to do this in Quarto.\nYou’ll notice at the top of the “Source” pane that Quarto has two options for editing, one called “Source” and one called “Visual”. Quarto, like Rmarkdown, relies on the Markdown language for rendering text. Markdown is a simple plain-text language for formatting text, including images, and generating attractive documents that can be rendered to html, pdf (via pandoc), and even docx files. The “Source” editor allows you to write directly with Markdown while the “Visual” editor resembles a more traditional word-processing program with buttons for formatting text that automatically generate the appropriate markdown syntax. For simple tasks, the “Visual” editor works pretty well, but for more complex documents (with equations or lots of code) the “Source” editor is the way to go.\nUse the Markdown formatting guide or the Visual editor to make the necessary formatting changes.\n\n\nAdding Code Chunks\nThe real power of Quarto is its ability to include both code, results, and text all together. This happens through the use of code “chunks”. You can insert a code chunk by typing ``` or by clicking “Insert” -&gt; “Code Block” in the Visual Editor.\n\nAfter the 3 backticks, you’ll see text in brackets. The first bit (r) tells Quarto what language the code is in (Quarto can handle a lot of different syntax), the second (testchunk) is the name of this piece of code. Adding names allows you to navigate more easily through the document and identify which parts of the code might be breaking if/when you get error messages. On the next line you see several things that start with #| these are options for how you want the code to be rendered in the final document. Setting echo: true tells Quarto to print the code (not just the result) and eval: false tells Quarto not to actually run the code. You can find more about code chunk options here.\n\n\nRendering your document\nAssuming you’ve made it to the end of the example and committed all of your changes, it’s time to render the document to convert the markdown into the final form (in this case html). To do this, simply push the blue arrow button at the top of the source pane.\n\nYou should now see an html file with the same name as your original file in the docs folder.\n\n\nPushing your changes to the remote\nIn order to fully take advantage of version control with Rstudio, we need to keep our online repository up-to-date with our local changes. We do that using the the github workflow:\n\n“Pull” the most recent version (you did this at the beginning of the lesson) - this ensures that any changes that have been made since the last time you were working on the files are updated.\nMake changes and “commit” them (you’ve done this throughout the exercise) - this ensures that you can “go back in time” if you need to undo changes assuming that you’ve left yourself helpful reminders for what each commit was.\n\n3 “Push” those commits back up to the remote version. Assuming you’ve followed all of the instructions in the exercise you should have 4 (or more) commits that are beyond the online version of the repository. In order to get your local changes back to the remote, you need to “push” them. You can do that by pushing the green “up” arrow in the git pane.\n\nAssuming everything went smoothly, you’ll see something like this:\n\nYou can then go to the repository page in GitHub.\n You’ll see the last commit message at the top. On the right, you see a little clock with a reverse arrow. This is the “commit history”. If you click that, you’ll see all of the commits that you made so far. This lets you verify that all of your changes are now accessible from the remote."
  },
  {
    "objectID": "lesson/02-lesson.html#final-thoughts",
    "href": "lesson/02-lesson.html#final-thoughts",
    "title": "Introducing Git and Quarto",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThere’s lots more to say about using Quarto and navigating the git workflow. You can check out more information on git here including common errors and how to fix them. We’ll continue to build this throughout the semester, but for now I just wanted you to be able to make it through a simple workflow from start to finish and give you enough to finish your self reflection!"
  },
  {
    "objectID": "lesson/getting-setup.html",
    "href": "lesson/getting-setup.html",
    "title": "Get(git)ting Setup",
    "section": "",
    "text": "This class relies on several tools that you may not be used to: an online instance of RStudio, git, and GitHub classroom."
  },
  {
    "objectID": "lesson/getting-setup.html#logging-into-your-rstudio-instance",
    "href": "lesson/getting-setup.html#logging-into-your-rstudio-instance",
    "title": "Get(git)ting Setup",
    "section": "Logging into your RStudio instance",
    "text": "Logging into your RStudio instance"
  },
  {
    "objectID": "lesson/getting-setup.html#introducing-yourself-to-git",
    "href": "lesson/getting-setup.html#introducing-yourself-to-git",
    "title": "Get(git)ting Setup",
    "section": "Introducing yourself to git",
    "text": "Introducing yourself to git"
  },
  {
    "objectID": "lesson/getting-setup.html#using-github-classroom",
    "href": "lesson/getting-setup.html#using-github-classroom",
    "title": "Get(git)ting Setup",
    "section": "Using GitHub classroom",
    "text": "Using GitHub classroom"
  },
  {
    "objectID": "lesson/quarto.html",
    "href": "lesson/quarto.html",
    "title": "Quarto and literate programming",
    "section": "",
    "text": "This is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\n\n\n\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!"
  },
  {
    "objectID": "lesson/quarto.html#quarto",
    "href": "lesson/quarto.html#quarto",
    "title": "Quarto and literate programming",
    "section": "",
    "text": "This is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\n\n\n\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!"
  },
  {
    "objectID": "lesson/quarto.html#the-example",
    "href": "lesson/quarto.html#the-example",
    "title": "Quarto and literate programming",
    "section": "The Example",
    "text": "The Example\n\nSetup\nThe University of Exeter has been conducting an ongoing survey to understand the age at which the belief in Santa Claus begins to drop off. A sample of the data is located in your assignment01 folder. Our task is to bring the data into R, conduct some preliminary exploration of the data, and then fit a model to the data to see if age predicts belief in Santa. We’ll start by branching off of the master Quarto doc in our GitHub repo and then work through the steps together.\n\n\nPseudocode\nBefore we get started, let’s sketch out the steps in our analysis using pseudocode. If you take a look at the tasks I’ve outlined above, you might construct your pseudocode like this:\n\n\nCode\nLOAD: all packages that we need for the analysis\nREAD: Data located in isthereasanta.txt\nCHECK: Data structure and values\nCLEAN: Are there odd values?\nPLOT: Age vs Belief\nMODEL: GLM of Age vs. belief\n\n\n\n\nProgramming\nNow that we have the basic steps in place, let’s transform the pseudocode into a repeatable Quarto document that explains what we’re doing, why, and what we found.\n\nLoad the packages\nPart of what makes R so powerful for data analysis is the number of ready-made functions and packages that are designed for all the things. That said, you can’t take advantage of that power if you don’t load them into your session so that their functions become available. In general, it’s best to do that first thing your document so that other folks can see what packages are necessary before you start running analyses. If you pay attention when these packages load, you may see warnings that a function is masked. This happens because two (or more) packages have functions with the same name. We can be explicit about which version we want by using packagename::functionname(). You’ll see that more later this semester.\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'ggplot2' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nRead the Data\nBased on our pseudocode our first step is the read the data. We can create headings in Quarto using different numbers of # symbols to keep things organized. The code below uses ``` to create the code chunk and then {r} to tell Quarto which environment to use when running it. I’m specifying a filepath because I’m not working within our git repo, this isn’t great practice, but it’s necessary for the webpage to render correctly. We use paste0 to combine the filepath with the file name (isthereasanta.txt) then read in the data using read_table.\n\n\nCode\nfilepath &lt;- \"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2022/assignment01/\"\n#READ\nsanta &lt;- read_table(paste0(filepath, \"isthereasanta.txt\"))\n\n\n\n\nCheck out the Data\nNow that we’ve got the data loaded and assigned it to the santa object. It’s always a good idea to take a look and make sure things look the way you expect, check for NAs, and get a basic understanding of the way your data is being represented by R. This process will get more involved once we start working with spatial data, but it’s good to get in the habit now. We’ll start by looking at the first few rows (using head()), then get a sense for the classes of data using str(), and check for any NAs.\n\n\nCode\nhead(santa)\nstr(santa)\nany(is.na(santa))\n\n\nYou’ll notice a few things. First, because we read this in using the read_table function, the result is a tibble. As such, head() returns both the data and the classes. This makes the result of str() largely redundant (note that if santa were a data.frame this would not be true). The combination of any() with is.na() asks whether any of the cells in santa have an NA value. You can see that there are NAs. Most statistical modeling functions in R don’t like NAs so we’ll try to clean those up here. Before we clean them, let’s try to learn what they are. We can use which() to identify the locations of the NAs.\n\n\nCode\nwhich(is.na(santa), arr.ind = TRUE)\n\n\nWe see that all of them are in the age column (our key predictor variable!). We could also have discovered this using summary().\n\n\nCode\nsummary(santa)\n\n\n\n\nClean the data\nDeciding how to clean NAs is an important decision. Many people choose to drop any incomplete records. We can do that with complete.cases() and see that the resulting object now has only 47 rows.\n\n\nCode\nsanta_complete_cases &lt;- santa[complete.cases(santa),]\n\n\nDropping the incomplete cases may seem like a “safe” approach, but what if there is some systematic reason for the data to be incomplete. Maybe older people are less likely to provide their age? If that’s the case, then dropping these cases may bias our dataset and the models that result. In that case, we may decide to “impute” values for the NAs based on some principled approach. We’ll talk more about what it means to take a principled approach to imputation later in this class. For now, let’s just try to strategies: 1 where we assign the mean() value of age and one where we assign the max() value (to reflect our hypothesis that older people may not provide their age). We’ll do this by using the ifelse() function. Note that we can only do this because all of the NAs are in a single column.\n\n\nCode\nsanta_mean &lt;- santa\nsanta_mean$Age &lt;- ifelse(is.na(santa_mean$Age), round(mean(santa_mean$Age, na.rm=TRUE),digits=0), santa_mean$Age)\n\nsanta_max &lt;- santa\nsanta_max$Age &lt;- ifelse(is.na(santa_max$Age), max(santa_max$Age, na.rm=TRUE), santa_max$Age)\n\n\n\n\nPlot the Data\nNow that we have a few clean datasets, let’s just take a quick look to see if our intuition is correct about the relationship between age and belief in santa. The idea isn’t so much to “prove” your hypothesis, but rather to get to know your data better as a means of identifying potential outliers and thinking about the distribution of your data.\n\n\nCode\nplot(Believe ~ Age, data=santa_complete_cases, main=\"Age vs. Belief in Santa (complete cases)\")\n\nplot(Believe ~ Age, data=santa_mean, main=\"Age vs. Belief in Santa (Age at mean)\")\n\nplot(Believe ~ Age, data=santa_max, main=\"Age vs. Belief in Santa (Age at max)\")\n\n\nThese plots highlight two things. First, because Believe is a logical variable, the only possible outcomes are 0 and 1. This means we can’t fit a typical linear regression (we’ll use a logistic regression instead). Also, we notice that our choice of imputation strategy makes a difference! Let’s fit some models and see what kind of difference it makes.\n\n\nFit Some Models\nWe’ll be using a generalized linear model for this analysis. The details will come up later, but for now, let’s keep it simple. The syntax for the glm() function is relatively straightforward. First we specify the model Believe ~ Age, then we tell it what family binomial(link=\"logit\"), then we remind R of the data. We use the binomial family because there are only 2 possible outcomes (TRUE and FALSE).\n\n\nCode\nfit_complete_cases &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_complete_cases)\nfit_mean &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_mean)\nfit_max &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_max)\n\nsummary(fit_complete_cases)$coef\nsummary(fit_mean)$coef\nsummary(fit_max)$coef\n\n\nWe see the older a person is, the less likely they are to believe in Santa! We also see that the choice of how we handle NAs affects the size of the effect, but not the direction. In class, we’ll write a function to simulate some new data based on this model and see if our results are robust to different assumptions.\n\n\n\nRendering the document\nWhen you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document."
  },
  {
    "objectID": "resource/data.html",
    "href": "resource/data.html",
    "title": "Fun datasets",
    "section": "",
    "text": "So much data, so little time… Here are some links to help you get started finding data for your geospatial projects"
  },
  {
    "objectID": "resource/data.html#spatial-data-repositories",
    "href": "resource/data.html#spatial-data-repositories",
    "title": "Fun datasets",
    "section": "Spatial Data Repositories",
    "text": "Spatial Data Repositories\n\nDataBasin: Lots of spatial data related to conservation issues across the US. The AdaptWest portal has tons of spatial data on climate change and its potential impacts.\nUS Protected Areas Database: PAD-US is America’s official national inventory of U.S. terrestrial and marine protected areas that are dedicated to the preservation of biological diversity and to other natural, recreation and cultural uses, managed for these purposes through legal or other effective means. PAD-US also includes the best available aggregation of federal land and marine areas provided directly by managing agencies, coordinated through the Federal Geographic Data Committee (FGDC) Federal Lands Working Group.\nUSGS Gap Analysis Project: A variety of datasets depicting land cover and species distributions."
  },
  {
    "objectID": "resource/data.html#general-data-repositories",
    "href": "resource/data.html#general-data-repositories",
    "title": "Fun datasets",
    "section": "General Data Repositories",
    "text": "General Data Repositories\n\nData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\nGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.\nKaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\nUS City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing."
  },
  {
    "objectID": "resource/data.html#political-science-and-economics-datasets",
    "href": "resource/data.html#political-science-and-economics-datasets",
    "title": "Fun datasets",
    "section": "Political science and economics datasets",
    "text": "Political science and economics datasets\nThere’s a wealth of data available for political science- and economics-related topics:\n\nFrançois Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities.\nThomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.).\nErik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)\nInside AirBnB a Creative Commons-licensed dataset with a ton of spatially referenced info on AirBnBs in cities across the globe."
  },
  {
    "objectID": "resource/data.html#the-30daymapchallenge",
    "href": "resource/data.html#the-30daymapchallenge",
    "title": "Fun datasets",
    "section": "The #30daymapchallenge",
    "text": "The #30daymapchallenge\nThe #30daymapchallenge is a social mapping/cartography/data visualization challenge designed to encourage experimentation with different types of datasets and mapping approaches. Searching the hashtag on social media (especially Twitter) will bring up a bunch of cool examples. Here are a few repositories to help you get started:\n\nThe Official #30DayMapChallenge Repo has an archive of past challenges and a description of what this is all about.\nBob Rudis’ 2019 bookdown project Contains both code and useful information for generating the visualizations along with sources for data.\nAlexandra Kapp’s 2020 repository makes use of some of the newer animation and interactive visualization techniques.\nThe R-Spatial list of 2020 challenge repositories"
  },
  {
    "objectID": "resource/getting-setup.html",
    "href": "resource/getting-setup.html",
    "title": "Getting Set Up",
    "section": "",
    "text": "This is an intro to git and github, most of which was originally created by Jessica Taylor and Nora Honkomp (2 former students in this class!)."
  },
  {
    "objectID": "resource/getting-setup.html#lets-git-started",
    "href": "resource/getting-setup.html#lets-git-started",
    "title": "Getting Set Up",
    "section": "Let’s “git” started",
    "text": "Let’s “git” started\nWe are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\nAccept the invitation to the assignment repo\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account."
  },
  {
    "objectID": "resource/getting-setup.html#installload-required-package",
    "href": "resource/getting-setup.html#installload-required-package",
    "title": "Getting Set Up",
    "section": "Install/Load Required Package",
    "text": "Install/Load Required Package\nLoad the following packages in RStudio. If you do not have them installed, you can do so using install.packages().\n\nlibrary(usethis)\nlibrary(gitcreds)\nlibrary(knitr)"
  },
  {
    "objectID": "resource/getting-setup.html#create-a-github-account",
    "href": "resource/getting-setup.html#create-a-github-account",
    "title": "Getting Set Up",
    "section": "Create a GitHub Account",
    "text": "Create a GitHub Account\nYou will need to access GitHub with an account for this tutorial. If you don’t already have an account, you can sign up for free here: https://github.com/\nYou will be asked to sign up using your email, a password you create, and a username. Your username is what will be visible to others that you collaborate with, so it’s a good idea to make it something straight forward and professional. You can skip personalization for now by scrolling to the bottom of the page."
  },
  {
    "objectID": "resource/getting-setup.html#github-and-rstudio-server",
    "href": "resource/getting-setup.html#github-and-rstudio-server",
    "title": "Getting Set Up",
    "section": "Github and RStudio Server",
    "text": "Github and RStudio Server\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\nusethis::create_github_token()\n\n\nStore your credential for use in RStudio\n\n\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\nVerify that Rstudio has saved your credential\n\n\ngitcreds_get()\n\nR should return something that looks like this:\n\n\n\n\n\n\nBring the project into RStudio\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\nVerify that the “Git” tab is available and that your project is shown in the upper right-hand corner\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:"
  },
  {
    "objectID": "resource/getting-setup.html#installing-git-for-your-local-machine",
    "href": "resource/getting-setup.html#installing-git-for-your-local-machine",
    "title": "Getting Set Up",
    "section": "Installing Git (for your local machine)",
    "text": "Installing Git (for your local machine)\nYou will need the program Git for this tutorial. First, let’s check to see if Git is already installed.\nSelect the Terminal tab next to the Console tab in the section of Rstudio displaying your console (bottom-left panel). Enter the following:\n\nwhich git\ngit --version\n\nYou will get something like the above output if Git is already installed. If it is not installed, you will get something like git: command not found.\n\nWindows\nDownload the program here: https://git-scm.com/downloads\n\n\nMac\nYou may have been prompted to install command line developer tools. You should accept this offer. If you were not prompted, use this command:\n\nxcode-select --install\n\n\n\nIntroduce Yourself to Git in R\nOnce Git is successfully installed and you have a GitHub account, you will need to let R/Git know your account information to access the remote repositories.\nEnter the following in the Console tab, substituting the user name and email with your GitHub user name and email:\n\nusethis::use_git_config(user.name = \"Bob Barker\", user.email = \"bobbarker@thepriceisright.org\")\n\n\n\nGet a Personal Access Token\nA personal access token (PAT) is used for GitHub as a type of authentication. You used to be able to use your username and password, but not any more. The token-based authentication has increased security.\nBefore generating a new PAT, check to see if you already have one. Run gitcreds_set() and one of two things will happen:\n\n\nR will prompt you to enter a token. This means you don’t already have one and need to create one. Hit Esc and run the second line of code, create_github_token(). This will bring you to GitHub where you can create a token. Save this token somewhere safe - you will not be able to access it via GitHub again.\nR will show you the saved credentials (username and password) and give you options to exit without changing (1: Abort), replace the credentials, or to see the current token. This means you already have a token and can keep it or change it if it has expired. If it expired and you need a new one, run create_github_token().\n\n\n## Run this to see if you already have credentials, or to change them\ngitcreds::gitcreds_set()\n## Only run this if you need to generate a token\ncreate_github_token()"
  },
  {
    "objectID": "resource/getting-setup.html#version-control",
    "href": "resource/getting-setup.html#version-control",
    "title": "Getting Set Up",
    "section": "Version Control",
    "text": "Version Control\nThe main function of Git is version control. This means Git will track the changes you make so you can revert to (or view) previous versions of the document. In order for this to work effectively, you need to start tracking your changes (make a repository) and frequently create versions with changes you have made (committing)."
  },
  {
    "objectID": "resource/getting-setup.html#setting-up-a-local-repository",
    "href": "resource/getting-setup.html#setting-up-a-local-repository",
    "title": "Getting Set Up",
    "section": "Setting Up a Local Repository",
    "text": "Setting Up a Local Repository\nAnother function of Git is to store all your relevant files in a repository, similar to a project. You can create a repository that is located only on your device (therefore it is considered “local”). This will allow you to track changes to a project on your computer and access previous versions of the document at any time. We will look at how to create a local repository using Rstudio, but know that it is also possible to do this using the terminal. (See page 96 of Gandrud (2015))\n\nIn Rstudio, select File in the top left corner, and then New Project\nWhen the New Project menu appears, select New Directory\nOn the following screen, select New Project\nFinally, type the name you want to use for your new project, browse to the location where you want it saved on your computer, and select the Create a git repository box.\n\n\nYou now have a folder with a .Rproj object and a .gitignore file. There is also a hidden .git folder that stores all the project information, including the version history files (commit history).\n\nAdding, Staging, and Committing\nYou can create new files or move existing files (including your data) into this project folder, and Git can track changes made to them. In order to do this, you will need to “add” files that have been created or moved into the project and “commit” these changes.\nFirst, you will need to save the new file or move an existing file into the project folder. Then, you will “add” those files to your commit by checking the boxes in the Git tab in your Rstudio window. If Git is already tracking a document, you will see an M in the status column, however, if the file hasn’t been added (and is therefore not being tracked), you will see a ? in the status column.\n\nIn order to save a version of your project as it is right now (with these added files), you will need to commit. To do this, select the Commit button, above the boxes you just checked. This will open a new window where you can see the list of files that you added.\nType a useful message in the Commit message box briefly describing the changes you made in this version. Then hit Commit.\n\nYou should see a window pop up telling you what changes were successfully made. If this window ever says “failed”, “execution halted”, or “aborted”, this means the commit did not work and you should read the message closely to determine why.\nSave and commit frequently in order to get the most use out of your version control."
  },
  {
    "objectID": "resource/getting-setup.html#branches-and-merging",
    "href": "resource/getting-setup.html#branches-and-merging",
    "title": "Getting Set Up",
    "section": "Branches and Merging",
    "text": "Branches and Merging\n\nBranches\nA new repository will have one branch called main. You can think of this as the master version. You can create additional branches which are an exact copy of the main branch where you can make changes without committing them to the master. This is useful when several people are working on the same thing, or if you want to try multiple approaches to the same file (i.e. test run some code).\nTo create a branch, select the button with small purple shapes on the right hand side of your Git tab in Rstudio.\n\nYou can now create a new branch, for example one called “test”. This is essentially a copy of everything that is in your repository, and making changes here will not affect the main branch.\nTo switch between branches, select the drop down menu to the right of the button you used to create a new branch.\n\nYou can easily switch to a different branch by selecting it on this menu.\nNote that any files you create on a side branch will not show up in the main branch unless you merge them.\nYou can have multiple branches coming off your main branch at once, so you can try multiple different approaches (or by multiple people) simultaneously. Anytime you make a new branch it will start as a duplicate of the main branch.\nYou can see a history of your commits on different branches by selecting the clock icon (designating “History”) near the Commit button on your Git tab. You will initially see the history for only the branch you are currently in, but if you select the drop down menu of branches at the top of this window, you can select another branch or all branches and see how the branches compare to one another.\n\n\n\nMerging\nIf you find a method that works in a side branch and you want to bring it in to the main branch, this is called a merge.\nTo merge a side branch with the main branch (changing the version of your repo main branch to match that of the successful side branch), we will use the terminal.\n\nMake sure you are on the main branch\nSelect the Terminal tab next to the Console tab in the section of Rstudio displaying your console.\nType git merge and then the name of the branch you want to merge into the main branch.\n\n\nThe repository of your main branch should now match the repository of whatever branch you merged with it.\nOne way to check if the branches merged the way you intended is to select the history button again (make sure you go to all branches) and check that the “HEAD” (which is the main branch) is at the same level as whichever branch you merged it with."
  },
  {
    "objectID": "resource/getting-setup.html#setting-up-remote-repositories",
    "href": "resource/getting-setup.html#setting-up-remote-repositories",
    "title": "Getting Set Up",
    "section": "Setting Up Remote Repositories",
    "text": "Setting Up Remote Repositories\nTo set up a remote repository, make sure you are able to log into GitHub. We will go over three different ways to make repositories: starting from scratch with a new repository, creating a repository for an already started project, and accessing a repository made by a collaborator.\n\nNew Repository\nTo create a brand new repository, click on the plus sign near your profile picture in the top right corner of GitHub and select New repository.\n\nOn the next page, type in a name for your repository and choose whether you want it to be public or private. It is recommended you select the box to create a README file. This will give you a place to describe the layout of your repository and the purpose of each file. Without a README file, visitors to your repository (and maybe future you) might not be able to figure out how to properly use the files in your repository, leading to your working being non-reproducible.\n\nNow you can click Create Repository.\nOnce your repository is created, you will see only the README file is present. To create other files, let’s create a directory for this repo on our computer.\n\nIn R studio, follow the instructions for creating a new project, but when you see the following menu, select Version Control this time.\n\n\n\n\nOn the next window, select Git.\nReturn to the GitHub page for the repository you created and select the green Code button.\nCopy the HTTPS link from the menu that pops up.\n\n\n\nReturn to Rstudio and paste the link in the Repository URL line at the top of the popped up window. Make sure to check the file path that is currently set and use the Browse button if you want to save the folder for this directory in a different place.\n\n\nYour repository is now set up in a new project in Rstudio, and you can begin by creating an Rmarkdown, R Script, etc.\nNote, you can edit the contents of the README file by opening it in Rstudio.\n\n\nFrom an Existing Project\nYou may find yourself wanting to create a repo on GitHub for a project you have already started working on. Fortunately, it is easy to start version control tracking on a project and add the project to a remote repository.\nHere are the steps to follow if you have a project folder with a .Rproj file in it and your directory is not already being tracked with Git:\n\nFollow the steps above for creating a New repository, but this time do not click the box to create a README file. This will bring you to a page with a “Quick Set Up” link. Keep this page open for later.\n\n\n\n\nGo to RStudio and open the .Rproj file that will be added to the GitHub repository./\nSelect the Terminal tab next to Console and enter the following lines of code.\n\n\n$ git init -b main\n\n“git” tells bash what program we want to use   “init” tells bash to initialize a Git repository fir this directory\n“-b main” is saying we want to create a branch called “main”\n\n\n$ git add .\n\nThis will add all of the files within the current folder to the repository. You may get a lot of warnings because you have a .Rproj file and potentially a .Rhistory file in this folder which are not usually ideal to track. We will take care of this by adding these file names to the .gitignore file shortly.\n\n$ git commit -m \"First commit\"\n\nThis line creates your first commit. You can change the commit message to anything that makes sense to you.\n\nGo back to GitHub and copy the Quick start link. Type the following code in the terminal, but replace &lt;REMOTE_URL&gt; with the copied link.\n\n\n$ git remote add origin &lt;REMOTE_URL&gt;\n# Replace \"&lt;REMOTE_URL&gt;\" with url from GitHub\n\n\nLast, we will push these changes to GitHub. We will further discuss what this means later, but for now run the following line in the terminal.\n\n\n$ git push origin main\n\nNow when you return to GitHub and refresh your repository page, you should see all of the files from your existing project.\nYou can add a README and .gitignore on the GitHub website by selecting “Add file” on the repository’s page. When you make a .gitignore file, it may suggest you use a template, in which case, select the R template from the drop down list and it will automatically fill the document with file types that should typically not be tracked.\n\n\nCloning a Repository\nIf you want to join a repository that has already been created, you can either find the repository by searching for it on GitHub (if it is publicly available) or contact the creator and have them add you as a collaborator to the repository. Either way, you will follow steps 1-5 in the “New Repository” section above, but this time the link is coming from the already created repository.\nIn cases where the repository is not accessible (you do not have cloning priveleges), you will have to create a pull request."
  },
  {
    "objectID": "resource/getting-setup.html#pushing-and-pulling",
    "href": "resource/getting-setup.html#pushing-and-pulling",
    "title": "Getting Set Up",
    "section": "“Pushing” and “Pulling”",
    "text": "“Pushing” and “Pulling”\nOnce you have your remote repository ready, you can make changes to the files and “add” and “commit” them like we did in the “Version Control” section above. However, now we must take steps to make sure our local version of the repository is up-to-date with the online version, and the versions that all other collaborators have on their computers.\nTo do this, we will need to “push” and “pull”. “Pulling” is when we bring recent and out-of-sync changes from the online version to our local device. “Pushing” is the opposite; we are sending our commits to the online version to update it with our recent changes. This will allow anyone else working in the repository to “pull” your changes onto their computer.\nTo push and pull, use the blue and green arrows on the Git tab in R studio.\n\nYou may also notice the next time you “commit” your changes, these same push and pull buttons are located above the “Commit Message” box on the pop up window.\nIt is good practice to “pull” each time you are about to start working in a repository and to push after you commit, or at least once at the end of your working period within a repository for the day. Keeping good “push” and “pull” habits will help you avoid merge conflicts with collaborators or yourself if you work on a project on more than one computer."
  },
  {
    "objectID": "resource/getting-setup.html#adding-and-managing-collaborators",
    "href": "resource/getting-setup.html#adding-and-managing-collaborators",
    "title": "Getting Set Up",
    "section": "Adding and Managing Collaborators",
    "text": "Adding and Managing Collaborators\nCollaborative coding is a huge benefit of GitHub. In order to invite your collaborators to clone your remote repository, you will need to know their GitHub username, or at least their email address.\n\nOn your repository page on GitHub, select Settings in the middle of the banner near the top of the page.\n\n\n\nIn the menu on the left, select the Collaborators page.\n\n\n\nEnter your password, and then select Add People under Manage Access\n\nAdd your collaborators one at a time. This will send them a message inviting them to join the repository.\nAs the owner of the repository, you will be able to remove people from the repository at any time."
  },
  {
    "objectID": "resource/getting-setup.html#merge-conflicts",
    "href": "resource/getting-setup.html#merge-conflicts",
    "title": "Getting Set Up",
    "section": "Merge Conflicts",
    "text": "Merge Conflicts\nWhen two people are working on a branch at the same time, changes were made and not pushed before someone else started working, or a collaborator forgets to pull before starting to make changes, a merge conflict may arise. Merge conflicts happen because Git is not sure how to combine the different changes that occurred in the same sections of the document. In some instances, Git is smart enough to figure it out and will merge the versions on its own. Other times, the merge conflict will be need to be fixed manually. Note that Git will not allow the push until the merge conflict is solved.\nIf a merge conflict occurs, you will see some specific things added to your code. There will be a line at the beginning that starts with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, a line at the end starting with &gt;&gt;&gt;&gt;&gt;&gt;&gt; and a line in the middle that just has =======. The two different version are shown above and below the middle line, labeled with which branch has which version. It is your job to decide how these versions fit together. Once you have modified the code to match the final version you want to keep, you can delete the three lines of code containing the &gt;, &lt; and = symbols and “stage” and “commit” your files as usual.\n\n\nOn GitHub\nYou can also merge branches and resolve merge conflicts on GitHub. You would push your branch to GitHub, then go to the GitHub repo webpage. You will need to create a pull request to merge your branch. If there is a merge conflict, you will need to click on “Resolve conflicts” before you can merge your branch. The syntax to edit the document and resolve the conflict is the same as above."
  },
  {
    "objectID": "resource/getting-setup.html#cloning-branching-and-forking-oh-my",
    "href": "resource/getting-setup.html#cloning-branching-and-forking-oh-my",
    "title": "Getting Set Up",
    "section": "Cloning, Branching, and Forking, Oh My!",
    "text": "Cloning, Branching, and Forking, Oh My!\nCloning, branching, and forking are functions that are similar, but they are not the same. When you clone a repository, you are connected to it and are working in that repository. You can commit changes and push them to the same repository. When you create a branch, you create a copy where you can work on a specific part of a document or run test code with the intent to merge it back to the main branch. Many branches are short-lived and deleted once their purpose has been served. Anyone that has access to the repository also has access to the branches in it. A fork creates a copy of the entire repository as well, however, the collaborators are disconnected from it. The intent is generally to diverge from the original repository and never be merged back into it."
  },
  {
    "objectID": "resource/getting-setup.html#gitignore-and-large-files",
    "href": "resource/getting-setup.html#gitignore-and-large-files",
    "title": "Getting Set Up",
    "section": ".gitignore and Large Files",
    "text": ".gitignore and Large Files\nGitHub does not allow repositories to be larger than 5GB. While we recommend keeping all of the files necessary to run your analysis together in the repo, this may not be possible if say for example your data files are larger than the size limit. If this is the case, you can manually distribute your data file a different way. (See https://git-lfs.github.com/ first though if this is an issue you are actually having.)\nOnce all collaborators (or just you) have the large data file in your Git repo, you may forget that you cannot send this to GitHub and accidentally try to commit and push it. Fortunately, Git will recognize the problem before it takes place and will give you the following warning:\n\nTo resolve this problem, you just need to tell Git to ignore the large file when you make your commit. First, determine which file(s) is/are too big by looking at their size. Then, navigate to your .gitignore document and open it. Last, add the name(s) of the file(s) that is/are too large. Now you will be able to commit and push the tracked files within your repository.\nYou can add any files you do not want to be tracked to .gitignore. An example is the html file generated by rendering this document. It is regenerated constantly, so there is no need to track the changes."
  },
  {
    "objectID": "resource/getting-setup.html#amend-commits",
    "href": "resource/getting-setup.html#amend-commits",
    "title": "Getting Set Up",
    "section": "Amend Commits",
    "text": "Amend Commits\nIf at any point you find you have made a commit that should not have been made (e.x. you accidentally added and committed files that are too large and now you aren’t able to push), you can easily fix this by making the necessary changes within your files, checking the “amend previous commit” box on the commit screen, and then committing as usual. This will overwrite your previous commit with the correct version you want to commit. If you need to amend an earlier commit (i.e. not the most recent commit), you will need to use the terminal (See Oh S#!*, Git!?!)."
  },
  {
    "objectID": "resource/getting-setup.html#installing-a-git-client",
    "href": "resource/getting-setup.html#installing-a-git-client",
    "title": "Getting Set Up",
    "section": "Installing a Git Client",
    "text": "Installing a Git Client\nWith all of the commits, branching, merging, and collaboration, it can be tricky to keep track of everything going on in your repository. Viewing the commit history in GitHub or under the Git tab in RStudio is useful, though it can still be a little hard to follow. Using a Git client software helps visualize the workflow and allows you to use commands in the graphic user interface (GUI) that you would normally need to type into the terminal.\n\nThere are a few Git Clients out there and you may want to try a few to see what works for you. I use GitKraken which you can download here."
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Resources",
    "section": "",
    "text": "I have included a bunch of extra resources and guides related to R and coding, potentially interesting data, and cool visualizations. Let me know when you find fun things to include here!"
  },
  {
    "objectID": "resource/lastyear.html",
    "href": "resource/lastyear.html",
    "title": "Last year’s class",
    "section": "",
    "text": "Last year was the first time I taught this class in it’s current format. I built a number of worked examples to try to clarify how different parts of a spatial workflow come together. Although the examples are far from perfect (I’m hoping this year’s are better), the page does have a number of potentially useful pieces. Check out the examples to access these."
  },
  {
    "objectID": "resource/rmarkdown.html",
    "href": "resource/rmarkdown.html",
    "title": "Authoring in Rmarkdown and Quarto",
    "section": "",
    "text": "[Rmarkdown] and [Quarto] are two powerful ways to combine writing (or presentations) and analysis into a single reproducible workflow. Although they can be a little more cumbersome to use than traditional word processors (e.g., Word or Pages) or presentation software (e.g., PowerPoint or Keynotes), they have the benefit of allowing you to keep all of the pieces of your manuscripts or presentations in one place. Change some data or analysis? The whole manuscript or presentation should update without forcing you to try and find all of the places where that change might alter your writing or slides. It may take a little getting used to, but the fact that both Rmarkdown and Quarto can utilize the power of LaTeX typesetting means that you’ll ultimately be able to produce publication quality equations, tables, and figures all in one place.\nThis webpage and all of my slides were built with Quarto (and last year’s was built with Rmarkdown and blogdown). Having gone through the process of learning how make that work, I’m convinced that having a working knowledge of one or both of these is useful. As such, you’ll be using Rmarkdown or Quarto (your choice) to complete your assignments and render them to html. To help you get started here are a few links:"
  },
  {
    "objectID": "slides/01-slides.html#todays-plan",
    "href": "slides/01-slides.html#todays-plan",
    "title": "What Is This Class???",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nIntroductions\nWhat can we do with spatial data?\nCourse logistics and resources\nTesting out RStudio and GitHub Classroom"
  },
  {
    "objectID": "slides/01-slides.html#about-you",
    "href": "slides/01-slides.html#about-you",
    "title": "What Is This Class???",
    "section": "About you?",
    "text": "About you?\n\n\n\nYour preferred pronouns\nWhere are you from?\nWhat do you like most about Boise?\nWhat do you miss most about “home”?\nWhat is your research?"
  },
  {
    "objectID": "slides/01-slides.html#about-me",
    "href": "slides/01-slides.html#about-me",
    "title": "What Is This Class???",
    "section": "About Me",
    "text": "About Me\n\n\n\nWhat I do\nMy path to this point\nWhy I teach this course"
  },
  {
    "objectID": "slides/01-slides.html#what-is-geography",
    "href": "slides/01-slides.html#what-is-geography",
    "title": "What Is This Class???",
    "section": "What is geography",
    "text": "What is geography\n\nGeo: land, earth, terrain\nGraph: writing, discourse\nTuan: Space (extent) and Place (location)\nAnalysis of the effects of extent and location on events or features"
  },
  {
    "objectID": "slides/01-slides.html#five-themes-in-geography",
    "href": "slides/01-slides.html#five-themes-in-geography",
    "title": "What Is This Class???",
    "section": "Five Themes in Geography",
    "text": "Five Themes in Geography\n\n\n\nLocation\nPlace\nRegion\nMovement\nHuman-Environment Interaction\n\n\n\n\n\nWGBH Educational Foundation"
  },
  {
    "objectID": "slides/01-slides.html#location",
    "href": "slides/01-slides.html#location",
    "title": "What Is This Class???",
    "section": "Location",
    "text": "Location\nThe place (on Earth) of a particular geographic feature"
  },
  {
    "objectID": "slides/01-slides.html#place",
    "href": "slides/01-slides.html#place",
    "title": "What Is This Class???",
    "section": "Place",
    "text": "Place\nWhat is a location like?\n\n \n\n\n\nForest cover map by Robert Simmons via Wikimedia Commons"
  },
  {
    "objectID": "slides/01-slides.html#region",
    "href": "slides/01-slides.html#region",
    "title": "What Is This Class???",
    "section": "Region",
    "text": "Region\nWhat attributes to different geographies share? What distinguishes them?"
  },
  {
    "objectID": "slides/01-slides.html#movement",
    "href": "slides/01-slides.html#movement",
    "title": "What Is This Class???",
    "section": "Movement",
    "text": "Movement\nHow do genes, individuals, populations, ideas, goods, etc traverse the landscape.\n\n \n\n\n\nLandGrab maps courtesy of High Country News"
  },
  {
    "objectID": "slides/01-slides.html#human-environment-interactions",
    "href": "slides/01-slides.html#human-environment-interactions",
    "title": "What Is This Class???",
    "section": "Human-Environment Interactions",
    "text": "Human-Environment Interactions\nHow do people relate to and change the physical world to meet their needs?\n\n \n\n\n\nSmoke map courtesy of Capital Public Radio; Nightlights courtesy of NASA Earth Observatory."
  },
  {
    "objectID": "slides/01-slides.html#description",
    "href": "slides/01-slides.html#description",
    "title": "What Is This Class???",
    "section": "Description",
    "text": "Description\n\n\n\n\n\n\n\nCoordinates\nDistances\nNeighbors\nSummary statistics\n\n\n\nRange Maps\nHotspots\nIndices"
  },
  {
    "objectID": "slides/01-slides.html#explanation-and-inference",
    "href": "slides/01-slides.html#explanation-and-inference",
    "title": "What Is This Class???",
    "section": "Explanation and Inference",
    "text": "Explanation and Inference\n\n\nCognitive Description: collection ordering and classification of data\nCause and Effect: design-based or model-based testing of the factors that give rise to geographic distributions\nSystems Analysis: describes the entire complex set of interactions that structure an activity"
  },
  {
    "objectID": "slides/01-slides.html#prediction",
    "href": "slides/01-slides.html#prediction",
    "title": "What Is This Class???",
    "section": "Prediction",
    "text": "Prediction\n\n\n\n\n\n\n\n\nExtend description or explanation into unmeasured space\nStationarity: the rules governing a process do not drift over space-time"
  },
  {
    "objectID": "slides/01-slides.html#logistics",
    "href": "slides/01-slides.html#logistics",
    "title": "What Is This Class???",
    "section": "Logistics",
    "text": "Logistics\n\n\nMeet on Mondays and Wednesdays\n~40 min lecture, 30 min practice\n5 major sections\nA note about readings\n\n“Office Hours” immediately following class"
  },
  {
    "objectID": "slides/01-slides.html#course-webpage",
    "href": "slides/01-slides.html#course-webpage",
    "title": "What Is This Class???",
    "section": "Course Webpage",
    "text": "Course Webpage\nhttps://isdrfall25.classes.spaseslab.com/\n\n\nSyllabus\nSchedule\nLectures\nAssignments\nResources"
  },
  {
    "objectID": "slides/01-slides.html#assignments",
    "href": "slides/01-slides.html#assignments",
    "title": "What Is This Class???",
    "section": "Assignments",
    "text": "Assignments\n\nCheck out the syllabus for more on grading!\n\n\n\n\n\nSelf-reflections (2x)\n\nYour goals for the course\nEvaluation criteria\n\nIn-class exercises (30x)\n\nProblem solving\nReproducible workflows\nMuscle memory\n\n\n\n\nHomework Assignments (5x)\n\nIntegrate skills from the section\nPractice visualization\nBuild version control habits\n\nCode Revisions (5x)\n\nCommon issues\nMore extensive feedback\n\n\n\n\n\n\n\nFinal project (1st draft, final draft): Practice a full analysis workflow; Integrate analysis & visuals to tell a story"
  },
  {
    "objectID": "slides/01-slides.html#checking-in",
    "href": "slides/01-slides.html#checking-in",
    "title": "What Is This Class???",
    "section": "Checking in",
    "text": "Checking in\n\nWhat can I clarify about the course?"
  },
  {
    "objectID": "slides/04-slides.html#checking-in",
    "href": "slides/04-slides.html#checking-in",
    "title": "Meeting Your Data",
    "section": "Checking in",
    "text": "Checking in\n\nHave you used the videos and lessons?\nHow can I make them more useful?\nAnything I can clarify from last class?"
  },
  {
    "objectID": "slides/04-slides.html#todays-plan",
    "href": "slides/04-slides.html#todays-plan",
    "title": "Meeting Your Data",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nData, information, and knowledge\nThe elements of data quality\nConcerns with the growth in available data\nTowards data justice"
  },
  {
    "objectID": "slides/04-slides.html#revisiting-workflows",
    "href": "slides/04-slides.html#revisiting-workflows",
    "title": "Meeting Your Data",
    "section": "Revisiting Workflows",
    "text": "Revisiting Workflows\n\n\n\n\n\n\nGet Data???"
  },
  {
    "objectID": "slides/04-slides.html#section",
    "href": "slides/04-slides.html#section",
    "title": "Meeting Your Data",
    "section": "",
    "text": "Boisot and Canals, 2004"
  },
  {
    "objectID": "slides/04-slides.html#section-1",
    "href": "slides/04-slides.html#section-1",
    "title": "Meeting Your Data",
    "section": "",
    "text": "Observations become data through the filters we put on them\n\n\n\nOur questions guide which observations we consider data\nBut what about found data?"
  },
  {
    "objectID": "slides/04-slides.html#fair-data-and-found-data",
    "href": "slides/04-slides.html#fair-data-and-found-data",
    "title": "Meeting Your Data",
    "section": "FAIR Data and Found Data",
    "text": "FAIR Data and Found Data\n\n\n\n\nAlvanço 2021\n\n\n\n\nFAIR data improves our ability to leverage existing data, but we haven’t really reached this ideal."
  },
  {
    "objectID": "slides/04-slides.html#most-spatial-data-is-found-data",
    "href": "slides/04-slides.html#most-spatial-data-is-found-data",
    "title": "Meeting Your Data",
    "section": "Most spatial data is “found” data",
    "text": "Most spatial data is “found” data\n\nMilitary advancement\nEnumeration of populations\nDelineations of authority"
  },
  {
    "objectID": "slides/04-slides.html#making-the-invisible-visible",
    "href": "slides/04-slides.html#making-the-invisible-visible",
    "title": "Meeting Your Data",
    "section": "Making the invisible visible…",
    "text": "Making the invisible visible…\n\n\nGlobalization of data resources\nData “traces” and policing\nBias in data ⇒ Bias in policy"
  },
  {
    "objectID": "slides/04-slides.html#data-justice",
    "href": "slides/04-slides.html#data-justice",
    "title": "Meeting Your Data",
    "section": "Data Justice",
    "text": "Data Justice\n\nData Justice promotes ethical and equitable practices in the collection, analysis, and use of data to serve the dignity, rights, and well-being of individuals and communities, especially those who have been historically marginalized"
  },
  {
    "objectID": "slides/04-slides.html#data-justice-in-practice",
    "href": "slides/04-slides.html#data-justice-in-practice",
    "title": "Meeting Your Data",
    "section": "Data Justice in Practice",
    "text": "Data Justice in Practice\n\n\n\n\nHoefsloot et al., 2022"
  },
  {
    "objectID": "slides/04-slides.html#from-fair-to-care",
    "href": "slides/04-slides.html#from-fair-to-care",
    "title": "Meeting Your Data",
    "section": "From FAIR to CARE",
    "text": "From FAIR to CARE\n\n\n\n\nGlobal Indigenous Data Alliance\n\n\n\n\nFAIR is about access and documentation\nCARE is about sovereignty, governance, and justice"
  },
  {
    "objectID": "slides/04-slides.html#questions-to-ask-yourself",
    "href": "slides/04-slides.html#questions-to-ask-yourself",
    "title": "Meeting Your Data",
    "section": "Questions to ask yourself?",
    "text": "Questions to ask yourself?\n\nWhere did this data come from and why was it collected?\nHow might the intention and biases of the collector affect the data?\nWho might benefit or suffer from this analysis and what do I owe them?\nAre there ways I can address this in my analysis or interpretation?"
  },
  {
    "objectID": "slides/04-slides.html#but-you-have-data-1",
    "href": "slides/04-slides.html#but-you-have-data-1",
    "title": "Meeting Your Data",
    "section": "…but you have data!!",
    "text": "…but you have data!!\n\nDocumenting your data\nExploring your data\nDoing both with Quarto"
  },
  {
    "objectID": "slides/06-slides.html#what-is-literate-programming",
    "href": "slides/06-slides.html#what-is-literate-programming",
    "title": "Pipes, Functions, and Iteration",
    "section": "What is literate programming?",
    "text": "What is literate programming?\n\nDocumentation containing code (not vice versa!)\nDirect connection between code and explanation\nConvey meaning to humans rather than telling computer what to do!"
  },
  {
    "objectID": "slides/06-slides.html#why-not-just-use-scripts",
    "href": "slides/06-slides.html#why-not-just-use-scripts",
    "title": "Pipes, Functions, and Iteration",
    "section": "Why not just use scripts",
    "text": "Why not just use scripts"
  },
  {
    "objectID": "slides/06-slides.html#better-documentation-better-reproducibility-quarto",
    "href": "slides/06-slides.html#better-documentation-better-reproducibility-quarto",
    "title": "Pipes, Functions, and Iteration",
    "section": "Better documentation, better reproducibility: Quarto",
    "text": "Better documentation, better reproducibility: Quarto"
  },
  {
    "objectID": "slides/06-slides.html#keeping-your-documents-readable",
    "href": "slides/06-slides.html#keeping-your-documents-readable",
    "title": "Pipes, Functions, and Iteration",
    "section": "Keeping your documents readable",
    "text": "Keeping your documents readable"
  },
  {
    "objectID": "slides/06-slides.html#pipes",
    "href": "slides/06-slides.html#pipes",
    "title": "Pipes, Functions, and Iteration",
    "section": "Pipes",
    "text": "Pipes"
  },
  {
    "objectID": "slides/06-slides.html#iteration",
    "href": "slides/06-slides.html#iteration",
    "title": "Pipes, Functions, and Iteration",
    "section": "Iteration",
    "text": "Iteration"
  },
  {
    "objectID": "slides/06-slides.html#building-your-own-functions",
    "href": "slides/06-slides.html#building-your-own-functions",
    "title": "Pipes, Functions, and Iteration",
    "section": "Building your own functions",
    "text": "Building your own functions"
  },
  {
    "objectID": "slides/06-slides.html#documenting-functions",
    "href": "slides/06-slides.html#documenting-functions",
    "title": "Pipes, Functions, and Iteration",
    "section": "Documenting functions",
    "text": "Documenting functions"
  },
  {
    "objectID": "slides/06-slides.html#directory-structure",
    "href": "slides/06-slides.html#directory-structure",
    "title": "Pipes, Functions, and Iteration",
    "section": "Directory structure",
    "text": "Directory structure"
  },
  {
    "objectID": "slides/06-slides.html#the-data-folder",
    "href": "slides/06-slides.html#the-data-folder",
    "title": "Pipes, Functions, and Iteration",
    "section": "The data folder",
    "text": "The data folder"
  },
  {
    "objectID": "slides/06-slides.html#the-scripts-folder",
    "href": "slides/06-slides.html#the-scripts-folder",
    "title": "Pipes, Functions, and Iteration",
    "section": "The scripts folder",
    "text": "The scripts folder"
  },
  {
    "objectID": "slides/06-slides.html#the-docs-folder",
    "href": "slides/06-slides.html#the-docs-folder",
    "title": "Pipes, Functions, and Iteration",
    "section": "The docs folder",
    "text": "The docs folder"
  },
  {
    "objectID": "slides/06-slides.html#avoiding-pitfalls-with-git",
    "href": "slides/06-slides.html#avoiding-pitfalls-with-git",
    "title": "Pipes, Functions, and Iteration",
    "section": "Avoiding pitfalls with git",
    "text": "Avoiding pitfalls with git\n\nAlways pull before you start working on anything new\nAvoid committing large files\nUsing .gitignore"
  }
]